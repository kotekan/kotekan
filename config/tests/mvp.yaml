##########################################
#
# CHORD correlator "minimum viable pipeline"
#
##########################################
---
type: config
# Logging level can be one of:
# OFF, ERROR, WARN, INFO, DEBUG, DEBUG2 (case insensitive)
# Note DEBUG and DEBUG2 require a build with (-DCMAKE_BUILD_TYPE=Debug)
log_level: info
num_links: 4
freq_array: [4,7,10,16]
timesamples_per_packet: 2
num_gpus: 1
num_gpu_frames: 1
cpu_affinity: [2,3,4,5,8,9,10,11]
sizeof_float: 4
sizeof_float16: 2
sizeof_int: 4

voltage_samples: 32768
voltage_freq: 16
num_dishes: 512
num_elements: num_dishes * 2

frame_arrival_period: 0.00000170667 * voltage_samples

n2coarse_sub_integration_ntime: 16384

# Fine visibilities
# NOTE that this divide-by-2 MUST match the hard-coded buffer setup logic in chordMVPSetup
fine_samples: voltage_samples / 2
fine_upchan: 16
n2fine_sub_integration_ntime: 1024

bb_beams: 96

frb_beams: 5000
frb_upchan: 16
# 2048
frb_samples: voltage_samples / frb_upchan
frb_freq: voltage_freq * frb_upchan
dish_grid_size: 24
frb_beam_grid_size: dish_grid_size * 2
frb_time_downsampling: 40

# Number of samples of padding allocated at the front of the voltage arrays
frb_bf_padding: 88

# Max number of samples passed from FRB1 to FRBRechunk
frb_td_max: 52

# FRB Rechunking
frb_td_rechunk: 256

buffer_depth: 4

# data gen:
num_frames: 100

# Pool
main_pool:
    kotekan_metadata_pool: oneHotMetadata
    num_metadata_objects: 15 * buffer_depth

# Buffers
host_upchan_Tactual_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: 4
    metadata_pool: main_pool

host_voltage_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: voltage_samples * num_dishes * 2 * voltage_freq
    metadata_pool: main_pool

host_coarse_correlation_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: voltage_freq * (voltage_samples / n2coarse_sub_integration_ntime) * num_elements * num_elements * 2 * sizeof_int
    metadata_pool: main_pool

# host_fine_correlation_buffer:
#     kotekan_buffer: standard
#     num_frames: buffer_depth
#     frame_size: voltage_freq * (fine_samples / n2fine_sub_integration_ntime) * num_elements * num_elements * 2 * sizeof_int
#     metadata_pool: main_pool

host_bb_phase_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: num_elements * voltage_freq * bb_beams * 2
    metadata_pool: main_pool

host_bb_shift_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: voltage_freq * bb_beams * 2 * sizeof_int
    metadata_pool: main_pool

host_bb_beams_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: voltage_samples * bb_beams * voltage_freq * 2
    metadata_pool: main_pool

host_frb_phase_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    frame_size: dish_grid_size * dish_grid_size * frb_freq * 2 * 2 * sizeof_float16
    metadata_pool: main_pool

host_beamquant_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    # / 2 because of int4 outputs.
    frame_size: frb_beams * frb_freq * frb_td_rechunk / 2
    metadata_pool: main_pool

host_beammeanstd_buffer:
    kotekan_buffer: standard
    num_frames: buffer_depth
    # *2*2: 2 for mean,std, 2 for float16
    frame_size: frb_beams * frb_freq * frb_td_rechunk * 2 * 2 / 256
    metadata_pool: main_pool

gen_data:
    kotekan_stage: testDataGen
    type: onehot
    values: [0x01, 0x02, 0x04, 0x07, 0x08, 0x09, 0xc, 0xe, 0xf, 0x10, 0x20, 0x40, 0x70, 0x80, 0x90, 0xc0, 0xe0, 0xf0]
    #type: random_signed
    #reuse_random: true
    # TIME, POL, FREQ, DISH
    array_shape: [32768, 2, 16, 512]
    out_buf: host_voltage_buffer

gen_upchan_Tactual:
    kotekan_stage: testDataGen
    type: int32
    value: 32768
    out_buf: host_upchan_Tactual_buffer

gen_frb_phase:
    kotekan_stage: testDataGen
    type: constf16
    value: 2.0
    out_buf: host_frb_phase_buffer

gen_bb_phase:
    kotekan_stage: testDataGen
    type: const
    value: 0x20
    out_buf: host_bb_phase_buffer

gen_bb_shift:
    kotekan_stage: testDataGen
    type: const32
    value: 0x4
    out_buf: host_bb_shift_buffer

gpu:
    # log_level: debug
    profiling: true
    log_profiling: true
    kernel_path: "lib/cuda/kernels"
    # More streams -- for FRBBeamReformer
    num_cuda_streams: 4
    #num_cuda_streams: 6
    commands: &command_list
    - name: chordMVPSetup
      log_level: debug
      upchan_factor: frb_upchan
      num_local_freq: voltage_freq
      samples_per_data_set: voltage_samples
      gpu_mem_frb_bf_input: bf_input
      gpu_mem_upchan_output: upchan_out
      gpu_mem_voltage: voltage
      gpu_mem_fine_upchan_input: fine_upchan_input
    - name: cudaInputData
      in_buf: host_upchan_Tactual
      gpu_mem: upchan_Tactual
    - name: cudaInputData
      in_buf: host_voltage
      gpu_mem: voltage
    - name: cudaInputData
      in_buf: host_frb_phase
      gpu_mem: frb_phase
    - name: cudaInputData
      in_buf: host_bb_phase
      gpu_mem: bb_phase
    - name: cudaInputData
      in_buf: host_bb_shift
      gpu_mem: bb_shift
    - name: cudaSyncInput
    # If using multiple compute streams in FRB Beam Reformer, must also sync those on inputs!
    # Could perhaps sync on stream 2 later??
    # - name: cudaSyncStream
    #   cuda_stream: 3
    #   source_cuda_streams: [0]
    # - name: cudaSyncStream
    #   cuda_stream: 4
    #   source_cuda_streams: [0]
    # - name: cudaSyncStream
    #   cuda_stream: 5
    #   source_cuda_streams: [0]
    # Coarse visibilities
    - name: cudaCorrelator
      gpu_mem_voltage: voltage
      gpu_mem_correlation_triangle: coarse_correlation_matrix
      num_local_freq: voltage_freq
      samples_per_data_set: voltage_samples
      sub_integration_ntime: n2coarse_sub_integration_ntime
    # # Fine visibilities -- upchannelized
    # - name: cudaUpchannelize16k
    #   gpu_mem_input_voltage: fine_upchan_input
    #   gpu_mem_output_voltage: fine_upchan_out
    #   gpu_mem_info: fine_upchan_info
    #   gpu_mem_gain: fine_upchan_gain
    #   num_local_freq: voltage_freq
    #   samples_per_data_set: fine_samples
    #   upchan_factor: fine_upchan
    # Fine visibilities
    - name: cudaCorrelator
      gpu_mem_voltage: fine_upchan_out
      gpu_mem_correlation_triangle: fine_correlation_matrix
      num_local_freq: voltage_freq
      samples_per_data_set: fine_samples
      sub_integration_ntime: n2fine_sub_integration_ntime
    # Baseband beamformer
    - name: cudaBasebandBeamformer
      gpu_mem_voltage: voltage
      gpu_mem_phase: bb_phase
      gpu_mem_output_scaling: bb_shift
      gpu_mem_formed_beams: bb_beams
      num_local_freq: voltage_freq
      samples_per_data_set: voltage_samples
      num_beams: bb_beams
    # FRB chain, starting with upchannelization
    - name: cudaUpchannelizer_U16
      Tactual: upchan_Tactual
      gpu_mem_input_voltage: voltage
      gpu_mem_output_voltage: upchan_out
      gpu_mem_gain: upchan_gains
      num_local_freq: voltage_freq
      samples_per_data_set: voltage_samples
      upchan_factor: frb_upchan
      freq_gains: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ]
    # FRB1: beamformer
    - name: cudaFRBBeamformer
      log_level: debug
      gpu_mem_voltage: bf_input
      gpu_mem_phase: frb_phase
      gpu_mem_dishlayout: dishlayout
      gpu_mem_beamgrid: beamgrid
      gpu_mem_info: upchan_info
      num_local_freq: frb_freq
      samples_per_data_set: frb_samples
      time_downsampling: frb_time_downsampling
      samples_padding: frb_bf_padding
      num_beams: frb_beams
    #
    # Rechunking!
    # Conceptually, the rechunker should follow the full-rate output stages.
    # However, the rechunker uses some memory copy operations that contend
    # for hardware resources (memory controllers) with the output stages,
    # so we avoid this by putting the rechunker's small memory copies first.
    # The minor cost is that the output stages are sequenced after the rechunker's
    # campute, but that's minor.
    #
    - name: cudaRechunk
      log_level: debug
      gpu_mem_input: beamgrid
      gpu_mem_output: beamchunk
      input_columns_field: frb_bf_bytes_per_freq
      # Erik's FRB beamformer produces output with rho varying fastest, then T,
      # then F, so T*rho is the inner size and F is the outer size.
      cols_input: frb_td_max * 2 * frb_beam_grid_size * frb_beam_grid_size
      cols_output: frb_td_rechunk * 2 * frb_beam_grid_size * frb_beam_grid_size
      rows: frb_freq
      set_flag: frb_rechunk
    # FRB2 & 3 follow after outputs!
    #
    # Begin output of full-rate results
    - name: cudaSyncOutput
    # Fine correlation -- for MVP we're not going to copy this back to host memory (too expensive)
    # - name: cudaOutputData
    #   in_buf: host_voltage # Metadata transfer from here
    #   gpu_mem: fine_correlation_matrix
    #   out_buf: host_fine_correlation
    # Coarse correlation
    - name: cudaOutputData
      in_buf: host_voltage # Metadata transfer from here
      gpu_mem: coarse_correlation_matrix
      out_buf: host_coarse_correlation
    # Baseband formed beams
    - name: cudaOutputData
      gpu_mem: bb_beams
      out_buf: host_bb_beams
      in_buf: host_voltage
    #
    # Operations on rechunked data
    #
    # FRB2: FRB Beam Reformer
    # Extra compute stream has to wait for Rechunk to finish!!
    - name: cudaSyncStream
      required_flag: frb_rechunk
      cuda_stream: 3
      source_cuda_streams: [2]
    - name: cudaFRBBeamReformer
      required_flag: frb_rechunk
      log_level: debug
      cuda_streams: [2,3]
      #cuda_streams: [2,3,4,5]
      gpu_mem_beamgrid: beamchunk
      gpu_mem_phase: beamphase
      gpu_mem_beamout: beamout
      num_local_freq: frb_freq
      samples_per_data_set: frb_td_rechunk
      num_beams: frb_beams
      beam_grid_size: frb_beam_grid_size
    # Wait for streams 3,4,5 to finish
    # - name: cudaSyncStream
    #   cuda_stream: 2
    #   source_cuda_streams: [3]
    #   #source_cuda_streams: [3,4,5]
    # FRB3: quantization into 4-bit int
    - name: cudaQuantize
      required_flag: frb_rechunk
      gpu_mem_input: beamout
      gpu_mem_output: beamquant
      gpu_mem_meanstd: beammeanstd
      num_chunks: frb_beams * frb_freq * frb_td_rechunk / 256
    # Sync rechunked outputs
    - name: cudaSyncOutput
      required_flag: frb_rechunk
    # Rechunked outputs
    - name: cudaOutputData
      required_flag: frb_rechunk
      gpu_mem: beamquant
      out_buf: host_beamquant
      in_buf: host_voltage # Metadata transfer from here
    - name: cudaOutputData
      required_flag: frb_rechunk
      gpu_mem: beammeanstd
      out_buf: host_beammeanstd
      in_buf: host_voltage # Metadata transfer from here
    gpu_0:
        kotekan_stage: cudaProcess
        gpu_id: 0
        commands: *command_list
        in_buffers:
            host_voltage: host_voltage_buffer
            host_bb_phase: host_bb_phase_buffer
            host_bb_shift: host_bb_shift_buffer
            host_upchan_Tactual: host_upchan_Tactual_buffer
            host_frb_phase: host_frb_phase_buffer
        out_buffers:
            # host_fine_correlation: host_fine_correlation_buffer
            host_coarse_correlation: host_coarse_correlation_buffer
            host_bb_beams: host_bb_beams_buffer
            host_beamquant: host_beamquant_buffer
            host_beammeanstd: host_beammeanstd_buffer

# printSparseGPU:
#     kotekan_stage: printSparseFloat16
#     # F, B, T
#     #array_shape: [frb_freq, frb_beams, frb_td_rechunk]
#     #input_buf: host_beamout_buffer
#     input_buf: host_beamquant_buffer
#     max_to_print: 8

# x = np.arange(24*24)
# M = x // 24
# N = x % 24
# json.dumps([int(x) for x in np.vstack((M, N)).T.ravel()])
frb_beamformer_dish_layout: [0, 0, 0, 1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9, 0, 10, 0, 11, 0, 12, 0, 13, 0, 14, 0, 15, 0, 16, 0, 17, 0, 18, 0, 19, 0, 20, 0, 21, 0, 22, 0, 23, 1, 0, 1, 1, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9, 1, 10, 1, 11, 1, 12, 1, 13, 1, 14, 1, 15, 1, 16, 1, 17, 1, 18, 1, 19, 1, 20, 1, 21, 1, 22, 1, 23, 2, 0, 2, 1, 2, 2, 2, 3, 2, 4, 2, 5, 2, 6, 2, 7, 2, 8, 2, 9, 2, 10, 2, 11, 2, 12, 2, 13, 2, 14, 2, 15, 2, 16, 2, 17, 2, 18, 2, 19, 2, 20, 2, 21, 2, 22, 2, 23, 3, 0, 3, 1, 3, 2, 3, 3, 3, 4, 3, 5, 3, 6, 3, 7, 3, 8, 3, 9, 3, 10, 3, 11, 3, 12, 3, 13, 3, 14, 3, 15, 3, 16, 3, 17, 3, 18, 3, 19, 3, 20, 3, 21, 3, 22, 3, 23, 4, 0, 4, 1, 4, 2, 4, 3, 4, 4, 4, 5, 4, 6, 4, 7, 4, 8, 4, 9, 4, 10, 4, 11, 4, 12, 4, 13, 4, 14, 4, 15, 4, 16, 4, 17, 4, 18, 4, 19, 4, 20, 4, 21, 4, 22, 4, 23, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 5, 5, 6, 5, 7, 5, 8, 5, 9, 5, 10, 5, 11, 5, 12, 5, 13, 5, 14, 5, 15, 5, 16, 5, 17, 5, 18, 5, 19, 5, 20, 5, 21, 5, 22, 5, 23, 6, 0, 6, 1, 6, 2, 6, 3, 6, 4, 6, 5, 6, 6, 6, 7, 6, 8, 6, 9, 6, 10, 6, 11, 6, 12, 6, 13, 6, 14, 6, 15, 6, 16, 6, 17, 6, 18, 6, 19, 6, 20, 6, 21, 6, 22, 6, 23, 7, 0, 7, 1, 7, 2, 7, 3, 7, 4, 7, 5, 7, 6, 7, 7, 7, 8, 7, 9, 7, 10, 7, 11, 7, 12, 7, 13, 7, 14, 7, 15, 7, 16, 7, 17, 7, 18, 7, 19, 7, 20, 7, 21, 7, 22, 7, 23, 8, 0, 8, 1, 8, 2, 8, 3, 8, 4, 8, 5, 8, 6, 8, 7, 8, 8, 8, 9, 8, 10, 8, 11, 8, 12, 8, 13, 8, 14, 8, 15, 8, 16, 8, 17, 8, 18, 8, 19, 8, 20, 8, 21, 8, 22, 8, 23, 9, 0, 9, 1, 9, 2, 9, 3, 9, 4, 9, 5, 9, 6, 9, 7, 9, 8, 9, 9, 9, 10, 9, 11, 9, 12, 9, 13, 9, 14, 9, 15, 9, 16, 9, 17, 9, 18, 9, 19, 9, 20, 9, 21, 9, 22, 9, 23, 10, 0, 10, 1, 10, 2, 10, 3, 10, 4, 10, 5, 10, 6, 10, 7, 10, 8, 10, 9, 10, 10, 10, 11, 10, 12, 10, 13, 10, 14, 10, 15, 10, 16, 10, 17, 10, 18, 10, 19, 10, 20, 10, 21, 10, 22, 10, 23, 11, 0, 11, 1, 11, 2, 11, 3, 11, 4, 11, 5, 11, 6, 11, 7, 11, 8, 11, 9, 11, 10, 11, 11, 11, 12, 11, 13, 11, 14, 11, 15, 11, 16, 11, 17, 11, 18, 11, 19, 11, 20, 11, 21, 11, 22, 11, 23, 12, 0, 12, 1, 12, 2, 12, 3, 12, 4, 12, 5, 12, 6, 12, 7, 12, 8, 12, 9, 12, 10, 12, 11, 12, 12, 12, 13, 12, 14, 12, 15, 12, 16, 12, 17, 12, 18, 12, 19, 12, 20, 12, 21, 12, 22, 12, 23, 13, 0, 13, 1, 13, 2, 13, 3, 13, 4, 13, 5, 13, 6, 13, 7, 13, 8, 13, 9, 13, 10, 13, 11, 13, 12, 13, 13, 13, 14, 13, 15, 13, 16, 13, 17, 13, 18, 13, 19, 13, 20, 13, 21, 13, 22, 13, 23, 14, 0, 14, 1, 14, 2, 14, 3, 14, 4, 14, 5, 14, 6, 14, 7, 14, 8, 14, 9, 14, 10, 14, 11, 14, 12, 14, 13, 14, 14, 14, 15, 14, 16, 14, 17, 14, 18, 14, 19, 14, 20, 14, 21, 14, 22, 14, 23, 15, 0, 15, 1, 15, 2, 15, 3, 15, 4, 15, 5, 15, 6, 15, 7, 15, 8, 15, 9, 15, 10, 15, 11, 15, 12, 15, 13, 15, 14, 15, 15, 15, 16, 15, 17, 15, 18, 15, 19, 15, 20, 15, 21, 15, 22, 15, 23, 16, 0, 16, 1, 16, 2, 16, 3, 16, 4, 16, 5, 16, 6, 16, 7, 16, 8, 16, 9, 16, 10, 16, 11, 16, 12, 16, 13, 16, 14, 16, 15, 16, 16, 16, 17, 16, 18, 16, 19, 16, 20, 16, 21, 16, 22, 16, 23, 17, 0, 17, 1, 17, 2, 17, 3, 17, 4, 17, 5, 17, 6, 17, 7, 17, 8, 17, 9, 17, 10, 17, 11, 17, 12, 17, 13, 17, 14, 17, 15, 17, 16, 17, 17, 17, 18, 17, 19, 17, 20, 17, 21, 17, 22, 17, 23, 18, 0, 18, 1, 18, 2, 18, 3, 18, 4, 18, 5, 18, 6, 18, 7, 18, 8, 18, 9, 18, 10, 18, 11, 18, 12, 18, 13, 18, 14, 18, 15, 18, 16, 18, 17, 18, 18, 18, 19, 18, 20, 18, 21, 18, 22, 18, 23, 19, 0, 19, 1, 19, 2, 19, 3, 19, 4, 19, 5, 19, 6, 19, 7, 19, 8, 19, 9, 19, 10, 19, 11, 19, 12, 19, 13, 19, 14, 19, 15, 19, 16, 19, 17, 19, 18, 19, 19, 19, 20, 19, 21, 19, 22, 19, 23, 20, 0, 20, 1, 20, 2, 20, 3, 20, 4, 20, 5, 20, 6, 20, 7, 20, 8, 20, 9, 20, 10, 20, 11, 20, 12, 20, 13, 20, 14, 20, 15, 20, 16, 20, 17, 20, 18, 20, 19, 20, 20, 20, 21, 20, 22, 20, 23, 21, 0, 21, 1, 21, 2, 21, 3, 21, 4, 21, 5, 21, 6, 21, 7, 21, 8, 21, 9, 21, 10, 21, 11, 21, 12, 21, 13, 21, 14, 21, 15, 21, 16, 21, 17, 21, 18, 21, 19, 21, 20, 21, 21, 21, 22, 21, 23, 22, 0, 22, 1, 22, 2, 22, 3, 22, 4, 22, 5, 22, 6, 22, 7, 22, 8, 22, 9, 22, 10, 22, 11, 22, 12, 22, 13, 22, 14, 22, 15, 22, 16, 22, 17, 22, 18, 22, 19, 22, 20, 22, 21, 22, 22, 22, 23, 23, 0, 23, 1, 23, 2, 23, 3, 23, 4, 23, 5, 23, 6, 23, 7, 23, 8, 23, 9, 23, 10, 23, 11, 23, 12, 23, 13, 23, 14, 23, 15, 23, 16, 23, 17, 23, 18, 23, 19, 23, 20, 23, 21, 23, 22, 23, 23]

// PTX CompilerJob of MethodInstance for bb(::CuDeviceVector{Int8x4, 1}, ::CuDeviceVector{Int4x8, 1}, ::CuDeviceVector{Int32, 1}, ::CuDeviceVector{Int4x8, 1}, ::CuDeviceVector{Int32, 1}) for sm_86, minthreads=768, blocks_per_sm=1

//
// Generated by LLVM NVPTX Back-End
//

.version 8.1
.target sm_86
.address_size 64

	// .globl	_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE // -- Begin function _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE
.func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
.noreturn
{
	trap;
}
.func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[16]
)
.noreturn
{
	trap;
}
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception925[6] = {101, 114, 114, 111, 114, 0};
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE
.visible .entry _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE(
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_0[16],
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_4[32],
	.param .align 8 .b8 _Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_5[32]
)
.reqntid 768, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<1364>;
	.reg .b64 	%rd<96>;

// %bb.0:                               // %conversion
	ld.param.u32 	%r88, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_0+8];
	ld.param.u64 	%rd19, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_0];
	// begin inline asm
	mov.u32 %r89, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r89, 16511;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	// begin inline asm
	mov.u32 %r90, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p2, %r90, 67711;
	@%p2 bra 	LBB0_4;
	bra.uni 	LBB0_3;
LBB0_4:                                 // %L31
	ld.param.u64 	%rd3, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_3];
	ld.param.u64 	%rd5, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r91, %r2, 768;
	mov.u32 	%r3, %tid.y;
	shl.b32 	%r4, %r3, 5;
	or.b32  	%r92, %r91, %r1;
	add.s32 	%r93, %r4, %r92;
	mul.wide.u32 	%rd24, %r93, 4;
	add.s64 	%rd6, %rd5, %rd24;
	mov.u32 	%r94, 1;
	st.global.u32 	[%rd6], %r94;
	shr.u32 	%r5, %r1, 3;
	shl.b32 	%r95, %r3, 2;
	or.b32  	%r96, %r95, %r5;
	mul.hi.u32 	%r97, %r96, -1431655765;
	shr.u32 	%r98, %r97, 6;
	mul.lo.s32 	%r99, %r98, 96;
	sub.s32 	%r6, %r96, %r99;
	bfe.u32 	%r7, %r2, 4, 1;
	shr.u32 	%r8, %r2, 5;
	mad.lo.s32 	%r100, %r7, 96, %r6;
	mad.lo.s32 	%r101, %r8, 192, %r100;
	mul.wide.u32 	%rd25, %r101, 4;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.u32 	%r9, [%rd26];
	add.s32 	%r10, %r9, -4;
	setp.lt.u32 	%p3, %r10, 31;
	@%p3 bra 	LBB0_6;
	bra.uni 	LBB0_5;
LBB0_6:                                 // %L389
	ld.param.u64 	%rd1, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_1];
	ld.param.u64 	%rd2, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_2];
	ld.param.u64 	%rd4, [_Z2bb13CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EES_IS2_Li1ELi1EE_param_4];
	add.s32 	%r360, %r9, -3;
	shr.u32 	%r361, %r3, 2;
	cvt.u16.u32 	%rs1, %r361;
	mul.lo.s16 	%rs2, %rs1, 171;
	shr.u16 	%rs3, %rs2, 10;
	mul.lo.s16 	%rs4, %rs3, 6;
	sub.s16 	%rs5, %rs1, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	cvt.u32.u16 	%r362, %rs6;
	and.b32  	%r363, %r362, 240;
	shr.u32 	%r11, %r1, 2;
	and.b32  	%r364, %r11, 6;
	or.b32  	%r365, %r364, %r363;
	cvt.u16.u32 	%rs7, %r365;
	mul.lo.s16 	%rs8, %rs7, 171;
	shr.u16 	%rs9, %rs8, 14;
	mul.lo.s16 	%rs10, %rs9, 96;
	sub.s16 	%rs11, %rs7, %rs10;
	and.b16  	%rs12, %rs11, 246;
	mul.wide.u16 	%r366, %rs12, 256;
	mul.lo.s32 	%r367, %r7, 24576;
	mul.lo.s32 	%r368, %r8, 49152;
	shl.b32 	%r369, %r1, 1;
	and.b32  	%r370, %r369, 12;
	shl.b32 	%r371, %r1, 4;
	and.b32  	%r12, %r371, 16;
	shl.b32 	%r372, %r3, 6;
	and.b32  	%r373, %r372, 192;
	add.s32 	%r374, %r366, %r367;
	add.s32 	%r375, %r374, %r368;
	or.b32  	%r376, %r375, %r370;
	or.b32  	%r377, %r376, %r12;
	or.b32  	%r378, %r377, %r373;
	shl.b32 	%r379, %r378, 2;
	cvt.u64.u32 	%rd29, %r379;
	add.s64 	%rd30, %rd29, %rd1;
	ld.global.v4.u32 	{%r104, %r105, %r120, %r121}, [%rd30];
	or.b16  	%rs13, %rs7, 8;
	mul.lo.s16 	%rs14, %rs13, 171;
	shr.u16 	%rs15, %rs14, 14;
	mul.lo.s16 	%rs16, %rs15, 96;
	sub.s16 	%rs17, %rs13, %rs16;
	and.b16  	%rs18, %rs17, 254;
	mul.wide.u16 	%r380, %rs18, 256;
	add.s32 	%r381, %r368, %r367;
	add.s32 	%r382, %r381, %r380;
	or.b32  	%r383, %r382, %r370;
	or.b32  	%r384, %r383, %r12;
	or.b32  	%r385, %r384, %r373;
	shl.b32 	%r386, %r385, 2;
	cvt.u64.u32 	%rd31, %r386;
	add.s64 	%rd32, %rd31, %rd1;
	ld.global.v4.u32 	{%r112, %r113, %r128, %r129}, [%rd32];
	or.b32  	%r387, %r367, 32;
	add.s32 	%r388, %r387, %r366;
	add.s32 	%r389, %r388, %r368;
	or.b32  	%r390, %r389, %r370;
	or.b32  	%r391, %r390, %r12;
	or.b32  	%r392, %r391, %r373;
	shl.b32 	%r393, %r392, 2;
	cvt.u64.u32 	%rd33, %r393;
	add.s64 	%rd34, %rd33, %rd1;
	ld.global.v4.u32 	{%r136, %r137, %r152, %r153}, [%rd34];
	add.s32 	%r394, %r387, %r368;
	add.s32 	%r395, %r394, %r380;
	or.b32  	%r396, %r395, %r370;
	or.b32  	%r397, %r396, %r12;
	or.b32  	%r398, %r397, %r373;
	shl.b32 	%r399, %r398, 2;
	cvt.u64.u32 	%rd35, %r399;
	add.s64 	%rd36, %rd35, %rd1;
	ld.global.v4.u32 	{%r144, %r145, %r160, %r161}, [%rd36];
	or.b32  	%r400, %r11, %r363;
	or.b32  	%r401, %r400, 1;
	mul.hi.u32 	%r402, %r401, -1431655765;
	shr.u32 	%r403, %r402, 6;
	mul.lo.s32 	%r404, %r403, 96;
	sub.s32 	%r405, %r401, %r404;
	shl.b32 	%r406, %r405, 8;
	add.s32 	%r407, %r381, %r406;
	or.b32  	%r408, %r407, %r370;
	or.b32  	%r409, %r408, %r12;
	or.b32  	%r410, %r409, %r373;
	shl.b32 	%r411, %r410, 2;
	cvt.u64.u32 	%rd37, %r411;
	add.s64 	%rd38, %rd37, %rd1;
	ld.global.v4.u32 	{%r168, %r169, %r184, %r185}, [%rd38];
	or.b32  	%r412, %r400, 9;
	mul.hi.u32 	%r413, %r412, -1431655765;
	shr.u32 	%r414, %r413, 6;
	mul.lo.s32 	%r415, %r414, 96;
	sub.s32 	%r416, %r412, %r415;
	shl.b32 	%r417, %r416, 8;
	add.s32 	%r418, %r381, %r417;
	or.b32  	%r419, %r418, %r370;
	or.b32  	%r420, %r419, %r12;
	or.b32  	%r421, %r420, %r373;
	shl.b32 	%r422, %r421, 2;
	cvt.u64.u32 	%rd39, %r422;
	add.s64 	%rd40, %rd39, %rd1;
	ld.global.v4.u32 	{%r176, %r177, %r192, %r193}, [%rd40];
	add.s32 	%r423, %r394, %r406;
	or.b32  	%r424, %r423, %r370;
	or.b32  	%r425, %r424, %r12;
	or.b32  	%r426, %r425, %r373;
	shl.b32 	%r427, %r426, 2;
	cvt.u64.u32 	%rd41, %r427;
	add.s64 	%rd42, %rd41, %rd1;
	ld.global.v4.u32 	{%r200, %r201, %r216, %r217}, [%rd42];
	add.s32 	%r428, %r394, %r417;
	or.b32  	%r429, %r428, %r370;
	or.b32  	%r430, %r429, %r12;
	or.b32  	%r431, %r430, %r373;
	shl.b32 	%r432, %r431, 2;
	cvt.u64.u32 	%rd43, %r432;
	add.s64 	%rd44, %rd43, %rd1;
	ld.global.v4.u32 	{%r208, %r209, %r224, %r225}, [%rd44];
	mov.u32 	%r106, 21520;
	// begin inline asm
	prmt.b32 %r232, %r104, %r105, %r106;
	// end inline asm
	mov.u32 	%r110, 30258;
	// begin inline asm
	prmt.b32 %r233, %r104, %r105, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r240, %r112, %r113, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r241, %r112, %r113, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r248, %r120, %r121, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r249, %r120, %r121, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r256, %r128, %r129, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r257, %r128, %r129, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r264, %r136, %r137, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r265, %r136, %r137, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r272, %r144, %r145, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r273, %r144, %r145, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r280, %r152, %r153, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r281, %r152, %r153, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r288, %r160, %r161, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r289, %r160, %r161, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r296, %r168, %r169, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r297, %r168, %r169, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r304, %r176, %r177, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r305, %r176, %r177, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r312, %r184, %r185, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r313, %r184, %r185, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r320, %r192, %r193, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r321, %r192, %r193, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r328, %r200, %r201, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r329, %r200, %r201, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r336, %r208, %r209, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r337, %r208, %r209, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r344, %r216, %r217, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r345, %r216, %r217, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r352, %r224, %r225, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r353, %r224, %r225, %r110;
	// end inline asm
	mov.u32 	%r234, 25152;
	// begin inline asm
	prmt.b32 %r231, %r232, %r233, %r234;
	// end inline asm
	mov.u32 	%r238, 29521;
	// begin inline asm
	prmt.b32 %r235, %r232, %r233, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r239, %r240, %r241, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r243, %r240, %r241, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r247, %r248, %r249, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r251, %r248, %r249, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r255, %r256, %r257, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r259, %r256, %r257, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r263, %r264, %r265, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r267, %r264, %r265, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r271, %r272, %r273, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r275, %r272, %r273, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r279, %r280, %r281, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r283, %r280, %r281, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r287, %r288, %r289, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r291, %r288, %r289, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r295, %r296, %r297, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r299, %r296, %r297, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r303, %r304, %r305, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r307, %r304, %r305, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r311, %r312, %r313, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r315, %r312, %r313, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r319, %r320, %r321, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r323, %r320, %r321, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r327, %r328, %r329, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r331, %r328, %r329, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r335, %r336, %r337, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r339, %r336, %r337, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r343, %r344, %r345, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r347, %r344, %r345, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r351, %r352, %r353, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r355, %r352, %r353, %r238;
	// end inline asm
	and.b32  	%r13, %r1, 2;
	setp.eq.s32 	%p4, %r13, 0;
	selp.b32 	%r433, %r263, %r231, %p4;
	shfl.sync.bfly.b32	%r434, %r433, 2, 31, -1;
	selp.b32 	%r435, %r231, %r434, %p4;
	selp.b32 	%r436, %r434, %r263, %p4;
	selp.b32 	%r437, %r271, %r239, %p4;
	shfl.sync.bfly.b32	%r438, %r437, 2, 31, -1;
	selp.b32 	%r439, %r239, %r438, %p4;
	selp.b32 	%r440, %r438, %r271, %p4;
	selp.b32 	%r441, %r267, %r235, %p4;
	shfl.sync.bfly.b32	%r442, %r441, 2, 31, -1;
	selp.b32 	%r443, %r235, %r442, %p4;
	selp.b32 	%r444, %r442, %r267, %p4;
	selp.b32 	%r445, %r275, %r243, %p4;
	shfl.sync.bfly.b32	%r446, %r445, 2, 31, -1;
	selp.b32 	%r447, %r243, %r446, %p4;
	selp.b32 	%r448, %r446, %r275, %p4;
	selp.b32 	%r449, %r279, %r247, %p4;
	shfl.sync.bfly.b32	%r450, %r449, 2, 31, -1;
	selp.b32 	%r451, %r247, %r450, %p4;
	selp.b32 	%r452, %r450, %r279, %p4;
	selp.b32 	%r453, %r287, %r255, %p4;
	shfl.sync.bfly.b32	%r454, %r453, 2, 31, -1;
	selp.b32 	%r455, %r255, %r454, %p4;
	selp.b32 	%r456, %r454, %r287, %p4;
	selp.b32 	%r457, %r283, %r251, %p4;
	shfl.sync.bfly.b32	%r458, %r457, 2, 31, -1;
	selp.b32 	%r459, %r251, %r458, %p4;
	selp.b32 	%r460, %r458, %r283, %p4;
	selp.b32 	%r461, %r291, %r259, %p4;
	shfl.sync.bfly.b32	%r462, %r461, 2, 31, -1;
	selp.b32 	%r463, %r259, %r462, %p4;
	selp.b32 	%r464, %r462, %r291, %p4;
	selp.b32 	%r465, %r327, %r295, %p4;
	shfl.sync.bfly.b32	%r466, %r465, 2, 31, -1;
	selp.b32 	%r467, %r295, %r466, %p4;
	selp.b32 	%r468, %r466, %r327, %p4;
	selp.b32 	%r469, %r335, %r303, %p4;
	shfl.sync.bfly.b32	%r470, %r469, 2, 31, -1;
	selp.b32 	%r471, %r303, %r470, %p4;
	selp.b32 	%r472, %r470, %r335, %p4;
	selp.b32 	%r473, %r331, %r299, %p4;
	shfl.sync.bfly.b32	%r474, %r473, 2, 31, -1;
	selp.b32 	%r475, %r299, %r474, %p4;
	selp.b32 	%r476, %r474, %r331, %p4;
	selp.b32 	%r477, %r339, %r307, %p4;
	shfl.sync.bfly.b32	%r478, %r477, 2, 31, -1;
	selp.b32 	%r479, %r307, %r478, %p4;
	selp.b32 	%r480, %r478, %r339, %p4;
	selp.b32 	%r481, %r343, %r311, %p4;
	shfl.sync.bfly.b32	%r482, %r481, 2, 31, -1;
	selp.b32 	%r483, %r311, %r482, %p4;
	selp.b32 	%r484, %r482, %r343, %p4;
	selp.b32 	%r485, %r351, %r319, %p4;
	shfl.sync.bfly.b32	%r486, %r485, 2, 31, -1;
	selp.b32 	%r487, %r319, %r486, %p4;
	selp.b32 	%r488, %r486, %r351, %p4;
	selp.b32 	%r489, %r347, %r315, %p4;
	shfl.sync.bfly.b32	%r490, %r489, 2, 31, -1;
	selp.b32 	%r491, %r315, %r490, %p4;
	selp.b32 	%r492, %r490, %r347, %p4;
	selp.b32 	%r493, %r355, %r323, %p4;
	shfl.sync.bfly.b32	%r494, %r493, 2, 31, -1;
	selp.b32 	%r495, %r323, %r494, %p4;
	selp.b32 	%r496, %r494, %r355, %p4;
	and.b32  	%r14, %r1, 4;
	setp.eq.s32 	%p5, %r14, 0;
	selp.b32 	%r497, %r467, %r435, %p5;
	shfl.sync.bfly.b32	%r498, %r497, 4, 31, -1;
	selp.b32 	%r591, %r435, %r498, %p5;
	selp.b32 	%r719, %r498, %r467, %p5;
	selp.b32 	%r499, %r471, %r439, %p5;
	shfl.sync.bfly.b32	%r500, %r499, 4, 31, -1;
	selp.b32 	%r853, %r439, %r500, %p5;
	selp.b32 	%r981, %r500, %r471, %p5;
	selp.b32 	%r501, %r475, %r443, %p5;
	shfl.sync.bfly.b32	%r502, %r501, 4, 31, -1;
	selp.b32 	%r597, %r443, %r502, %p5;
	selp.b32 	%r725, %r502, %r475, %p5;
	selp.b32 	%r503, %r479, %r447, %p5;
	shfl.sync.bfly.b32	%r504, %r503, 4, 31, -1;
	selp.b32 	%r859, %r447, %r504, %p5;
	selp.b32 	%r987, %r504, %r479, %p5;
	selp.b32 	%r505, %r483, %r451, %p5;
	shfl.sync.bfly.b32	%r506, %r505, 4, 31, -1;
	selp.b32 	%r623, %r451, %r506, %p5;
	selp.b32 	%r751, %r506, %r483, %p5;
	selp.b32 	%r507, %r487, %r455, %p5;
	shfl.sync.bfly.b32	%r508, %r507, 4, 31, -1;
	selp.b32 	%r885, %r455, %r508, %p5;
	selp.b32 	%r1013, %r508, %r487, %p5;
	selp.b32 	%r509, %r491, %r459, %p5;
	shfl.sync.bfly.b32	%r510, %r509, 4, 31, -1;
	selp.b32 	%r629, %r459, %r510, %p5;
	selp.b32 	%r757, %r510, %r491, %p5;
	selp.b32 	%r511, %r495, %r463, %p5;
	shfl.sync.bfly.b32	%r512, %r511, 4, 31, -1;
	selp.b32 	%r891, %r463, %r512, %p5;
	selp.b32 	%r1019, %r512, %r495, %p5;
	selp.b32 	%r513, %r468, %r436, %p5;
	shfl.sync.bfly.b32	%r514, %r513, 4, 31, -1;
	selp.b32 	%r655, %r436, %r514, %p5;
	selp.b32 	%r783, %r514, %r468, %p5;
	selp.b32 	%r515, %r472, %r440, %p5;
	shfl.sync.bfly.b32	%r516, %r515, 4, 31, -1;
	selp.b32 	%r917, %r440, %r516, %p5;
	selp.b32 	%r1045, %r516, %r472, %p5;
	selp.b32 	%r517, %r476, %r444, %p5;
	shfl.sync.bfly.b32	%r518, %r517, 4, 31, -1;
	selp.b32 	%r661, %r444, %r518, %p5;
	selp.b32 	%r789, %r518, %r476, %p5;
	selp.b32 	%r519, %r480, %r448, %p5;
	shfl.sync.bfly.b32	%r520, %r519, 4, 31, -1;
	selp.b32 	%r923, %r448, %r520, %p5;
	selp.b32 	%r1051, %r520, %r480, %p5;
	selp.b32 	%r521, %r484, %r452, %p5;
	shfl.sync.bfly.b32	%r522, %r521, 4, 31, -1;
	selp.b32 	%r687, %r452, %r522, %p5;
	selp.b32 	%r815, %r522, %r484, %p5;
	selp.b32 	%r523, %r488, %r456, %p5;
	shfl.sync.bfly.b32	%r524, %r523, 4, 31, -1;
	selp.b32 	%r949, %r456, %r524, %p5;
	selp.b32 	%r1077, %r524, %r488, %p5;
	selp.b32 	%r525, %r492, %r460, %p5;
	shfl.sync.bfly.b32	%r526, %r525, 4, 31, -1;
	selp.b32 	%r693, %r460, %r526, %p5;
	selp.b32 	%r821, %r526, %r492, %p5;
	selp.b32 	%r527, %r496, %r464, %p5;
	shfl.sync.bfly.b32	%r528, %r527, 4, 31, -1;
	selp.b32 	%r955, %r464, %r528, %p5;
	selp.b32 	%r1083, %r528, %r496, %p5;
	shl.b32 	%r529, %r2, 3;
	and.b32  	%r47, %r529, 128;
	shl.b32 	%r530, %r2, 11;
	and.b32  	%r48, %r530, 30720;
	and.b32  	%r49, %r3, 12;
	shl.b32 	%r531, %r1, 2;
	and.b32  	%r50, %r531, 28;
	and.b32  	%r51, %r529, 3840;
	and.b32  	%r52, %r4, 96;
	or.b32  	%r53, %r50, %r52;
	or.b32  	%r532, %r49, %r5;
	mul.lo.s32 	%r533, %r532, 129;
	add.s32 	%r534, %r53, %r533;
	mul.wide.u32 	%rd45, %r534, 4;
	mov.u64 	%rd46, shmem;
	add.s64 	%rd7, %rd46, %rd45;
	cvt.u64.u32 	%rd47, %r533;
	cvt.u64.u32 	%rd48, %r53;
	add.s64 	%rd49, %rd48, %rd47;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd8, %rd46, %rd50;
	or.b32  	%r535, %r532, 16;
	mul.lo.s32 	%r536, %r535, 129;
	add.s32 	%r537, %r53, %r536;
	mul.wide.u32 	%rd51, %r537, 4;
	add.s64 	%rd9, %rd46, %rd51;
	cvt.u64.u32 	%rd52, %r536;
	add.s64 	%rd53, %rd48, %rd52;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd10, %rd46, %rd54;
	shl.b32 	%r538, %r1, 3;
	and.b32  	%r539, %r538, 24;
	or.b32  	%r54, %r52, %r539;
	mul.hi.u32 	%r540, %r400, -1431655765;
	shr.u32 	%r541, %r540, 6;
	mul.lo.s32 	%r542, %r541, 96;
	sub.s32 	%r55, %r400, %r542;
	and.b32  	%r56, %r369, 6;
	and.b32  	%r543, %r3, 3;
	mul.lo.s32 	%r57, %r543, 3200;
	or.b32  	%r58, %r57, %r55;
	or.b32  	%r544, %r400, 8;
	mul.hi.u32 	%r545, %r544, -1431655765;
	shr.u32 	%r546, %r545, 6;
	mul.lo.s32 	%r547, %r546, 96;
	sub.s32 	%r59, %r544, %r547;
	or.b32  	%r60, %r57, %r59;
	and.b32  	%r548, %r1, 7;
	mul.lo.s32 	%r549, %r548, 100;
	add.s32 	%r550, %r549, %r6;
	mul.wide.u32 	%rd55, %r550, 4;
	add.s64 	%rd56, %rd46, 16512;
	add.s64 	%rd11, %rd56, %rd55;
	cvt.u64.u32 	%rd57, %r549;
	cvt.u64.u32 	%rd58, %r6;
	add.s64 	%rd59, %rd58, %rd57;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd12, %rd56, %rd60;
	add.s32 	%r551, %r549, 800;
	add.s32 	%r552, %r551, %r6;
	mul.wide.u32 	%rd61, %r552, 4;
	add.s64 	%rd13, %rd56, %rd61;
	cvt.u64.u32 	%rd62, %r551;
	add.s64 	%rd63, %rd58, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd14, %rd56, %rd64;
	add.s32 	%r553, %r549, 1600;
	add.s32 	%r554, %r553, %r6;
	mul.wide.u32 	%rd65, %r554, 4;
	add.s64 	%rd15, %rd56, %rd65;
	cvt.u64.u32 	%rd66, %r553;
	add.s64 	%rd67, %rd58, %rd66;
	shl.b64 	%rd68, %rd67, 2;
	add.s64 	%rd16, %rd56, %rd68;
	or.b32  	%r555, %r1, 24;
	mul.lo.s32 	%r556, %r555, 100;
	add.s32 	%r557, %r556, %r6;
	mul.wide.u32 	%rd69, %r557, 4;
	add.s64 	%rd17, %rd56, %rd69;
	cvt.u64.u32 	%rd70, %r556;
	add.s64 	%rd71, %rd58, %rd70;
	shl.b64 	%rd72, %rd71, 2;
	add.s64 	%rd18, %rd56, %rd72;
	shl.b32 	%r61, %r94, %r10;
	min.u32 	%r62, %r360, 31;
	and.b32  	%r63, %r1, 1;
	shl.b32 	%r559, %r1, 5;
	and.b32  	%r64, %r559, 64;
	and.b32  	%r65, %r530, 32768;
	shl.b32 	%r66, %r6, 20;
	and.b32  	%r67, %r530, 983040;
	and.b32  	%r68, %r538, 32;
	mov.u32 	%r359, 0;
	setp.gt.u32 	%p6, %r3, 15;
	mov.u32 	%r1355, %r359;
LBB0_7:                                 // %L2684
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_8 Depth 2
                                        //       Child Loop BB0_11 Depth 3
	mov.u32 	%r1356, %r359;
	mov.u32 	%r1357, %r359;
	mov.u32 	%r1358, %r359;
	mov.u32 	%r1359, %r359;
	mov.u32 	%r1360, %r359;
	mov.u32 	%r1361, %r359;
	mov.u32 	%r1362, %r359;
LBB0_8:                                 // %L2687
                                        //   Parent Loop BB0_7 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB0_11 Depth 3
	@%p6 bra 	LBB0_10;
// %bb.9:                               // %L2710
                                        //   in Loop: Header=BB0_8 Depth=2
	or.b32  	%r561, %r1356, %r1355;
	or.b32  	%r562, %r561, %r48;
	or.b32  	%r563, %r562, %r5;
	or.b32  	%r564, %r563, %r49;
	shl.b32 	%r565, %r564, 12;
	or.b32  	%r566, %r565, %r47;
	or.b32  	%r567, %r566, %r51;
	or.b32  	%r568, %r53, %r567;
	mul.wide.s32 	%rd73, %r568, 4;
	add.s64 	%rd74, %rd2, %rd73;
	ld.global.v4.u32 	{%r569, %r570, %r571, %r572}, [%rd74];
	or.b32  	%r573, %r567, %r50;
	or.b32  	%r574, %r573, %r52;
	or.b32  	%r575, %r574, 65536;
	mul.wide.s32 	%rd75, %r575, 4;
	add.s64 	%rd76, %rd2, %rd75;
	ld.global.v4.u32 	{%r576, %r577, %r578, %r579}, [%rd76];
	st.shared.u32 	[%rd7], %r569;
	st.shared.u32 	[%rd8+4], %r570;
	st.shared.u32 	[%rd8+8], %r571;
	st.shared.u32 	[%rd8+12], %r572;
	st.shared.u32 	[%rd9], %r576;
	st.shared.u32 	[%rd10+4], %r577;
	st.shared.u32 	[%rd10+8], %r578;
	st.shared.u32 	[%rd10+12], %r579;
LBB0_10:                                // %L4794
                                        //   in Loop: Header=BB0_8 Depth=2
	bar.sync 	0;
	mov.u32 	%r593, 0;
	mov.u32 	%r1363, %r593;
LBB0_11:                                // %L4796
                                        //   Parent Loop BB0_7 Depth=1
                                        //     Parent Loop BB0_8 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	or.b32  	%r1105, %r11, %r1363;
	mul.lo.s32 	%r1106, %r1105, 129;
	add.s32 	%r1107, %r54, %r1106;
	mul.wide.u32 	%rd77, %r1107, 4;
	add.s64 	%rd79, %rd46, %rd77;
	ld.shared.u32 	%r582, [%rd79];
	mov.u32 	%r583, 134744072;
	mov.u32 	%r584, 252645135;
	// begin inline asm
	lop3.b32 %r581, %r582, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1108, %r581, 2021161080;
	xor.b32  	%r592, %r1108, -2139062144;
	shr.u32 	%r586, %r582, 4;
	// begin inline asm
	lop3.b32 %r585, %r586, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1109, %r585, 2021161080;
	xor.b32  	%r598, %r1109, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r589, %r590}, {%r591}, {%r592}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r595, %r596}, {%r597}, {%r598}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r601, %r602}, {%r591}, {%r598}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r607, %r608}, {%r597}, {%r592}, {%r601, %r602};
	// end inline asm
	cvt.u64.u32 	%rd80, %r1106;
	cvt.u64.u32 	%rd81, %r54;
	add.s64 	%rd82, %rd81, %rd80;
	shl.b64 	%rd83, %rd82, 2;
	add.s64 	%rd84, %rd46, %rd83;
	ld.shared.u32 	%r614, [%rd84+4];
	// begin inline asm
	lop3.b32 %r613, %r614, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1110, %r613, 2021161080;
	xor.b32  	%r624, %r1110, -2139062144;
	shr.u32 	%r618, %r614, 4;
	// begin inline asm
	lop3.b32 %r617, %r618, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1111, %r617, 2021161080;
	xor.b32  	%r630, %r1111, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r621, %r622}, {%r623}, {%r624}, {%r589, %r590};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r627, %r628}, {%r629}, {%r630}, {%r595, %r596};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r633, %r634}, {%r623}, {%r630}, {%r607, %r608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r639, %r640}, {%r629}, {%r624}, {%r633, %r634};
	// end inline asm
	ld.shared.u32 	%r646, [%rd84+8];
	// begin inline asm
	lop3.b32 %r645, %r646, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1112, %r645, 2021161080;
	xor.b32  	%r656, %r1112, -2139062144;
	shr.u32 	%r650, %r646, 4;
	// begin inline asm
	lop3.b32 %r649, %r650, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1113, %r649, 2021161080;
	xor.b32  	%r662, %r1113, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r653, %r654}, {%r655}, {%r656}, {%r621, %r622};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r659, %r660}, {%r661}, {%r662}, {%r627, %r628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r665, %r666}, {%r655}, {%r662}, {%r639, %r640};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r671, %r672}, {%r661}, {%r656}, {%r665, %r666};
	// end inline asm
	ld.shared.u32 	%r678, [%rd84+12];
	// begin inline asm
	lop3.b32 %r677, %r678, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1114, %r677, 2021161080;
	xor.b32  	%r688, %r1114, -2139062144;
	shr.u32 	%r682, %r678, 4;
	// begin inline asm
	lop3.b32 %r681, %r682, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1115, %r681, 2021161080;
	xor.b32  	%r694, %r1115, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r685, %r686}, {%r687}, {%r688}, {%r653, %r654};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r691, %r692}, {%r693}, {%r694}, {%r659, %r660};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r697, %r698}, {%r687}, {%r694}, {%r671, %r672};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r703, %r704}, {%r693}, {%r688}, {%r697, %r698};
	// end inline asm
	ld.shared.u32 	%r710, [%rd84+16];
	// begin inline asm
	lop3.b32 %r709, %r710, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1116, %r709, 2021161080;
	xor.b32  	%r720, %r1116, -2139062144;
	shr.u32 	%r714, %r710, 4;
	// begin inline asm
	lop3.b32 %r713, %r714, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1117, %r713, 2021161080;
	xor.b32  	%r726, %r1117, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r717, %r718}, {%r719}, {%r720}, {%r685, %r686};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r723, %r724}, {%r725}, {%r726}, {%r691, %r692};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r729, %r730}, {%r719}, {%r726}, {%r703, %r704};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r735, %r736}, {%r725}, {%r720}, {%r729, %r730};
	// end inline asm
	ld.shared.u32 	%r742, [%rd84+20];
	// begin inline asm
	lop3.b32 %r741, %r742, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1118, %r741, 2021161080;
	xor.b32  	%r752, %r1118, -2139062144;
	shr.u32 	%r746, %r742, 4;
	// begin inline asm
	lop3.b32 %r745, %r746, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1119, %r745, 2021161080;
	xor.b32  	%r758, %r1119, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r749, %r750}, {%r751}, {%r752}, {%r717, %r718};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r755, %r756}, {%r757}, {%r758}, {%r723, %r724};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r761, %r762}, {%r751}, {%r758}, {%r735, %r736};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r767, %r768}, {%r757}, {%r752}, {%r761, %r762};
	// end inline asm
	ld.shared.u32 	%r774, [%rd84+24];
	// begin inline asm
	lop3.b32 %r773, %r774, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1120, %r773, 2021161080;
	xor.b32  	%r784, %r1120, -2139062144;
	shr.u32 	%r778, %r774, 4;
	// begin inline asm
	lop3.b32 %r777, %r778, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1121, %r777, 2021161080;
	xor.b32  	%r790, %r1121, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r781, %r782}, {%r783}, {%r784}, {%r749, %r750};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r787, %r788}, {%r789}, {%r790}, {%r755, %r756};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r793, %r794}, {%r783}, {%r790}, {%r767, %r768};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r799, %r800}, {%r789}, {%r784}, {%r793, %r794};
	// end inline asm
	ld.shared.u32 	%r806, [%rd84+28];
	// begin inline asm
	lop3.b32 %r805, %r806, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1122, %r805, 2021161080;
	xor.b32  	%r816, %r1122, -2139062144;
	shr.u32 	%r810, %r806, 4;
	// begin inline asm
	lop3.b32 %r809, %r810, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1123, %r809, 2021161080;
	xor.b32  	%r822, %r1123, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r813, %r814}, {%r815}, {%r816}, {%r781, %r782};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r819, %r820}, {%r821}, {%r822}, {%r787, %r788};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r825, %r826}, {%r815}, {%r822}, {%r799, %r800};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r831, %r832}, {%r821}, {%r816}, {%r825, %r826};
	// end inline asm
	sub.s32 	%r1124, %r813, %r819;
	add.s32 	%r1125, %r1124, 4;
	shr.s32 	%r839, %r1125, 3;
	add.s32 	%r1126, %r831, 4;
	shr.s32 	%r838, %r1126, 3;
	sub.s32 	%r1127, %r814, %r820;
	add.s32 	%r1128, %r1127, 4;
	shr.s32 	%r842, %r1128, 3;
	add.s32 	%r1129, %r832, 4;
	shr.s32 	%r841, %r1129, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r837, %r838, %r839;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r840, %r841, %r842;
	// end inline asm
	or.b32  	%r1130, %r56, %r1363;
	mul.lo.s32 	%r1131, %r1130, 100;
	add.s32 	%r1132, %r1131, %r55;
	add.s32 	%r1133, %r1132, %r57;
	mul.wide.u32 	%rd85, %r1133, 4;
	add.s64 	%rd87, %rd56, %rd85;
	st.shared.u32 	[%rd87], %r837;
	add.s32 	%r1134, %r1131, 100;
	add.s32 	%r1135, %r58, %r1134;
	mul.wide.u32 	%rd88, %r1135, 4;
	add.s64 	%rd89, %rd56, %rd88;
	st.shared.u32 	[%rd89], %r840;
	ld.shared.u32 	%r844, [%rd79];
	// begin inline asm
	lop3.b32 %r843, %r844, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1136, %r843, 2021161080;
	xor.b32  	%r854, %r1136, -2139062144;
	shr.u32 	%r848, %r844, 4;
	// begin inline asm
	lop3.b32 %r847, %r848, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1137, %r847, 2021161080;
	xor.b32  	%r860, %r1137, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r851, %r852}, {%r853}, {%r854}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r857, %r858}, {%r859}, {%r860}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r863, %r864}, {%r853}, {%r860}, {%r593, %r593};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r869, %r870}, {%r859}, {%r854}, {%r863, %r864};
	// end inline asm
	ld.shared.u32 	%r876, [%rd84+4];
	// begin inline asm
	lop3.b32 %r875, %r876, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1138, %r875, 2021161080;
	xor.b32  	%r886, %r1138, -2139062144;
	shr.u32 	%r880, %r876, 4;
	// begin inline asm
	lop3.b32 %r879, %r880, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1139, %r879, 2021161080;
	xor.b32  	%r892, %r1139, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r883, %r884}, {%r885}, {%r886}, {%r851, %r852};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r889, %r890}, {%r891}, {%r892}, {%r857, %r858};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r895, %r896}, {%r885}, {%r892}, {%r869, %r870};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r901, %r902}, {%r891}, {%r886}, {%r895, %r896};
	// end inline asm
	ld.shared.u32 	%r908, [%rd84+8];
	// begin inline asm
	lop3.b32 %r907, %r908, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1140, %r907, 2021161080;
	xor.b32  	%r918, %r1140, -2139062144;
	shr.u32 	%r912, %r908, 4;
	// begin inline asm
	lop3.b32 %r911, %r912, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1141, %r911, 2021161080;
	xor.b32  	%r924, %r1141, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r915, %r916}, {%r917}, {%r918}, {%r883, %r884};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r921, %r922}, {%r923}, {%r924}, {%r889, %r890};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r927, %r928}, {%r917}, {%r924}, {%r901, %r902};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r933, %r934}, {%r923}, {%r918}, {%r927, %r928};
	// end inline asm
	ld.shared.u32 	%r940, [%rd84+12];
	// begin inline asm
	lop3.b32 %r939, %r940, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1142, %r939, 2021161080;
	xor.b32  	%r950, %r1142, -2139062144;
	shr.u32 	%r944, %r940, 4;
	// begin inline asm
	lop3.b32 %r943, %r944, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1143, %r943, 2021161080;
	xor.b32  	%r956, %r1143, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r947, %r948}, {%r949}, {%r950}, {%r915, %r916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r953, %r954}, {%r955}, {%r956}, {%r921, %r922};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r959, %r960}, {%r949}, {%r956}, {%r933, %r934};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r965, %r966}, {%r955}, {%r950}, {%r959, %r960};
	// end inline asm
	ld.shared.u32 	%r972, [%rd84+16];
	// begin inline asm
	lop3.b32 %r971, %r972, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1144, %r971, 2021161080;
	xor.b32  	%r982, %r1144, -2139062144;
	shr.u32 	%r976, %r972, 4;
	// begin inline asm
	lop3.b32 %r975, %r976, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1145, %r975, 2021161080;
	xor.b32  	%r988, %r1145, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r979, %r980}, {%r981}, {%r982}, {%r947, %r948};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r985, %r986}, {%r987}, {%r988}, {%r953, %r954};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r991, %r992}, {%r981}, {%r988}, {%r965, %r966};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r997, %r998}, {%r987}, {%r982}, {%r991, %r992};
	// end inline asm
	ld.shared.u32 	%r1004, [%rd84+20];
	// begin inline asm
	lop3.b32 %r1003, %r1004, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1146, %r1003, 2021161080;
	xor.b32  	%r1014, %r1146, -2139062144;
	shr.u32 	%r1008, %r1004, 4;
	// begin inline asm
	lop3.b32 %r1007, %r1008, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1147, %r1007, 2021161080;
	xor.b32  	%r1020, %r1147, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1011, %r1012}, {%r1013}, {%r1014}, {%r979, %r980};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1017, %r1018}, {%r1019}, {%r1020}, {%r985, %r986};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1023, %r1024}, {%r1013}, {%r1020}, {%r997, %r998};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1029, %r1030}, {%r1019}, {%r1014}, {%r1023, %r1024};
	// end inline asm
	ld.shared.u32 	%r1036, [%rd84+24];
	// begin inline asm
	lop3.b32 %r1035, %r1036, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1148, %r1035, 2021161080;
	xor.b32  	%r1046, %r1148, -2139062144;
	shr.u32 	%r1040, %r1036, 4;
	// begin inline asm
	lop3.b32 %r1039, %r1040, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1149, %r1039, 2021161080;
	xor.b32  	%r1052, %r1149, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1043, %r1044}, {%r1045}, {%r1046}, {%r1011, %r1012};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1049, %r1050}, {%r1051}, {%r1052}, {%r1017, %r1018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1055, %r1056}, {%r1045}, {%r1052}, {%r1029, %r1030};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1061, %r1062}, {%r1051}, {%r1046}, {%r1055, %r1056};
	// end inline asm
	ld.shared.u32 	%r1068, [%rd84+28];
	// begin inline asm
	lop3.b32 %r1067, %r1068, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1150, %r1067, 2021161080;
	xor.b32  	%r1078, %r1150, -2139062144;
	shr.u32 	%r1072, %r1068, 4;
	// begin inline asm
	lop3.b32 %r1071, %r1072, %r583, %r584, 40;
	// end inline asm
	add.s32 	%r1151, %r1071, 2021161080;
	xor.b32  	%r1084, %r1151, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1075, %r1076}, {%r1077}, {%r1078}, {%r1043, %r1044};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1081, %r1082}, {%r1083}, {%r1084}, {%r1049, %r1050};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1087, %r1088}, {%r1077}, {%r1084}, {%r1061, %r1062};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1093, %r1094}, {%r1083}, {%r1078}, {%r1087, %r1088};
	// end inline asm
	sub.s32 	%r1152, %r1075, %r1081;
	add.s32 	%r1153, %r1152, 4;
	shr.s32 	%r1101, %r1153, 3;
	add.s32 	%r1154, %r1093, 4;
	shr.s32 	%r1100, %r1154, 3;
	sub.s32 	%r1155, %r1076, %r1082;
	add.s32 	%r1156, %r1155, 4;
	shr.s32 	%r1104, %r1156, 3;
	add.s32 	%r1157, %r1094, 4;
	shr.s32 	%r1103, %r1157, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1099, %r1100, %r1101;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1102, %r1103, %r1104;
	// end inline asm
	add.s32 	%r1158, %r1131, %r59;
	add.s32 	%r1159, %r1158, %r57;
	mul.wide.u32 	%rd90, %r1159, 4;
	add.s64 	%rd91, %rd56, %rd90;
	st.shared.u32 	[%rd91], %r1099;
	add.s32 	%r1160, %r60, %r1134;
	mul.wide.u32 	%rd92, %r1160, 4;
	add.s64 	%rd93, %rd56, %rd92;
	st.shared.u32 	[%rd93], %r1102;
	add.s32 	%r1363, %r1363, 8;
	setp.ne.s32 	%p7, %r1363, 32;
	@%p7 bra 	LBB0_11;
// %bb.12:                              // %L9413
                                        //   in Loop: Header=BB0_8 Depth=2
	bar.sync 	0;
	ld.shared.u32 	%r1179, [%rd11];
	ld.shared.u32 	%r1180, [%rd12+12800];
	ld.shared.u32 	%r1181, [%rd12+25600];
	ld.shared.u32 	%r1182, [%rd12+38400];
	ld.shared.u32 	%r1183, [%rd13];
	ld.shared.u32 	%r1184, [%rd14+12800];
	ld.shared.u32 	%r1185, [%rd14+25600];
	ld.shared.u32 	%r1186, [%rd14+38400];
	ld.shared.u32 	%r1187, [%rd15];
	ld.shared.u32 	%r1188, [%rd16+12800];
	ld.shared.u32 	%r1189, [%rd16+25600];
	ld.shared.u32 	%r1190, [%rd16+38400];
	ld.shared.u32 	%r1191, [%rd17];
	ld.shared.u32 	%r1192, [%rd18+12800];
	ld.shared.u32 	%r1193, [%rd18+25600];
	ld.shared.u32 	%r1194, [%rd18+38400];
	cvt.s32.s16 	%r1195, %r1179;
	shr.s32 	%r1196, %r1179, 16;
	cvt.s32.s16 	%r1197, %r1180;
	shr.s32 	%r1198, %r1180, 16;
	cvt.s32.s16 	%r1199, %r1181;
	shr.s32 	%r1200, %r1181, 16;
	cvt.s32.s16 	%r1201, %r1182;
	shr.s32 	%r1202, %r1182, 16;
	cvt.s32.s16 	%r1203, %r1183;
	shr.s32 	%r1204, %r1183, 16;
	cvt.s32.s16 	%r1205, %r1184;
	shr.s32 	%r1206, %r1184, 16;
	cvt.s32.s16 	%r1207, %r1185;
	shr.s32 	%r1208, %r1185, 16;
	cvt.s32.s16 	%r1209, %r1186;
	shr.s32 	%r1210, %r1186, 16;
	cvt.s32.s16 	%r1211, %r1187;
	shr.s32 	%r1212, %r1187, 16;
	cvt.s32.s16 	%r1213, %r1188;
	shr.s32 	%r1214, %r1188, 16;
	cvt.s32.s16 	%r1215, %r1189;
	shr.s32 	%r1216, %r1189, 16;
	cvt.s32.s16 	%r1217, %r1190;
	shr.s32 	%r1218, %r1190, 16;
	cvt.s32.s16 	%r1219, %r1191;
	shr.s32 	%r1220, %r1191, 16;
	cvt.s32.s16 	%r1221, %r1192;
	shr.s32 	%r1222, %r1192, 16;
	cvt.s32.s16 	%r1223, %r1193;
	shr.s32 	%r1224, %r1193, 16;
	cvt.s32.s16 	%r1225, %r1194;
	shr.s32 	%r1226, %r1194, 16;
	add.s32 	%r1227, %r1195, %r61;
	add.s32 	%r1228, %r1227, %r1197;
	add.s32 	%r1229, %r1228, %r1199;
	add.s32 	%r1230, %r1229, %r1201;
	shr.s32 	%r1231, %r1230, %r62;
	add.s32 	%r1232, %r1196, %r61;
	add.s32 	%r1233, %r1232, %r1198;
	add.s32 	%r1234, %r1233, %r1200;
	add.s32 	%r1235, %r1234, %r1202;
	shr.s32 	%r1236, %r1235, %r62;
	add.s32 	%r1237, %r1203, %r61;
	add.s32 	%r1238, %r1237, %r1205;
	add.s32 	%r1239, %r1238, %r1207;
	add.s32 	%r1240, %r1239, %r1209;
	shr.s32 	%r1241, %r1240, %r62;
	add.s32 	%r1242, %r1204, %r61;
	add.s32 	%r1243, %r1242, %r1206;
	add.s32 	%r1244, %r1243, %r1208;
	add.s32 	%r1245, %r1244, %r1210;
	shr.s32 	%r1246, %r1245, %r62;
	add.s32 	%r1247, %r1211, %r61;
	add.s32 	%r1248, %r1247, %r1213;
	add.s32 	%r1249, %r1248, %r1215;
	add.s32 	%r1250, %r1249, %r1217;
	shr.s32 	%r1251, %r1250, %r62;
	add.s32 	%r1252, %r1212, %r61;
	add.s32 	%r1253, %r1252, %r1214;
	add.s32 	%r1254, %r1253, %r1216;
	add.s32 	%r1255, %r1254, %r1218;
	shr.s32 	%r1256, %r1255, %r62;
	add.s32 	%r1257, %r1219, %r61;
	add.s32 	%r1258, %r1257, %r1221;
	add.s32 	%r1259, %r1258, %r1223;
	add.s32 	%r1260, %r1259, %r1225;
	shr.s32 	%r1261, %r1260, %r62;
	add.s32 	%r1262, %r1220, %r61;
	add.s32 	%r1263, %r1262, %r1222;
	add.s32 	%r1264, %r1263, %r1224;
	add.s32 	%r1265, %r1264, %r1226;
	shr.s32 	%r1266, %r1265, %r62;
	max.s32 	%r1267, %r1231, -7;
	min.s32 	%r1166, %r1267, 7;
	max.s32 	%r1268, %r1236, -7;
	min.s32 	%r1173, %r1268, 7;
	max.s32 	%r1269, %r1241, -7;
	min.s32 	%r1165, %r1269, 7;
	max.s32 	%r1270, %r1246, -7;
	min.s32 	%r1172, %r1270, 7;
	max.s32 	%r1271, %r1251, -7;
	min.s32 	%r1163, %r1271, 7;
	max.s32 	%r1272, %r1256, -7;
	min.s32 	%r1170, %r1272, 7;
	max.s32 	%r1273, %r1261, -7;
	min.s32 	%r1162, %r1273, 7;
	max.s32 	%r1274, %r1266, -7;
	min.s32 	%r1169, %r1274, 7;
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r1161, %r1162, %r1163, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r1164, %r1165, %r1166, %r1161;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r1168, %r1169, %r1170, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r1171, %r1172, %r1173, %r1168;
	// end inline asm
	shl.b32 	%r1178, %r1171, 4;
	// begin inline asm
	lop3.b32 %r1285, %r584, %r1164, %r1178, 202;
	// end inline asm
	setp.eq.s32 	%p8, %r1356, 0;
	selp.b32 	%r1361, %r1285, %r1361, %p8;
	selp.b32 	%r1362, %r1285, %r1362, %p8;
	setp.eq.s32 	%p9, %r1356, 32;
	selp.b32 	%r1357, %r1285, %r1357, %p9;
	selp.b32 	%r1358, %r1285, %r1358, %p9;
	setp.eq.s32 	%p10, %r1356, 64;
	selp.b32 	%r1359, %r1285, %r1359, %p10;
	selp.b32 	%r1360, %r1285, %r1360, %p10;
	add.s32 	%r86, %r1356, 32;
	setp.ne.s32 	%p11, %r1356, 96;
	mov.u32 	%r1356, %r86;
	@%p11 bra 	LBB0_8;
// %bb.13:                              // %L12444
                                        //   in Loop: Header=BB0_7 Depth=1
	setp.eq.s32 	%p12, %r63, 0;
	// begin inline asm
	prmt.b32 %r1275, %r1361, %r1357, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1279, %r1362, %r1358, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1283, %r1359, %r1285, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1287, %r1360, %r1285, %r238;
	// end inline asm
	selp.b32 	%r1323, %r1279, %r1275, %p12;
	shfl.sync.bfly.b32	%r1324, %r1323, 1, 31, -1;
	selp.b32 	%r1292, %r1275, %r1324, %p12;
	selp.b32 	%r1293, %r1324, %r1279, %p12;
	selp.b32 	%r1325, %r1287, %r1283, %p12;
	shfl.sync.bfly.b32	%r1326, %r1325, 1, 31, -1;
	selp.b32 	%r1300, %r1283, %r1326, %p12;
	selp.b32 	%r1301, %r1326, %r1287, %p12;
	// begin inline asm
	prmt.b32 %r1291, %r1292, %r1293, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1295, %r1292, %r1293, %r238;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1299, %r1300, %r1301, %r234;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1303, %r1300, %r1301, %r238;
	// end inline asm
	selp.b32 	%r1327, %r1299, %r1291, %p4;
	shfl.sync.bfly.b32	%r1328, %r1327, 2, 31, -1;
	selp.b32 	%r1308, %r1291, %r1328, %p4;
	selp.b32 	%r1309, %r1328, %r1299, %p4;
	selp.b32 	%r1329, %r1303, %r1295, %p4;
	shfl.sync.bfly.b32	%r1330, %r1329, 2, 31, -1;
	selp.b32 	%r1316, %r1295, %r1330, %p4;
	selp.b32 	%r1317, %r1330, %r1303, %p4;
	// begin inline asm
	prmt.b32 %r1307, %r1308, %r1309, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1311, %r1308, %r1309, %r110;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1315, %r1316, %r1317, %r106;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1319, %r1316, %r1317, %r110;
	// end inline asm
	selp.b32 	%r1331, %r1315, %r1307, %p5;
	shfl.sync.bfly.b32	%r1332, %r1331, 4, 31, -1;
	selp.b32 	%r1333, %r1307, %r1332, %p5;
	selp.b32 	%r1334, %r1332, %r1315, %p5;
	selp.b32 	%r1335, %r1319, %r1311, %p5;
	shfl.sync.bfly.b32	%r1336, %r1335, 4, 31, -1;
	selp.b32 	%r1337, %r1311, %r1336, %p5;
	selp.b32 	%r1338, %r1336, %r1319, %p5;
	selp.b32 	%r1339, %r1337, %r1333, %p12;
	shfl.sync.bfly.b32	%r1340, %r1339, 1, 31, -1;
	selp.b32 	%r1341, %r1333, %r1340, %p12;
	selp.b32 	%r1342, %r1340, %r1337, %p12;
	selp.b32 	%r1343, %r1338, %r1334, %p12;
	shfl.sync.bfly.b32	%r1344, %r1343, 1, 31, -1;
	selp.b32 	%r1345, %r1334, %r1344, %p12;
	selp.b32 	%r1346, %r1344, %r1338, %p12;
	or.b32  	%r1347, %r48, %r1355;
	or.b32  	%r1348, %r1347, %r68;
	or.b32  	%r1349, %r1348, %r12;
	or.b32  	%r1350, %r1349, %r64;
	or.b32  	%r1351, %r1350, %r65;
	or.b32  	%r1352, %r1351, %r66;
	or.b32  	%r1353, %r1352, %r67;
	cvt.u64.u32 	%rd94, %r1353;
	add.s64 	%rd95, %rd4, %rd94;
	st.global.v4.u32 	[%rd95], {%r1341, %r1345, %r1342, %r1346};
	add.s32 	%r87, %r1355, 128;
	setp.ne.s32 	%p15, %r1355, 1920;
	mov.u32 	%r1355, %r87;
	@%p15 bra 	LBB0_7;
// %bb.14:                              // %L12899
	mov.u32 	%r1354, 0;
	st.global.u32 	[%rd6], %r1354;
	ret;
LBB0_1:                                 // %L9
	mov.u64 	%rd20, exception1;
	cvta.global.u64 	%rd21, %rd20;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd21;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 0
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd19;
	st.param.b32 	[param0+8], %r88;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 1
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
LBB0_3:                                 // %L29
	mov.u64 	%rd22, exception1;
	cvta.global.u64 	%rd23, %rd22;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd23;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 2
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd19;
	st.param.b32 	[param0+8], %r88;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 3
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
LBB0_5:                                 // %L284
	mov.u32 	%r102, 2;
	st.global.u32 	[%rd6], %r102;
	mov.u64 	%rd27, exception925;
	cvta.global.u64 	%rd28, %rd27;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd28;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd19;
	st.param.b32 	[param0+8], %r88;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}

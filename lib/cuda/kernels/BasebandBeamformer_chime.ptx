// PTX kernel code for CUDA baseband beamformer
// This file has been generated automatically by `bb.jl`.
// Do not modify this file, your changes will be lost.

// PTX CompilerJob of MethodInstance for bb(::Int32, ::Int32, ::CuDeviceVector{Int8x4, 1}, ::CuDeviceVector{Int4x8, 1}, ::CuDeviceVector{Int32, 1}, ::CuDeviceVector{Int4x8, 1}, ::CuDeviceVector{Int32, 1}, ::CuDeviceVector{Int32, 1}) for sm_86, minthreads=128, blocks_per_sm=8

//
// Generated by LLVM NVPTX Back-End
//

.version 8.2
.target sm_86
.address_size 64

	// .globl	_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE // -- Begin function _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE
.func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
.noreturn
{
	trap;
}
.func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[16]
)
.noreturn
{
	trap;
}
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception2743[6] = {101, 114, 114, 111, 114, 0};
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE
.visible .entry _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE(
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_0[16],
	.param .u32 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_1,
	.param .u32 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_2,
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_4[32],
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_5[32],
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_6[32],
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_7[32],
	.param .align 8 .b8 _Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_8[32]
)
.reqntid 128, 1, 1
.minnctapersm 8
{
	.reg .pred 	%p<70>;
	.reg .b32 	%r<2420>;
	.reg .b64 	%rd<344>;

// %bb.0:                               // %conversion
	ld.param.u32 	%r119, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_0+8];
	ld.param.u64 	%rd32, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_0];
	// begin inline asm
	mov.u32 %r122, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p6, %r122, 32895;
	@%p6 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:                              // %L10
	// begin inline asm
	mov.u32 %r123, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p7, %r123, 43135;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;
$L__BB0_4:                              // %L26
	ld.param.u32 	%r120, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_1];
	ld.param.u64 	%rd5, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_7];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r124, %r1, 7;
	mov.u32 	%r2, %tid.y;
	shl.b32 	%r3, %r2, 5;
	mov.u32 	%r4, %tid.x;
	or.b32  	%r125, %r3, %r4;
	or.b32  	%r126, %r125, %r124;
	mul.wide.u32 	%rd37, %r126, 4;
	add.s64 	%rd7, %rd5, %rd37;
	mov.u32 	%r127, 1;
	st.global.u32 	[%rd7], %r127;
	setp.gt.u32 	%p8, %r120, 65535;
	@%p8 bra 	$L__BB0_7;
// %bb.5:                               // %L133
	ld.param.u32 	%r121, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_2];
	setp.lt.s32 	%p9, %r121, %r120;
	setp.gt.s32 	%p10, %r121, 131071;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB0_7;
// %bb.6:                               // %L140
	sub.s32 	%r128, %r121, %r120;
	and.b32  	%r129, %r128, 127;
	setp.eq.s32 	%p12, %r129, 0;
	@%p12 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_7;
$L__BB0_20:                             // %pass120
	ld.param.u64 	%rd3, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_5];
	shl.b32 	%r130, %r1, 4;
	shl.b32 	%r131, %r2, 2;
	shr.u32 	%r115, %r4, 3;
	or.b32  	%r116, %r131, %r115;
	or.b32  	%r132, %r116, %r130;
	mul.wide.u32 	%rd38, %r132, 4;
	add.s64 	%rd39, %rd3, %rd38;
	ld.global.u32 	%r133, [%rd39];
	add.s32 	%r118, %r133, -5;
	setp.lt.u32 	%p13, %r118, 31;
	@%p13 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_21;
$L__BB0_8:                              // %L463
	ld.param.u64 	%rd1, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_3];
	ld.param.u64 	%rd6, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_8];
	add.s32 	%r117, %r133, -4;
	or.b32  	%r135, %r4, %r2;
	setp.ne.s32 	%p1, %r135, 0;
	mul.wide.u32 	%rd42, %r1, 4;
	add.s64 	%rd8, %rd6, %rd42;
	mov.u32 	%r2411, 0;
	@%p1 bra 	$L__BB0_10;
// %bb.9:                               // %L487
	st.global.u32 	[%rd8], %r2411;
$L__BB0_10:                             // %L533
	ld.param.u64 	%rd2, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_4];
	ld.param.u64 	%rd4, [_Z2bb5Int32S_13CuDeviceArrayI6Int8x4Li1ELi1EES0_I6Int4x8Li1ELi1EES0_IS_Li1ELi1EES0_IS2_Li1ELi1EES0_IS_Li1ELi1EES0_IS_Li1ELi1EE_param_6];
	bar.sync 	0;
	shl.b32 	%r650, %r4, 7;
	and.b32  	%r651, %r650, 3584;
	shl.b32 	%r652, %r1, 13;
	shl.b32 	%r653, %r2, 7;
	shl.b32 	%r654, %r4, 5;
	and.b32  	%r655, %r654, 96;
	or.b32  	%r656, %r655, %r653;
	or.b32  	%r657, %r651, %r652;
	or.b32  	%r658, %r657, %r656;
	mul.wide.u32 	%rd43, %r658, 4;
	add.s64 	%rd44, %rd1, %rd43;
	ld.global.u32 	%r142, [%rd44];
	or.b32  	%r659, %r658, 4096;
	mul.wide.u32 	%rd45, %r659, 4;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.u32 	%r150, [%rd46];
	or.b32  	%r660, %r658, 1;
	mul.wide.u32 	%rd47, %r660, 4;
	add.s64 	%rd48, %rd1, %rd47;
	ld.global.u32 	%r143, [%rd48];
	or.b32  	%r661, %r658, 4097;
	mul.wide.u32 	%rd49, %r661, 4;
	add.s64 	%rd50, %rd1, %rd49;
	ld.global.u32 	%r151, [%rd50];
	or.b32  	%r662, %r658, 2;
	mul.wide.u32 	%rd51, %r662, 4;
	add.s64 	%rd52, %rd1, %rd51;
	ld.global.u32 	%r158, [%rd52];
	or.b32  	%r663, %r658, 4098;
	mul.wide.u32 	%rd53, %r663, 4;
	add.s64 	%rd54, %rd1, %rd53;
	ld.global.u32 	%r166, [%rd54];
	or.b32  	%r664, %r658, 3;
	mul.wide.u32 	%rd55, %r664, 4;
	add.s64 	%rd56, %rd1, %rd55;
	ld.global.u32 	%r159, [%rd56];
	or.b32  	%r665, %r658, 4099;
	mul.wide.u32 	%rd57, %r665, 4;
	add.s64 	%rd58, %rd1, %rd57;
	ld.global.u32 	%r167, [%rd58];
	or.b32  	%r666, %r658, 4;
	mul.wide.u32 	%rd59, %r666, 4;
	add.s64 	%rd60, %rd1, %rd59;
	ld.global.u32 	%r174, [%rd60];
	or.b32  	%r667, %r658, 4100;
	mul.wide.u32 	%rd61, %r667, 4;
	add.s64 	%rd62, %rd1, %rd61;
	ld.global.u32 	%r182, [%rd62];
	or.b32  	%r668, %r658, 5;
	mul.wide.u32 	%rd63, %r668, 4;
	add.s64 	%rd64, %rd1, %rd63;
	ld.global.u32 	%r175, [%rd64];
	or.b32  	%r669, %r658, 4101;
	mul.wide.u32 	%rd65, %r669, 4;
	add.s64 	%rd66, %rd1, %rd65;
	ld.global.u32 	%r183, [%rd66];
	or.b32  	%r670, %r658, 6;
	mul.wide.u32 	%rd67, %r670, 4;
	add.s64 	%rd68, %rd1, %rd67;
	ld.global.u32 	%r190, [%rd68];
	or.b32  	%r671, %r658, 4102;
	mul.wide.u32 	%rd69, %r671, 4;
	add.s64 	%rd70, %rd1, %rd69;
	ld.global.u32 	%r198, [%rd70];
	or.b32  	%r672, %r658, 7;
	mul.wide.u32 	%rd71, %r672, 4;
	add.s64 	%rd72, %rd1, %rd71;
	ld.global.u32 	%r191, [%rd72];
	or.b32  	%r673, %r658, 4103;
	mul.wide.u32 	%rd73, %r673, 4;
	add.s64 	%rd74, %rd1, %rd73;
	ld.global.u32 	%r199, [%rd74];
	or.b32  	%r674, %r658, 8;
	mul.wide.u32 	%rd75, %r674, 4;
	add.s64 	%rd76, %rd1, %rd75;
	ld.global.u32 	%r206, [%rd76];
	or.b32  	%r675, %r658, 4104;
	mul.wide.u32 	%rd77, %r675, 4;
	add.s64 	%rd78, %rd1, %rd77;
	ld.global.u32 	%r214, [%rd78];
	or.b32  	%r676, %r658, 9;
	mul.wide.u32 	%rd79, %r676, 4;
	add.s64 	%rd80, %rd1, %rd79;
	ld.global.u32 	%r207, [%rd80];
	or.b32  	%r677, %r658, 4105;
	mul.wide.u32 	%rd81, %r677, 4;
	add.s64 	%rd82, %rd1, %rd81;
	ld.global.u32 	%r215, [%rd82];
	or.b32  	%r678, %r658, 10;
	mul.wide.u32 	%rd83, %r678, 4;
	add.s64 	%rd84, %rd1, %rd83;
	ld.global.u32 	%r222, [%rd84];
	or.b32  	%r679, %r658, 4106;
	mul.wide.u32 	%rd85, %r679, 4;
	add.s64 	%rd86, %rd1, %rd85;
	ld.global.u32 	%r230, [%rd86];
	or.b32  	%r680, %r658, 11;
	mul.wide.u32 	%rd87, %r680, 4;
	add.s64 	%rd88, %rd1, %rd87;
	ld.global.u32 	%r223, [%rd88];
	or.b32  	%r681, %r658, 4107;
	mul.wide.u32 	%rd89, %r681, 4;
	add.s64 	%rd90, %rd1, %rd89;
	ld.global.u32 	%r231, [%rd90];
	or.b32  	%r682, %r658, 12;
	mul.wide.u32 	%rd91, %r682, 4;
	add.s64 	%rd92, %rd1, %rd91;
	ld.global.u32 	%r238, [%rd92];
	or.b32  	%r683, %r658, 4108;
	mul.wide.u32 	%rd93, %r683, 4;
	add.s64 	%rd94, %rd1, %rd93;
	ld.global.u32 	%r246, [%rd94];
	or.b32  	%r684, %r658, 13;
	mul.wide.u32 	%rd95, %r684, 4;
	add.s64 	%rd96, %rd1, %rd95;
	ld.global.u32 	%r239, [%rd96];
	or.b32  	%r685, %r658, 4109;
	mul.wide.u32 	%rd97, %r685, 4;
	add.s64 	%rd98, %rd1, %rd97;
	ld.global.u32 	%r247, [%rd98];
	or.b32  	%r686, %r658, 14;
	mul.wide.u32 	%rd99, %r686, 4;
	add.s64 	%rd100, %rd1, %rd99;
	ld.global.u32 	%r254, [%rd100];
	or.b32  	%r687, %r658, 4110;
	mul.wide.u32 	%rd101, %r687, 4;
	add.s64 	%rd102, %rd1, %rd101;
	ld.global.u32 	%r262, [%rd102];
	or.b32  	%r688, %r658, 15;
	mul.wide.u32 	%rd103, %r688, 4;
	add.s64 	%rd104, %rd1, %rd103;
	ld.global.u32 	%r255, [%rd104];
	or.b32  	%r689, %r658, 4111;
	mul.wide.u32 	%rd105, %r689, 4;
	add.s64 	%rd106, %rd1, %rd105;
	ld.global.u32 	%r263, [%rd106];
	or.b32  	%r690, %r658, 16;
	mul.wide.u32 	%rd107, %r690, 4;
	add.s64 	%rd108, %rd1, %rd107;
	ld.global.u32 	%r270, [%rd108];
	or.b32  	%r691, %r658, 4112;
	mul.wide.u32 	%rd109, %r691, 4;
	add.s64 	%rd110, %rd1, %rd109;
	ld.global.u32 	%r278, [%rd110];
	or.b32  	%r692, %r658, 17;
	mul.wide.u32 	%rd111, %r692, 4;
	add.s64 	%rd112, %rd1, %rd111;
	ld.global.u32 	%r271, [%rd112];
	or.b32  	%r693, %r658, 4113;
	mul.wide.u32 	%rd113, %r693, 4;
	add.s64 	%rd114, %rd1, %rd113;
	ld.global.u32 	%r279, [%rd114];
	or.b32  	%r694, %r658, 18;
	mul.wide.u32 	%rd115, %r694, 4;
	add.s64 	%rd116, %rd1, %rd115;
	ld.global.u32 	%r286, [%rd116];
	or.b32  	%r695, %r658, 4114;
	mul.wide.u32 	%rd117, %r695, 4;
	add.s64 	%rd118, %rd1, %rd117;
	ld.global.u32 	%r294, [%rd118];
	or.b32  	%r696, %r658, 19;
	mul.wide.u32 	%rd119, %r696, 4;
	add.s64 	%rd120, %rd1, %rd119;
	ld.global.u32 	%r287, [%rd120];
	or.b32  	%r697, %r658, 4115;
	mul.wide.u32 	%rd121, %r697, 4;
	add.s64 	%rd122, %rd1, %rd121;
	ld.global.u32 	%r295, [%rd122];
	or.b32  	%r698, %r658, 20;
	mul.wide.u32 	%rd123, %r698, 4;
	add.s64 	%rd124, %rd1, %rd123;
	ld.global.u32 	%r302, [%rd124];
	or.b32  	%r699, %r658, 4116;
	mul.wide.u32 	%rd125, %r699, 4;
	add.s64 	%rd126, %rd1, %rd125;
	ld.global.u32 	%r310, [%rd126];
	or.b32  	%r700, %r658, 21;
	mul.wide.u32 	%rd127, %r700, 4;
	add.s64 	%rd128, %rd1, %rd127;
	ld.global.u32 	%r303, [%rd128];
	or.b32  	%r701, %r658, 4117;
	mul.wide.u32 	%rd129, %r701, 4;
	add.s64 	%rd130, %rd1, %rd129;
	ld.global.u32 	%r311, [%rd130];
	or.b32  	%r702, %r658, 22;
	mul.wide.u32 	%rd131, %r702, 4;
	add.s64 	%rd132, %rd1, %rd131;
	ld.global.u32 	%r318, [%rd132];
	or.b32  	%r703, %r658, 4118;
	mul.wide.u32 	%rd133, %r703, 4;
	add.s64 	%rd134, %rd1, %rd133;
	ld.global.u32 	%r326, [%rd134];
	or.b32  	%r704, %r658, 23;
	mul.wide.u32 	%rd135, %r704, 4;
	add.s64 	%rd136, %rd1, %rd135;
	ld.global.u32 	%r319, [%rd136];
	or.b32  	%r705, %r658, 4119;
	mul.wide.u32 	%rd137, %r705, 4;
	add.s64 	%rd138, %rd1, %rd137;
	ld.global.u32 	%r327, [%rd138];
	or.b32  	%r706, %r658, 24;
	mul.wide.u32 	%rd139, %r706, 4;
	add.s64 	%rd140, %rd1, %rd139;
	ld.global.u32 	%r334, [%rd140];
	or.b32  	%r707, %r658, 4120;
	mul.wide.u32 	%rd141, %r707, 4;
	add.s64 	%rd142, %rd1, %rd141;
	ld.global.u32 	%r342, [%rd142];
	or.b32  	%r708, %r658, 25;
	mul.wide.u32 	%rd143, %r708, 4;
	add.s64 	%rd144, %rd1, %rd143;
	ld.global.u32 	%r335, [%rd144];
	or.b32  	%r709, %r658, 4121;
	mul.wide.u32 	%rd145, %r709, 4;
	add.s64 	%rd146, %rd1, %rd145;
	ld.global.u32 	%r343, [%rd146];
	or.b32  	%r710, %r658, 26;
	mul.wide.u32 	%rd147, %r710, 4;
	add.s64 	%rd148, %rd1, %rd147;
	ld.global.u32 	%r350, [%rd148];
	or.b32  	%r711, %r658, 4122;
	mul.wide.u32 	%rd149, %r711, 4;
	add.s64 	%rd150, %rd1, %rd149;
	ld.global.u32 	%r358, [%rd150];
	or.b32  	%r712, %r658, 27;
	mul.wide.u32 	%rd151, %r712, 4;
	add.s64 	%rd152, %rd1, %rd151;
	ld.global.u32 	%r351, [%rd152];
	or.b32  	%r713, %r658, 4123;
	mul.wide.u32 	%rd153, %r713, 4;
	add.s64 	%rd154, %rd1, %rd153;
	ld.global.u32 	%r359, [%rd154];
	or.b32  	%r714, %r658, 28;
	mul.wide.u32 	%rd155, %r714, 4;
	add.s64 	%rd156, %rd1, %rd155;
	ld.global.u32 	%r366, [%rd156];
	or.b32  	%r715, %r658, 4124;
	mul.wide.u32 	%rd157, %r715, 4;
	add.s64 	%rd158, %rd1, %rd157;
	ld.global.u32 	%r374, [%rd158];
	or.b32  	%r716, %r658, 29;
	mul.wide.u32 	%rd159, %r716, 4;
	add.s64 	%rd160, %rd1, %rd159;
	ld.global.u32 	%r367, [%rd160];
	or.b32  	%r717, %r658, 4125;
	mul.wide.u32 	%rd161, %r717, 4;
	add.s64 	%rd162, %rd1, %rd161;
	ld.global.u32 	%r375, [%rd162];
	or.b32  	%r718, %r658, 30;
	mul.wide.u32 	%rd163, %r718, 4;
	add.s64 	%rd164, %rd1, %rd163;
	ld.global.u32 	%r382, [%rd164];
	or.b32  	%r719, %r658, 4126;
	mul.wide.u32 	%rd165, %r719, 4;
	add.s64 	%rd166, %rd1, %rd165;
	ld.global.u32 	%r390, [%rd166];
	or.b32  	%r720, %r658, 31;
	mul.wide.u32 	%rd167, %r720, 4;
	add.s64 	%rd168, %rd1, %rd167;
	ld.global.u32 	%r383, [%rd168];
	or.b32  	%r721, %r658, 4127;
	mul.wide.u32 	%rd169, %r721, 4;
	add.s64 	%rd170, %rd1, %rd169;
	ld.global.u32 	%r391, [%rd170];
	mov.u32 	%r140, 21520;
	// begin inline asm
	prmt.b32 %r394, %r142, %r143, %r140;
	// end inline asm
	mov.u32 	%r144, 30258;
	// begin inline asm
	prmt.b32 %r395, %r142, %r143, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r402, %r150, %r151, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r403, %r150, %r151, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r410, %r158, %r159, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r411, %r158, %r159, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r418, %r166, %r167, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r419, %r166, %r167, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r426, %r174, %r175, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r427, %r174, %r175, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r434, %r182, %r183, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r435, %r182, %r183, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r442, %r190, %r191, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r443, %r190, %r191, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r450, %r198, %r199, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r451, %r198, %r199, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r458, %r206, %r207, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r459, %r206, %r207, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r466, %r214, %r215, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r467, %r214, %r215, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r474, %r222, %r223, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r475, %r222, %r223, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r482, %r230, %r231, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r483, %r230, %r231, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r490, %r238, %r239, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r491, %r238, %r239, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r498, %r246, %r247, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r499, %r246, %r247, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r506, %r254, %r255, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r507, %r254, %r255, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r514, %r262, %r263, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r515, %r262, %r263, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r522, %r270, %r271, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r523, %r270, %r271, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r530, %r278, %r279, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r531, %r278, %r279, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r538, %r286, %r287, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r539, %r286, %r287, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r546, %r294, %r295, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r547, %r294, %r295, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r554, %r302, %r303, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r555, %r302, %r303, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r562, %r310, %r311, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r563, %r310, %r311, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r570, %r318, %r319, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r571, %r318, %r319, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r578, %r326, %r327, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r579, %r326, %r327, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r586, %r334, %r335, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r587, %r334, %r335, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r594, %r342, %r343, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r595, %r342, %r343, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r602, %r350, %r351, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r603, %r350, %r351, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r610, %r358, %r359, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r611, %r358, %r359, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r618, %r366, %r367, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r619, %r366, %r367, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r626, %r374, %r375, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r627, %r374, %r375, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r634, %r382, %r383, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r635, %r382, %r383, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r642, %r390, %r391, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r643, %r390, %r391, %r144;
	// end inline asm
	mov.u32 	%r396, 25152;
	// begin inline asm
	prmt.b32 %r1099, %r394, %r395, %r396;
	// end inline asm
	mov.u32 	%r400, 29521;
	// begin inline asm
	prmt.b32 %r1105, %r394, %r395, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1617, %r402, %r403, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1623, %r402, %r403, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1131, %r410, %r411, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1137, %r410, %r411, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1649, %r418, %r419, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1655, %r418, %r419, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1163, %r426, %r427, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1169, %r426, %r427, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1681, %r434, %r435, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1687, %r434, %r435, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1195, %r442, %r443, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1201, %r442, %r443, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1713, %r450, %r451, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1719, %r450, %r451, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1227, %r458, %r459, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1233, %r458, %r459, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1745, %r466, %r467, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1751, %r466, %r467, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1259, %r474, %r475, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1265, %r474, %r475, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1777, %r482, %r483, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1783, %r482, %r483, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1291, %r490, %r491, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1297, %r490, %r491, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1809, %r498, %r499, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1815, %r498, %r499, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1323, %r506, %r507, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1329, %r506, %r507, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1841, %r514, %r515, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1847, %r514, %r515, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1355, %r522, %r523, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1361, %r522, %r523, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1873, %r530, %r531, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1879, %r530, %r531, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1387, %r538, %r539, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1393, %r538, %r539, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1905, %r546, %r547, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1911, %r546, %r547, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1419, %r554, %r555, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1425, %r554, %r555, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1937, %r562, %r563, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1943, %r562, %r563, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1451, %r570, %r571, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1457, %r570, %r571, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1969, %r578, %r579, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1975, %r578, %r579, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1483, %r586, %r587, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1489, %r586, %r587, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2001, %r594, %r595, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2007, %r594, %r595, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1515, %r602, %r603, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1521, %r602, %r603, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2033, %r610, %r611, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2039, %r610, %r611, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1547, %r618, %r619, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1553, %r618, %r619, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2065, %r626, %r627, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2071, %r626, %r627, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1579, %r634, %r635, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1585, %r634, %r635, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2097, %r642, %r643, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2103, %r642, %r643, %r400;
	// end inline asm
	shl.b32 	%r69, %r120, 13;
	shl.b32 	%r70, %r1, 8;
	shl.b32 	%r722, %r4, 2;
	and.b32  	%r723, %r722, 28;
	or.b32  	%r724, %r723, %r3;
	or.b32  	%r71, %r724, 128;
	or.b32  	%r72, %r724, 129;
	or.b32  	%r73, %r724, 130;
	or.b32  	%r74, %r724, 131;
	shr.u32 	%r75, %r4, 2;
	shl.b32 	%r725, %r2, 6;
	shl.b32 	%r726, %r4, 4;
	and.b32  	%r727, %r726, 48;
	or.b32  	%r76, %r727, %r725;
	mul.lo.s32 	%r728, %r2, 640;
	shl.b32 	%r729, %r4, 1;
	and.b32  	%r77, %r729, 6;
	or.b32  	%r78, %r728, %r75;
	or.b32  	%r79, %r78, 8;
	and.b32  	%r730, %r4, 7;
	or.b32  	%r731, %r116, 640;
	or.b32  	%r732, %r116, 1280;
	or.b32  	%r733, %r116, 1920;
	shl.b32 	%r735, %r127, %r118;
	setp.gt.u32 	%p15, %r118, 31;
	selp.b32 	%r80, 0, %r735, %p15;
	min.u32 	%r81, %r117, 31;
	and.b32  	%r82, %r4, 1;
	and.b32  	%r83, %r4, 2;
	and.b32  	%r84, %r4, 4;
	shl.b32 	%r736, %r1, 14;
	shl.b32 	%r737, %r2, 21;
	shl.b32 	%r738, %r4, 16;
	and.b32  	%r739, %r738, 1572864;
	or.b32  	%r740, %r739, %r737;
	and.b32  	%r741, %r654, 64;
	shl.b32 	%r742, %r4, 3;
	and.b32  	%r743, %r742, 32;
	or.b32  	%r744, %r741, %r743;
	or.b32  	%r85, %r740, %r736;
	mul.lo.s32 	%r745, %r115, 257;
	add.s32 	%r746, %r724, %r745;
	mul.wide.u32 	%rd171, %r746, 4;
	mov.u64 	%rd172, shmem;
	add.s64 	%rd9, %rd172, %rd171;
	cvt.u64.u32 	%rd173, %r745;
	cvt.u64.u32 	%rd10, %r724;
	add.s64 	%rd174, %rd10, %rd173;
	shl.b64 	%rd175, %rd174, 2;
	add.s64 	%rd11, %rd172, %rd175;
	add.s32 	%r747, %r745, %r71;
	mul.wide.u32 	%rd176, %r747, 4;
	add.s64 	%rd12, %rd172, %rd176;
	add.s32 	%r748, %r745, %r72;
	mul.wide.u32 	%rd177, %r748, 4;
	add.s64 	%rd13, %rd172, %rd177;
	add.s32 	%r749, %r745, %r73;
	mul.wide.u32 	%rd178, %r749, 4;
	add.s64 	%rd14, %rd172, %rd178;
	add.s32 	%r750, %r745, %r74;
	mul.wide.u32 	%rd179, %r750, 4;
	add.s64 	%rd15, %rd172, %rd179;
	mul.lo.s32 	%r751, %r730, 20;
	add.s32 	%r752, %r751, %r116;
	mul.wide.u32 	%rd180, %r752, 4;
	add.s64 	%rd181, %rd172, 32896;
	add.s64 	%rd16, %rd181, %rd180;
	add.s32 	%r753, %r751, %r731;
	mul.wide.u32 	%rd182, %r753, 4;
	add.s64 	%rd17, %rd181, %rd182;
	add.s32 	%r754, %r751, %r732;
	mul.wide.u32 	%rd183, %r754, 4;
	add.s64 	%rd18, %rd181, %rd183;
	add.s32 	%r755, %r751, %r733;
	mul.wide.u32 	%rd184, %r755, 4;
	add.s64 	%rd19, %rd181, %rd184;
	add.s32 	%r756, %r751, 160;
	add.s32 	%r757, %r756, %r116;
	mul.wide.u32 	%rd185, %r757, 4;
	add.s64 	%rd20, %rd181, %rd185;
	add.s32 	%r758, %r756, %r731;
	mul.wide.u32 	%rd186, %r758, 4;
	add.s64 	%rd21, %rd181, %rd186;
	add.s32 	%r759, %r756, %r732;
	mul.wide.u32 	%rd187, %r759, 4;
	add.s64 	%rd22, %rd181, %rd187;
	add.s32 	%r760, %r756, %r733;
	mul.wide.u32 	%rd188, %r760, 4;
	add.s64 	%rd23, %rd181, %rd188;
	add.s32 	%r761, %r751, 320;
	add.s32 	%r762, %r761, %r116;
	mul.wide.u32 	%rd189, %r762, 4;
	add.s64 	%rd24, %rd181, %rd189;
	add.s32 	%r763, %r761, %r731;
	mul.wide.u32 	%rd190, %r763, 4;
	add.s64 	%rd25, %rd181, %rd190;
	add.s32 	%r764, %r761, %r732;
	mul.wide.u32 	%rd191, %r764, 4;
	add.s64 	%rd26, %rd181, %rd191;
	add.s32 	%r765, %r761, %r733;
	mul.wide.u32 	%rd192, %r765, 4;
	add.s64 	%rd27, %rd181, %rd192;
	or.b32  	%r766, %r4, 24;
	mul.lo.s32 	%r767, %r766, 20;
	add.s32 	%r768, %r767, %r116;
	mul.wide.u32 	%rd193, %r768, 4;
	add.s64 	%rd28, %rd181, %rd193;
	add.s32 	%r769, %r767, %r731;
	mul.wide.u32 	%rd194, %r769, 4;
	add.s64 	%rd29, %rd181, %rd194;
	add.s32 	%r770, %r767, %r732;
	mul.wide.u32 	%rd195, %r770, 4;
	add.s64 	%rd30, %rd181, %rd195;
	add.s32 	%r771, %r767, %r733;
	mul.wide.u32 	%rd196, %r771, 4;
	add.s64 	%rd31, %rd181, %rd196;
	and.b32  	%r772, %r726, 16;
	or.b32  	%r86, %r772, %r744;
	mov.pred 	%p69, 0;
$L__BB0_11:                             // %L9943
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_13 Depth 2
                                        //       Child Loop BB0_14 Depth 3
	add.s32 	%r773, %r2411, %r120;
	setp.ge.s32 	%p16, %r773, %r121;
	@%p16 bra 	$L__BB0_17;
// %bb.12:                              // %L9951.preheader
                                        //   in Loop: Header=BB0_11 Depth=1
	or.b32  	%r88, %r2411, %r115;
	or.b32  	%r89, %r88, 4;
	or.b32  	%r90, %r88, 8;
	or.b32  	%r91, %r88, 12;
	or.b32  	%r92, %r88, 16;
	or.b32  	%r93, %r88, 20;
	or.b32  	%r94, %r88, 24;
	or.b32  	%r95, %r88, 28;
	mov.u32 	%r1101, 0;
	mov.u32 	%r2412, %r1101;
	mov.u32 	%r2413, %r1101;
	mov.u32 	%r2414, %r1101;
	mov.u32 	%r2415, %r1101;
	mov.u32 	%r2416, %r1101;
	mov.u32 	%r2417, %r1101;
	mov.u32 	%r2418, %r1101;
$L__BB0_13:                             // %L9951
                                        //   Parent Loop BB0_11 Depth=1
                                        // =>  This Loop Header: Depth=2
                                        //       Child Loop BB0_14 Depth 3
	cvt.u32.u64 	%r776, %rd10;
	add.s32 	%r777, %r88, %r2412;
	shl.b32 	%r778, %r777, 13;
	or.b32  	%r779, %r778, %r70;
	or.b32  	%r780, %r779, %r776;
	add.s32 	%r781, %r780, %r69;
	shr.s32 	%r782, %r781, 31;
	shr.u32 	%r783, %r782, 3;
	add.s32 	%r784, %r781, %r783;
	shr.s32 	%r785, %r784, 29;
	setp.lt.s32 	%p17, %r781, 0;
	and.b32  	%r786, %r784, -536870912;
	setp.ne.s32 	%p18, %r786, %r781;
	and.pred  	%p19, %p17, %p18;
	selp.u32 	%r787, 1, 0, %p19;
	sub.s32 	%r788, %r787, %r785;
	shl.b32 	%r789, %r788, 29;
	add.s32 	%r790, %r789, %r781;
	mul.wide.s32 	%rd197, %r790, 4;
	add.s64 	%rd198, %rd2, %rd197;
	ld.global.v4.u32 	{%r791, %r792, %r793, %r794}, [%rd198];
	or.b32  	%r795, %r71, %r779;
	add.s32 	%r796, %r795, %r69;
	shr.s32 	%r797, %r796, 31;
	shr.u32 	%r798, %r797, 3;
	add.s32 	%r799, %r796, %r798;
	shr.u32 	%r800, %r799, 29;
	shr.u32 	%r801, %r796, 31;
	sub.s32 	%r802, %r801, %r800;
	shl.b32 	%r803, %r802, 29;
	add.s32 	%r804, %r803, %r796;
	mul.wide.s32 	%rd199, %r804, 4;
	add.s64 	%rd200, %rd2, %rd199;
	ld.global.v4.u32 	{%r805, %r806, %r807, %r808}, [%rd200];
	add.s32 	%r809, %r2412, %r89;
	shl.b32 	%r810, %r809, 13;
	and.b32  	%r811, %r810, 536666112;
	or.b32  	%r812, %r811, %r70;
	or.b32  	%r813, %r812, %r776;
	add.s32 	%r814, %r813, %r69;
	shr.s32 	%r815, %r814, 31;
	shr.u32 	%r816, %r815, 3;
	add.s32 	%r817, %r814, %r816;
	shr.s32 	%r818, %r817, 29;
	setp.lt.s32 	%p20, %r814, 0;
	and.b32  	%r819, %r817, -536870912;
	setp.ne.s32 	%p21, %r819, %r814;
	and.pred  	%p22, %p20, %p21;
	selp.u32 	%r820, 1, 0, %p22;
	sub.s32 	%r821, %r820, %r818;
	shl.b32 	%r822, %r821, 29;
	add.s32 	%r823, %r822, %r814;
	mul.wide.s32 	%rd201, %r823, 4;
	add.s64 	%rd202, %rd2, %rd201;
	ld.global.v4.u32 	{%r824, %r825, %r826, %r827}, [%rd202];
	or.b32  	%r828, %r71, %r812;
	add.s32 	%r829, %r828, %r69;
	shr.s32 	%r830, %r829, 31;
	shr.u32 	%r831, %r830, 3;
	add.s32 	%r832, %r829, %r831;
	shr.u32 	%r833, %r832, 29;
	shr.u32 	%r834, %r829, 31;
	sub.s32 	%r835, %r834, %r833;
	shl.b32 	%r836, %r835, 29;
	add.s32 	%r837, %r836, %r829;
	mul.wide.s32 	%rd203, %r837, 4;
	add.s64 	%rd204, %rd2, %rd203;
	ld.global.v4.u32 	{%r838, %r839, %r840, %r841}, [%rd204];
	add.s32 	%r842, %r2412, %r90;
	shl.b32 	%r843, %r842, 13;
	and.b32  	%r844, %r843, 536698880;
	or.b32  	%r845, %r844, %r70;
	or.b32  	%r846, %r845, %r776;
	add.s32 	%r847, %r846, %r69;
	shr.s32 	%r848, %r847, 31;
	shr.u32 	%r849, %r848, 3;
	add.s32 	%r850, %r847, %r849;
	shr.s32 	%r851, %r850, 29;
	setp.lt.s32 	%p23, %r847, 0;
	and.b32  	%r852, %r850, -536870912;
	setp.ne.s32 	%p24, %r852, %r847;
	and.pred  	%p25, %p23, %p24;
	selp.u32 	%r853, 1, 0, %p25;
	sub.s32 	%r854, %r853, %r851;
	shl.b32 	%r855, %r854, 29;
	add.s32 	%r856, %r855, %r847;
	mul.wide.s32 	%rd205, %r856, 4;
	add.s64 	%rd206, %rd2, %rd205;
	ld.global.v4.u32 	{%r857, %r858, %r859, %r860}, [%rd206];
	or.b32  	%r861, %r71, %r845;
	add.s32 	%r862, %r861, %r69;
	shr.s32 	%r863, %r862, 31;
	shr.u32 	%r864, %r863, 3;
	add.s32 	%r865, %r862, %r864;
	shr.u32 	%r866, %r865, 29;
	shr.u32 	%r867, %r862, 31;
	sub.s32 	%r868, %r867, %r866;
	shl.b32 	%r869, %r868, 29;
	add.s32 	%r870, %r869, %r862;
	mul.wide.s32 	%rd207, %r870, 4;
	add.s64 	%rd208, %rd2, %rd207;
	ld.global.v4.u32 	{%r871, %r872, %r873, %r874}, [%rd208];
	add.s32 	%r875, %r2412, %r91;
	shl.b32 	%r876, %r875, 13;
	and.b32  	%r877, %r876, 536731648;
	or.b32  	%r878, %r877, %r70;
	or.b32  	%r879, %r878, %r776;
	add.s32 	%r880, %r879, %r69;
	shr.s32 	%r881, %r880, 31;
	shr.u32 	%r882, %r881, 3;
	add.s32 	%r883, %r880, %r882;
	shr.s32 	%r884, %r883, 29;
	setp.lt.s32 	%p26, %r880, 0;
	and.b32  	%r885, %r883, -536870912;
	setp.ne.s32 	%p27, %r885, %r880;
	and.pred  	%p28, %p26, %p27;
	selp.u32 	%r886, 1, 0, %p28;
	sub.s32 	%r887, %r886, %r884;
	shl.b32 	%r888, %r887, 29;
	add.s32 	%r889, %r888, %r880;
	mul.wide.s32 	%rd209, %r889, 4;
	add.s64 	%rd210, %rd2, %rd209;
	ld.global.v4.u32 	{%r890, %r891, %r892, %r893}, [%rd210];
	or.b32  	%r894, %r71, %r878;
	add.s32 	%r895, %r894, %r69;
	shr.s32 	%r896, %r895, 31;
	shr.u32 	%r897, %r896, 3;
	add.s32 	%r898, %r895, %r897;
	shr.u32 	%r899, %r898, 29;
	shr.u32 	%r900, %r895, 31;
	sub.s32 	%r901, %r900, %r899;
	shl.b32 	%r902, %r901, 29;
	add.s32 	%r903, %r902, %r895;
	mul.wide.s32 	%rd211, %r903, 4;
	add.s64 	%rd212, %rd2, %rd211;
	ld.global.v4.u32 	{%r904, %r905, %r906, %r907}, [%rd212];
	add.s32 	%r908, %r2412, %r92;
	shl.b32 	%r909, %r908, 13;
	and.b32  	%r910, %r909, 536764416;
	or.b32  	%r911, %r910, %r70;
	or.b32  	%r912, %r911, %r776;
	add.s32 	%r913, %r912, %r69;
	shr.s32 	%r914, %r913, 31;
	shr.u32 	%r915, %r914, 3;
	add.s32 	%r916, %r913, %r915;
	shr.s32 	%r917, %r916, 29;
	setp.lt.s32 	%p29, %r913, 0;
	and.b32  	%r918, %r916, -536870912;
	setp.ne.s32 	%p30, %r918, %r913;
	and.pred  	%p31, %p29, %p30;
	selp.u32 	%r919, 1, 0, %p31;
	sub.s32 	%r920, %r919, %r917;
	shl.b32 	%r921, %r920, 29;
	add.s32 	%r922, %r921, %r913;
	mul.wide.s32 	%rd213, %r922, 4;
	add.s64 	%rd214, %rd2, %rd213;
	ld.global.v4.u32 	{%r923, %r924, %r925, %r926}, [%rd214];
	or.b32  	%r927, %r71, %r911;
	add.s32 	%r928, %r927, %r69;
	shr.s32 	%r929, %r928, 31;
	shr.u32 	%r930, %r929, 3;
	add.s32 	%r931, %r928, %r930;
	shr.u32 	%r932, %r931, 29;
	shr.u32 	%r933, %r928, 31;
	sub.s32 	%r934, %r933, %r932;
	shl.b32 	%r935, %r934, 29;
	add.s32 	%r936, %r935, %r928;
	mul.wide.s32 	%rd215, %r936, 4;
	add.s64 	%rd216, %rd2, %rd215;
	ld.global.v4.u32 	{%r937, %r938, %r939, %r940}, [%rd216];
	add.s32 	%r941, %r2412, %r93;
	shl.b32 	%r942, %r941, 13;
	and.b32  	%r943, %r942, 536797184;
	or.b32  	%r944, %r943, %r70;
	or.b32  	%r945, %r944, %r776;
	add.s32 	%r946, %r945, %r69;
	shr.s32 	%r947, %r946, 31;
	shr.u32 	%r948, %r947, 3;
	add.s32 	%r949, %r946, %r948;
	shr.s32 	%r950, %r949, 29;
	setp.lt.s32 	%p32, %r946, 0;
	and.b32  	%r951, %r949, -536870912;
	setp.ne.s32 	%p33, %r951, %r946;
	and.pred  	%p34, %p32, %p33;
	selp.u32 	%r952, 1, 0, %p34;
	sub.s32 	%r953, %r952, %r950;
	shl.b32 	%r954, %r953, 29;
	add.s32 	%r955, %r954, %r946;
	mul.wide.s32 	%rd217, %r955, 4;
	add.s64 	%rd218, %rd2, %rd217;
	ld.global.v4.u32 	{%r956, %r957, %r958, %r959}, [%rd218];
	or.b32  	%r960, %r71, %r944;
	add.s32 	%r961, %r960, %r69;
	shr.s32 	%r962, %r961, 31;
	shr.u32 	%r963, %r962, 3;
	add.s32 	%r964, %r961, %r963;
	shr.u32 	%r965, %r964, 29;
	shr.u32 	%r966, %r961, 31;
	sub.s32 	%r967, %r966, %r965;
	shl.b32 	%r968, %r967, 29;
	add.s32 	%r969, %r968, %r961;
	mul.wide.s32 	%rd219, %r969, 4;
	add.s64 	%rd220, %rd2, %rd219;
	ld.global.v4.u32 	{%r970, %r971, %r972, %r973}, [%rd220];
	add.s32 	%r974, %r2412, %r94;
	shl.b32 	%r975, %r974, 13;
	and.b32  	%r976, %r975, 536829952;
	or.b32  	%r977, %r976, %r70;
	or.b32  	%r978, %r977, %r776;
	add.s32 	%r979, %r978, %r69;
	shr.s32 	%r980, %r979, 31;
	shr.u32 	%r981, %r980, 3;
	add.s32 	%r982, %r979, %r981;
	shr.s32 	%r983, %r982, 29;
	setp.lt.s32 	%p35, %r979, 0;
	and.b32  	%r984, %r982, -536870912;
	setp.ne.s32 	%p36, %r984, %r979;
	and.pred  	%p37, %p35, %p36;
	selp.u32 	%r985, 1, 0, %p37;
	sub.s32 	%r986, %r985, %r983;
	shl.b32 	%r987, %r986, 29;
	add.s32 	%r988, %r987, %r979;
	mul.wide.s32 	%rd221, %r988, 4;
	add.s64 	%rd222, %rd2, %rd221;
	ld.global.v4.u32 	{%r989, %r990, %r991, %r992}, [%rd222];
	or.b32  	%r993, %r71, %r977;
	add.s32 	%r994, %r993, %r69;
	shr.s32 	%r995, %r994, 31;
	shr.u32 	%r996, %r995, 3;
	add.s32 	%r997, %r994, %r996;
	shr.u32 	%r998, %r997, 29;
	shr.u32 	%r999, %r994, 31;
	sub.s32 	%r1000, %r999, %r998;
	shl.b32 	%r1001, %r1000, 29;
	add.s32 	%r1002, %r1001, %r994;
	mul.wide.s32 	%rd223, %r1002, 4;
	add.s64 	%rd224, %rd2, %rd223;
	ld.global.v4.u32 	{%r1003, %r1004, %r1005, %r1006}, [%rd224];
	add.s32 	%r1007, %r2412, %r95;
	shl.b32 	%r1008, %r1007, 13;
	and.b32  	%r1009, %r1008, 536862720;
	or.b32  	%r1010, %r1009, %r70;
	or.b32  	%r1011, %r1010, %r776;
	add.s32 	%r1012, %r1011, %r69;
	shr.s32 	%r1013, %r1012, 31;
	shr.u32 	%r1014, %r1013, 3;
	add.s32 	%r1015, %r1012, %r1014;
	shr.s32 	%r1016, %r1015, 29;
	setp.lt.s32 	%p38, %r1012, 0;
	and.b32  	%r1017, %r1015, -536870912;
	setp.ne.s32 	%p39, %r1017, %r1012;
	and.pred  	%p40, %p38, %p39;
	selp.u32 	%r1018, 1, 0, %p40;
	sub.s32 	%r1019, %r1018, %r1016;
	shl.b32 	%r1020, %r1019, 29;
	add.s32 	%r1021, %r1020, %r1012;
	mul.wide.s32 	%rd225, %r1021, 4;
	add.s64 	%rd226, %rd2, %rd225;
	ld.global.v4.u32 	{%r1022, %r1023, %r1024, %r1025}, [%rd226];
	or.b32  	%r1026, %r71, %r1010;
	add.s32 	%r1027, %r1026, %r69;
	shr.s32 	%r1028, %r1027, 31;
	shr.u32 	%r1029, %r1028, 3;
	add.s32 	%r1030, %r1027, %r1029;
	shr.u32 	%r1031, %r1030, 29;
	shr.u32 	%r1032, %r1027, 31;
	sub.s32 	%r1033, %r1032, %r1031;
	shl.b32 	%r1034, %r1033, 29;
	add.s32 	%r1035, %r1034, %r1027;
	mul.wide.s32 	%rd227, %r1035, 4;
	add.s64 	%rd228, %rd2, %rd227;
	ld.global.v4.u32 	{%r1036, %r1037, %r1038, %r1039}, [%rd228];
	st.shared.u32 	[%rd9], %r791;
	st.shared.u32 	[%rd11+4], %r792;
	st.shared.u32 	[%rd11+8], %r793;
	st.shared.u32 	[%rd11+12], %r794;
	st.shared.u32 	[%rd12], %r805;
	st.shared.u32 	[%rd13], %r806;
	st.shared.u32 	[%rd14], %r807;
	st.shared.u32 	[%rd15], %r808;
	and.b32  	%r1040, %r809, 7;
	mul.lo.s32 	%r1041, %r1040, 257;
	add.s32 	%r1042, %r776, %r1041;
	mul.wide.u32 	%rd229, %r1042, 4;
	add.s64 	%rd231, %rd172, %rd229;
	st.shared.u32 	[%rd231], %r824;
	cvt.u64.u32 	%rd232, %r1041;
	add.s64 	%rd233, %rd10, %rd232;
	shl.b64 	%rd234, %rd233, 2;
	add.s64 	%rd235, %rd172, %rd234;
	st.shared.u32 	[%rd235+4], %r825;
	st.shared.u32 	[%rd235+8], %r826;
	st.shared.u32 	[%rd235+12], %r827;
	add.s32 	%r1043, %r1041, %r71;
	mul.wide.u32 	%rd236, %r1043, 4;
	add.s64 	%rd237, %rd172, %rd236;
	st.shared.u32 	[%rd237], %r838;
	add.s32 	%r1044, %r1041, %r72;
	mul.wide.u32 	%rd238, %r1044, 4;
	add.s64 	%rd239, %rd172, %rd238;
	st.shared.u32 	[%rd239], %r839;
	add.s32 	%r1045, %r1041, %r73;
	mul.wide.u32 	%rd240, %r1045, 4;
	add.s64 	%rd241, %rd172, %rd240;
	st.shared.u32 	[%rd241], %r840;
	add.s32 	%r1046, %r1041, %r74;
	mul.wide.u32 	%rd242, %r1046, 4;
	add.s64 	%rd243, %rd172, %rd242;
	st.shared.u32 	[%rd243], %r841;
	and.b32  	%r1047, %r842, 11;
	mul.lo.s32 	%r1048, %r1047, 257;
	add.s32 	%r1049, %r776, %r1048;
	mul.wide.u32 	%rd244, %r1049, 4;
	add.s64 	%rd245, %rd172, %rd244;
	st.shared.u32 	[%rd245], %r857;
	cvt.u64.u32 	%rd246, %r1048;
	add.s64 	%rd247, %rd10, %rd246;
	shl.b64 	%rd248, %rd247, 2;
	add.s64 	%rd249, %rd172, %rd248;
	st.shared.u32 	[%rd249+4], %r858;
	st.shared.u32 	[%rd249+8], %r859;
	st.shared.u32 	[%rd249+12], %r860;
	add.s32 	%r1050, %r1048, %r71;
	mul.wide.u32 	%rd250, %r1050, 4;
	add.s64 	%rd251, %rd172, %rd250;
	st.shared.u32 	[%rd251], %r871;
	add.s32 	%r1051, %r1048, %r72;
	mul.wide.u32 	%rd252, %r1051, 4;
	add.s64 	%rd253, %rd172, %rd252;
	st.shared.u32 	[%rd253], %r872;
	add.s32 	%r1052, %r1048, %r73;
	mul.wide.u32 	%rd254, %r1052, 4;
	add.s64 	%rd255, %rd172, %rd254;
	st.shared.u32 	[%rd255], %r873;
	add.s32 	%r1053, %r1048, %r74;
	mul.wide.u32 	%rd256, %r1053, 4;
	add.s64 	%rd257, %rd172, %rd256;
	st.shared.u32 	[%rd257], %r874;
	and.b32  	%r1054, %r875, 15;
	mul.lo.s32 	%r1055, %r1054, 257;
	add.s32 	%r1056, %r776, %r1055;
	mul.wide.u32 	%rd258, %r1056, 4;
	add.s64 	%rd259, %rd172, %rd258;
	st.shared.u32 	[%rd259], %r890;
	cvt.u64.u32 	%rd260, %r1055;
	add.s64 	%rd261, %rd10, %rd260;
	shl.b64 	%rd262, %rd261, 2;
	add.s64 	%rd263, %rd172, %rd262;
	st.shared.u32 	[%rd263+4], %r891;
	st.shared.u32 	[%rd263+8], %r892;
	st.shared.u32 	[%rd263+12], %r893;
	add.s32 	%r1057, %r1055, %r71;
	mul.wide.u32 	%rd264, %r1057, 4;
	add.s64 	%rd265, %rd172, %rd264;
	st.shared.u32 	[%rd265], %r904;
	add.s32 	%r1058, %r1055, %r72;
	mul.wide.u32 	%rd266, %r1058, 4;
	add.s64 	%rd267, %rd172, %rd266;
	st.shared.u32 	[%rd267], %r905;
	add.s32 	%r1059, %r1055, %r73;
	mul.wide.u32 	%rd268, %r1059, 4;
	add.s64 	%rd269, %rd172, %rd268;
	st.shared.u32 	[%rd269], %r906;
	add.s32 	%r1060, %r1055, %r74;
	mul.wide.u32 	%rd270, %r1060, 4;
	add.s64 	%rd271, %rd172, %rd270;
	st.shared.u32 	[%rd271], %r907;
	and.b32  	%r1061, %r908, 19;
	mul.lo.s32 	%r1062, %r1061, 257;
	add.s32 	%r1063, %r776, %r1062;
	mul.wide.u32 	%rd272, %r1063, 4;
	add.s64 	%rd273, %rd172, %rd272;
	st.shared.u32 	[%rd273], %r923;
	cvt.u64.u32 	%rd274, %r1062;
	add.s64 	%rd275, %rd10, %rd274;
	shl.b64 	%rd276, %rd275, 2;
	add.s64 	%rd277, %rd172, %rd276;
	st.shared.u32 	[%rd277+4], %r924;
	st.shared.u32 	[%rd277+8], %r925;
	st.shared.u32 	[%rd277+12], %r926;
	add.s32 	%r1064, %r1062, %r71;
	mul.wide.u32 	%rd278, %r1064, 4;
	add.s64 	%rd279, %rd172, %rd278;
	st.shared.u32 	[%rd279], %r937;
	add.s32 	%r1065, %r1062, %r72;
	mul.wide.u32 	%rd280, %r1065, 4;
	add.s64 	%rd281, %rd172, %rd280;
	st.shared.u32 	[%rd281], %r938;
	add.s32 	%r1066, %r1062, %r73;
	mul.wide.u32 	%rd282, %r1066, 4;
	add.s64 	%rd283, %rd172, %rd282;
	st.shared.u32 	[%rd283], %r939;
	add.s32 	%r1067, %r1062, %r74;
	mul.wide.u32 	%rd284, %r1067, 4;
	add.s64 	%rd285, %rd172, %rd284;
	st.shared.u32 	[%rd285], %r940;
	and.b32  	%r1068, %r941, 23;
	mul.lo.s32 	%r1069, %r1068, 257;
	add.s32 	%r1070, %r776, %r1069;
	mul.wide.u32 	%rd286, %r1070, 4;
	add.s64 	%rd287, %rd172, %rd286;
	st.shared.u32 	[%rd287], %r956;
	cvt.u64.u32 	%rd288, %r1069;
	add.s64 	%rd289, %rd10, %rd288;
	shl.b64 	%rd290, %rd289, 2;
	add.s64 	%rd291, %rd172, %rd290;
	st.shared.u32 	[%rd291+4], %r957;
	st.shared.u32 	[%rd291+8], %r958;
	st.shared.u32 	[%rd291+12], %r959;
	add.s32 	%r1071, %r1069, %r71;
	mul.wide.u32 	%rd292, %r1071, 4;
	add.s64 	%rd293, %rd172, %rd292;
	st.shared.u32 	[%rd293], %r970;
	add.s32 	%r1072, %r1069, %r72;
	mul.wide.u32 	%rd294, %r1072, 4;
	add.s64 	%rd295, %rd172, %rd294;
	st.shared.u32 	[%rd295], %r971;
	add.s32 	%r1073, %r1069, %r73;
	mul.wide.u32 	%rd296, %r1073, 4;
	add.s64 	%rd297, %rd172, %rd296;
	st.shared.u32 	[%rd297], %r972;
	add.s32 	%r1074, %r1069, %r74;
	mul.wide.u32 	%rd298, %r1074, 4;
	add.s64 	%rd299, %rd172, %rd298;
	st.shared.u32 	[%rd299], %r973;
	and.b32  	%r1075, %r974, 27;
	mul.lo.s32 	%r1076, %r1075, 257;
	add.s32 	%r1077, %r776, %r1076;
	mul.wide.u32 	%rd300, %r1077, 4;
	add.s64 	%rd301, %rd172, %rd300;
	st.shared.u32 	[%rd301], %r989;
	cvt.u64.u32 	%rd302, %r1076;
	add.s64 	%rd303, %rd10, %rd302;
	shl.b64 	%rd304, %rd303, 2;
	add.s64 	%rd305, %rd172, %rd304;
	st.shared.u32 	[%rd305+4], %r990;
	st.shared.u32 	[%rd305+8], %r991;
	st.shared.u32 	[%rd305+12], %r992;
	add.s32 	%r1078, %r1076, %r71;
	mul.wide.u32 	%rd306, %r1078, 4;
	add.s64 	%rd307, %rd172, %rd306;
	st.shared.u32 	[%rd307], %r1003;
	add.s32 	%r1079, %r1076, %r72;
	mul.wide.u32 	%rd308, %r1079, 4;
	add.s64 	%rd309, %rd172, %rd308;
	st.shared.u32 	[%rd309], %r1004;
	add.s32 	%r1080, %r1076, %r73;
	mul.wide.u32 	%rd310, %r1080, 4;
	add.s64 	%rd311, %rd172, %rd310;
	st.shared.u32 	[%rd311], %r1005;
	add.s32 	%r1081, %r1076, %r74;
	mul.wide.u32 	%rd312, %r1081, 4;
	add.s64 	%rd313, %rd172, %rd312;
	st.shared.u32 	[%rd313], %r1006;
	and.b32  	%r1082, %r1007, 31;
	mul.lo.s32 	%r1083, %r1082, 257;
	add.s32 	%r1084, %r776, %r1083;
	mul.wide.u32 	%rd314, %r1084, 4;
	add.s64 	%rd315, %rd172, %rd314;
	st.shared.u32 	[%rd315], %r1022;
	cvt.u64.u32 	%rd316, %r1083;
	add.s64 	%rd317, %rd10, %rd316;
	shl.b64 	%rd318, %rd317, 2;
	add.s64 	%rd319, %rd172, %rd318;
	st.shared.u32 	[%rd319+4], %r1023;
	st.shared.u32 	[%rd319+8], %r1024;
	st.shared.u32 	[%rd319+12], %r1025;
	add.s32 	%r1085, %r1083, %r71;
	mul.wide.u32 	%rd320, %r1085, 4;
	add.s64 	%rd321, %rd172, %rd320;
	st.shared.u32 	[%rd321], %r1036;
	add.s32 	%r1086, %r1083, %r72;
	mul.wide.u32 	%rd322, %r1086, 4;
	add.s64 	%rd323, %rd172, %rd322;
	st.shared.u32 	[%rd323], %r1037;
	add.s32 	%r1087, %r1083, %r73;
	mul.wide.u32 	%rd324, %r1087, 4;
	add.s64 	%rd325, %rd172, %rd324;
	st.shared.u32 	[%rd325], %r1038;
	add.s32 	%r1088, %r1083, %r74;
	mul.wide.u32 	%rd326, %r1088, 4;
	add.s64 	%rd327, %rd172, %rd326;
	st.shared.u32 	[%rd327], %r1039;
	bar.sync 	0;
	add.s32 	%r103, %r2412, %r2411;
	mov.u32 	%r2419, %r1101;
$L__BB0_14:                             // %L21862
                                        //   Parent Loop BB0_11 Depth=1
                                        //     Parent Loop BB0_13 Depth=2
                                        // =>    This Inner Loop Header: Depth=3
	or.b32  	%r2125, %r2419, %r75;
	add.s32 	%r2126, %r103, %r2125;
	shr.s32 	%r2127, %r2126, 31;
	shr.u32 	%r2128, %r2127, 27;
	add.s32 	%r2129, %r2126, %r2128;
	and.b32  	%r2130, %r2129, -32;
	sub.s32 	%r2131, %r2126, %r2130;
	mad.lo.s32 	%r2132, %r2131, 257, %r76;
	mul.wide.s32 	%rd328, %r2132, 4;
	add.s64 	%rd330, %rd172, %rd328;
	ld.shared.u32 	%r1090, [%rd330];
	mov.u32 	%r1091, 134744072;
	mov.u32 	%r1092, 252645135;
	// begin inline asm
	lop3.b32 %r1089, %r1090, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2133, %r1089, 2021161080;
	xor.b32  	%r1100, %r2133, -2139062144;
	shr.u32 	%r1094, %r1090, 4;
	// begin inline asm
	lop3.b32 %r1093, %r1094, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2134, %r1093, 2021161080;
	xor.b32  	%r1106, %r2134, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1097, %r1098}, {%r1099}, {%r1100}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1103, %r1104}, {%r1105}, {%r1106}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1109, %r1110}, {%r1099}, {%r1106}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1115, %r1116}, {%r1105}, {%r1100}, {%r1109, %r1110};
	// end inline asm
	ld.shared.u32 	%r1122, [%rd330+4];
	// begin inline asm
	lop3.b32 %r1121, %r1122, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2135, %r1121, 2021161080;
	xor.b32  	%r1132, %r2135, -2139062144;
	shr.u32 	%r1126, %r1122, 4;
	// begin inline asm
	lop3.b32 %r1125, %r1126, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2136, %r1125, 2021161080;
	xor.b32  	%r1138, %r2136, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1129, %r1130}, {%r1131}, {%r1132}, {%r1097, %r1098};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1135, %r1136}, {%r1137}, {%r1138}, {%r1103, %r1104};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1141, %r1142}, {%r1131}, {%r1138}, {%r1115, %r1116};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1147, %r1148}, {%r1137}, {%r1132}, {%r1141, %r1142};
	// end inline asm
	ld.shared.u32 	%r1154, [%rd330+8];
	// begin inline asm
	lop3.b32 %r1153, %r1154, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2137, %r1153, 2021161080;
	xor.b32  	%r1164, %r2137, -2139062144;
	shr.u32 	%r1158, %r1154, 4;
	// begin inline asm
	lop3.b32 %r1157, %r1158, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2138, %r1157, 2021161080;
	xor.b32  	%r1170, %r2138, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1161, %r1162}, {%r1163}, {%r1164}, {%r1129, %r1130};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1167, %r1168}, {%r1169}, {%r1170}, {%r1135, %r1136};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1173, %r1174}, {%r1163}, {%r1170}, {%r1147, %r1148};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1179, %r1180}, {%r1169}, {%r1164}, {%r1173, %r1174};
	// end inline asm
	ld.shared.u32 	%r1186, [%rd330+12];
	// begin inline asm
	lop3.b32 %r1185, %r1186, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2139, %r1185, 2021161080;
	xor.b32  	%r1196, %r2139, -2139062144;
	shr.u32 	%r1190, %r1186, 4;
	// begin inline asm
	lop3.b32 %r1189, %r1190, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2140, %r1189, 2021161080;
	xor.b32  	%r1202, %r2140, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1193, %r1194}, {%r1195}, {%r1196}, {%r1161, %r1162};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1199, %r1200}, {%r1201}, {%r1202}, {%r1167, %r1168};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1205, %r1206}, {%r1195}, {%r1202}, {%r1179, %r1180};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1211, %r1212}, {%r1201}, {%r1196}, {%r1205, %r1206};
	// end inline asm
	ld.shared.u32 	%r1218, [%rd330+16];
	// begin inline asm
	lop3.b32 %r1217, %r1218, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2141, %r1217, 2021161080;
	xor.b32  	%r1228, %r2141, -2139062144;
	shr.u32 	%r1222, %r1218, 4;
	// begin inline asm
	lop3.b32 %r1221, %r1222, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2142, %r1221, 2021161080;
	xor.b32  	%r1234, %r2142, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1225, %r1226}, {%r1227}, {%r1228}, {%r1193, %r1194};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1231, %r1232}, {%r1233}, {%r1234}, {%r1199, %r1200};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1237, %r1238}, {%r1227}, {%r1234}, {%r1211, %r1212};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1243, %r1244}, {%r1233}, {%r1228}, {%r1237, %r1238};
	// end inline asm
	ld.shared.u32 	%r1250, [%rd330+20];
	// begin inline asm
	lop3.b32 %r1249, %r1250, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2143, %r1249, 2021161080;
	xor.b32  	%r1260, %r2143, -2139062144;
	shr.u32 	%r1254, %r1250, 4;
	// begin inline asm
	lop3.b32 %r1253, %r1254, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2144, %r1253, 2021161080;
	xor.b32  	%r1266, %r2144, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1257, %r1258}, {%r1259}, {%r1260}, {%r1225, %r1226};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1263, %r1264}, {%r1265}, {%r1266}, {%r1231, %r1232};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1269, %r1270}, {%r1259}, {%r1266}, {%r1243, %r1244};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1275, %r1276}, {%r1265}, {%r1260}, {%r1269, %r1270};
	// end inline asm
	ld.shared.u32 	%r1282, [%rd330+24];
	// begin inline asm
	lop3.b32 %r1281, %r1282, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2145, %r1281, 2021161080;
	xor.b32  	%r1292, %r2145, -2139062144;
	shr.u32 	%r1286, %r1282, 4;
	// begin inline asm
	lop3.b32 %r1285, %r1286, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2146, %r1285, 2021161080;
	xor.b32  	%r1298, %r2146, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1289, %r1290}, {%r1291}, {%r1292}, {%r1257, %r1258};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1295, %r1296}, {%r1297}, {%r1298}, {%r1263, %r1264};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1301, %r1302}, {%r1291}, {%r1298}, {%r1275, %r1276};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1307, %r1308}, {%r1297}, {%r1292}, {%r1301, %r1302};
	// end inline asm
	ld.shared.u32 	%r1314, [%rd330+28];
	// begin inline asm
	lop3.b32 %r1313, %r1314, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2147, %r1313, 2021161080;
	xor.b32  	%r1324, %r2147, -2139062144;
	shr.u32 	%r1318, %r1314, 4;
	// begin inline asm
	lop3.b32 %r1317, %r1318, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2148, %r1317, 2021161080;
	xor.b32  	%r1330, %r2148, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1321, %r1322}, {%r1323}, {%r1324}, {%r1289, %r1290};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1327, %r1328}, {%r1329}, {%r1330}, {%r1295, %r1296};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1333, %r1334}, {%r1323}, {%r1330}, {%r1307, %r1308};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1339, %r1340}, {%r1329}, {%r1324}, {%r1333, %r1334};
	// end inline asm
	ld.shared.u32 	%r1346, [%rd330+32];
	// begin inline asm
	lop3.b32 %r1345, %r1346, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2149, %r1345, 2021161080;
	xor.b32  	%r1356, %r2149, -2139062144;
	shr.u32 	%r1350, %r1346, 4;
	// begin inline asm
	lop3.b32 %r1349, %r1350, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2150, %r1349, 2021161080;
	xor.b32  	%r1362, %r2150, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1353, %r1354}, {%r1355}, {%r1356}, {%r1321, %r1322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1359, %r1360}, {%r1361}, {%r1362}, {%r1327, %r1328};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1365, %r1366}, {%r1355}, {%r1362}, {%r1339, %r1340};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1371, %r1372}, {%r1361}, {%r1356}, {%r1365, %r1366};
	// end inline asm
	ld.shared.u32 	%r1378, [%rd330+36];
	// begin inline asm
	lop3.b32 %r1377, %r1378, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2151, %r1377, 2021161080;
	xor.b32  	%r1388, %r2151, -2139062144;
	shr.u32 	%r1382, %r1378, 4;
	// begin inline asm
	lop3.b32 %r1381, %r1382, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2152, %r1381, 2021161080;
	xor.b32  	%r1394, %r2152, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1385, %r1386}, {%r1387}, {%r1388}, {%r1353, %r1354};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1391, %r1392}, {%r1393}, {%r1394}, {%r1359, %r1360};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1397, %r1398}, {%r1387}, {%r1394}, {%r1371, %r1372};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1403, %r1404}, {%r1393}, {%r1388}, {%r1397, %r1398};
	// end inline asm
	ld.shared.u32 	%r1410, [%rd330+40];
	// begin inline asm
	lop3.b32 %r1409, %r1410, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2153, %r1409, 2021161080;
	xor.b32  	%r1420, %r2153, -2139062144;
	shr.u32 	%r1414, %r1410, 4;
	// begin inline asm
	lop3.b32 %r1413, %r1414, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2154, %r1413, 2021161080;
	xor.b32  	%r1426, %r2154, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1417, %r1418}, {%r1419}, {%r1420}, {%r1385, %r1386};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1423, %r1424}, {%r1425}, {%r1426}, {%r1391, %r1392};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1429, %r1430}, {%r1419}, {%r1426}, {%r1403, %r1404};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1435, %r1436}, {%r1425}, {%r1420}, {%r1429, %r1430};
	// end inline asm
	ld.shared.u32 	%r1442, [%rd330+44];
	// begin inline asm
	lop3.b32 %r1441, %r1442, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2155, %r1441, 2021161080;
	xor.b32  	%r1452, %r2155, -2139062144;
	shr.u32 	%r1446, %r1442, 4;
	// begin inline asm
	lop3.b32 %r1445, %r1446, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2156, %r1445, 2021161080;
	xor.b32  	%r1458, %r2156, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1449, %r1450}, {%r1451}, {%r1452}, {%r1417, %r1418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1455, %r1456}, {%r1457}, {%r1458}, {%r1423, %r1424};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1461, %r1462}, {%r1451}, {%r1458}, {%r1435, %r1436};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1467, %r1468}, {%r1457}, {%r1452}, {%r1461, %r1462};
	// end inline asm
	ld.shared.u32 	%r1474, [%rd330+48];
	// begin inline asm
	lop3.b32 %r1473, %r1474, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2157, %r1473, 2021161080;
	xor.b32  	%r1484, %r2157, -2139062144;
	shr.u32 	%r1478, %r1474, 4;
	// begin inline asm
	lop3.b32 %r1477, %r1478, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2158, %r1477, 2021161080;
	xor.b32  	%r1490, %r2158, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1481, %r1482}, {%r1483}, {%r1484}, {%r1449, %r1450};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1487, %r1488}, {%r1489}, {%r1490}, {%r1455, %r1456};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1493, %r1494}, {%r1483}, {%r1490}, {%r1467, %r1468};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1499, %r1500}, {%r1489}, {%r1484}, {%r1493, %r1494};
	// end inline asm
	ld.shared.u32 	%r1506, [%rd330+52];
	// begin inline asm
	lop3.b32 %r1505, %r1506, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2159, %r1505, 2021161080;
	xor.b32  	%r1516, %r2159, -2139062144;
	shr.u32 	%r1510, %r1506, 4;
	// begin inline asm
	lop3.b32 %r1509, %r1510, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2160, %r1509, 2021161080;
	xor.b32  	%r1522, %r2160, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1513, %r1514}, {%r1515}, {%r1516}, {%r1481, %r1482};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1519, %r1520}, {%r1521}, {%r1522}, {%r1487, %r1488};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1525, %r1526}, {%r1515}, {%r1522}, {%r1499, %r1500};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1531, %r1532}, {%r1521}, {%r1516}, {%r1525, %r1526};
	// end inline asm
	ld.shared.u32 	%r1538, [%rd330+56];
	// begin inline asm
	lop3.b32 %r1537, %r1538, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2161, %r1537, 2021161080;
	xor.b32  	%r1548, %r2161, -2139062144;
	shr.u32 	%r1542, %r1538, 4;
	// begin inline asm
	lop3.b32 %r1541, %r1542, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2162, %r1541, 2021161080;
	xor.b32  	%r1554, %r2162, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1545, %r1546}, {%r1547}, {%r1548}, {%r1513, %r1514};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1551, %r1552}, {%r1553}, {%r1554}, {%r1519, %r1520};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1557, %r1558}, {%r1547}, {%r1554}, {%r1531, %r1532};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1563, %r1564}, {%r1553}, {%r1548}, {%r1557, %r1558};
	// end inline asm
	ld.shared.u32 	%r1570, [%rd330+60];
	// begin inline asm
	lop3.b32 %r1569, %r1570, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2163, %r1569, 2021161080;
	xor.b32  	%r1580, %r2163, -2139062144;
	shr.u32 	%r1574, %r1570, 4;
	// begin inline asm
	lop3.b32 %r1573, %r1574, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2164, %r1573, 2021161080;
	xor.b32  	%r1586, %r2164, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1577, %r1578}, {%r1579}, {%r1580}, {%r1545, %r1546};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1583, %r1584}, {%r1585}, {%r1586}, {%r1551, %r1552};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1589, %r1590}, {%r1579}, {%r1586}, {%r1563, %r1564};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1595, %r1596}, {%r1585}, {%r1580}, {%r1589, %r1590};
	// end inline asm
	sub.s32 	%r2165, %r1577, %r1583;
	add.s32 	%r2166, %r2165, 8;
	shr.s32 	%r1603, %r2166, 4;
	add.s32 	%r2167, %r1595, 8;
	shr.s32 	%r1602, %r2167, 4;
	sub.s32 	%r2168, %r1578, %r1584;
	add.s32 	%r2169, %r2168, 8;
	shr.s32 	%r1606, %r2169, 4;
	add.s32 	%r2170, %r1596, 8;
	shr.s32 	%r1605, %r2170, 4;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1601, %r1602, %r1603;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1604, %r1605, %r1606;
	// end inline asm
	or.b32  	%r2171, %r77, %r2419;
	mul.lo.s32 	%r2172, %r2171, 20;
	add.s32 	%r2173, %r2172, %r78;
	mul.wide.u32 	%rd331, %r2173, 4;
	add.s64 	%rd333, %rd181, %rd331;
	st.shared.u32 	[%rd333], %r1601;
	add.s32 	%r2174, %r2172, 20;
	add.s32 	%r2175, %r2174, %r78;
	mul.wide.u32 	%rd334, %r2175, 4;
	add.s64 	%rd335, %rd181, %rd334;
	st.shared.u32 	[%rd335], %r1604;
	ld.shared.u32 	%r1608, [%rd330];
	// begin inline asm
	lop3.b32 %r1607, %r1608, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2176, %r1607, 2021161080;
	xor.b32  	%r1618, %r2176, -2139062144;
	shr.u32 	%r1612, %r1608, 4;
	// begin inline asm
	lop3.b32 %r1611, %r1612, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2177, %r1611, 2021161080;
	xor.b32  	%r1624, %r2177, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1615, %r1616}, {%r1617}, {%r1618}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1621, %r1622}, {%r1623}, {%r1624}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1627, %r1628}, {%r1617}, {%r1624}, {%r1101, %r1101};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1633, %r1634}, {%r1623}, {%r1618}, {%r1627, %r1628};
	// end inline asm
	ld.shared.u32 	%r1640, [%rd330+4];
	// begin inline asm
	lop3.b32 %r1639, %r1640, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2178, %r1639, 2021161080;
	xor.b32  	%r1650, %r2178, -2139062144;
	shr.u32 	%r1644, %r1640, 4;
	// begin inline asm
	lop3.b32 %r1643, %r1644, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2179, %r1643, 2021161080;
	xor.b32  	%r1656, %r2179, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1647, %r1648}, {%r1649}, {%r1650}, {%r1615, %r1616};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1653, %r1654}, {%r1655}, {%r1656}, {%r1621, %r1622};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1659, %r1660}, {%r1649}, {%r1656}, {%r1633, %r1634};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1665, %r1666}, {%r1655}, {%r1650}, {%r1659, %r1660};
	// end inline asm
	ld.shared.u32 	%r1672, [%rd330+8];
	// begin inline asm
	lop3.b32 %r1671, %r1672, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2180, %r1671, 2021161080;
	xor.b32  	%r1682, %r2180, -2139062144;
	shr.u32 	%r1676, %r1672, 4;
	// begin inline asm
	lop3.b32 %r1675, %r1676, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2181, %r1675, 2021161080;
	xor.b32  	%r1688, %r2181, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1679, %r1680}, {%r1681}, {%r1682}, {%r1647, %r1648};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1685, %r1686}, {%r1687}, {%r1688}, {%r1653, %r1654};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1691, %r1692}, {%r1681}, {%r1688}, {%r1665, %r1666};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1697, %r1698}, {%r1687}, {%r1682}, {%r1691, %r1692};
	// end inline asm
	ld.shared.u32 	%r1704, [%rd330+12];
	// begin inline asm
	lop3.b32 %r1703, %r1704, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2182, %r1703, 2021161080;
	xor.b32  	%r1714, %r2182, -2139062144;
	shr.u32 	%r1708, %r1704, 4;
	// begin inline asm
	lop3.b32 %r1707, %r1708, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2183, %r1707, 2021161080;
	xor.b32  	%r1720, %r2183, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1711, %r1712}, {%r1713}, {%r1714}, {%r1679, %r1680};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1717, %r1718}, {%r1719}, {%r1720}, {%r1685, %r1686};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1723, %r1724}, {%r1713}, {%r1720}, {%r1697, %r1698};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1729, %r1730}, {%r1719}, {%r1714}, {%r1723, %r1724};
	// end inline asm
	ld.shared.u32 	%r1736, [%rd330+16];
	// begin inline asm
	lop3.b32 %r1735, %r1736, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2184, %r1735, 2021161080;
	xor.b32  	%r1746, %r2184, -2139062144;
	shr.u32 	%r1740, %r1736, 4;
	// begin inline asm
	lop3.b32 %r1739, %r1740, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2185, %r1739, 2021161080;
	xor.b32  	%r1752, %r2185, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1743, %r1744}, {%r1745}, {%r1746}, {%r1711, %r1712};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1749, %r1750}, {%r1751}, {%r1752}, {%r1717, %r1718};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1755, %r1756}, {%r1745}, {%r1752}, {%r1729, %r1730};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1761, %r1762}, {%r1751}, {%r1746}, {%r1755, %r1756};
	// end inline asm
	ld.shared.u32 	%r1768, [%rd330+20];
	// begin inline asm
	lop3.b32 %r1767, %r1768, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2186, %r1767, 2021161080;
	xor.b32  	%r1778, %r2186, -2139062144;
	shr.u32 	%r1772, %r1768, 4;
	// begin inline asm
	lop3.b32 %r1771, %r1772, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2187, %r1771, 2021161080;
	xor.b32  	%r1784, %r2187, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1775, %r1776}, {%r1777}, {%r1778}, {%r1743, %r1744};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1781, %r1782}, {%r1783}, {%r1784}, {%r1749, %r1750};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1787, %r1788}, {%r1777}, {%r1784}, {%r1761, %r1762};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1793, %r1794}, {%r1783}, {%r1778}, {%r1787, %r1788};
	// end inline asm
	ld.shared.u32 	%r1800, [%rd330+24];
	// begin inline asm
	lop3.b32 %r1799, %r1800, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2188, %r1799, 2021161080;
	xor.b32  	%r1810, %r2188, -2139062144;
	shr.u32 	%r1804, %r1800, 4;
	// begin inline asm
	lop3.b32 %r1803, %r1804, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2189, %r1803, 2021161080;
	xor.b32  	%r1816, %r2189, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1807, %r1808}, {%r1809}, {%r1810}, {%r1775, %r1776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1813, %r1814}, {%r1815}, {%r1816}, {%r1781, %r1782};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1819, %r1820}, {%r1809}, {%r1816}, {%r1793, %r1794};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1825, %r1826}, {%r1815}, {%r1810}, {%r1819, %r1820};
	// end inline asm
	ld.shared.u32 	%r1832, [%rd330+28];
	// begin inline asm
	lop3.b32 %r1831, %r1832, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2190, %r1831, 2021161080;
	xor.b32  	%r1842, %r2190, -2139062144;
	shr.u32 	%r1836, %r1832, 4;
	// begin inline asm
	lop3.b32 %r1835, %r1836, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2191, %r1835, 2021161080;
	xor.b32  	%r1848, %r2191, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1839, %r1840}, {%r1841}, {%r1842}, {%r1807, %r1808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1845, %r1846}, {%r1847}, {%r1848}, {%r1813, %r1814};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1851, %r1852}, {%r1841}, {%r1848}, {%r1825, %r1826};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1857, %r1858}, {%r1847}, {%r1842}, {%r1851, %r1852};
	// end inline asm
	ld.shared.u32 	%r1864, [%rd330+32];
	// begin inline asm
	lop3.b32 %r1863, %r1864, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2192, %r1863, 2021161080;
	xor.b32  	%r1874, %r2192, -2139062144;
	shr.u32 	%r1868, %r1864, 4;
	// begin inline asm
	lop3.b32 %r1867, %r1868, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2193, %r1867, 2021161080;
	xor.b32  	%r1880, %r2193, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1871, %r1872}, {%r1873}, {%r1874}, {%r1839, %r1840};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1877, %r1878}, {%r1879}, {%r1880}, {%r1845, %r1846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1883, %r1884}, {%r1873}, {%r1880}, {%r1857, %r1858};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1889, %r1890}, {%r1879}, {%r1874}, {%r1883, %r1884};
	// end inline asm
	ld.shared.u32 	%r1896, [%rd330+36];
	// begin inline asm
	lop3.b32 %r1895, %r1896, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2194, %r1895, 2021161080;
	xor.b32  	%r1906, %r2194, -2139062144;
	shr.u32 	%r1900, %r1896, 4;
	// begin inline asm
	lop3.b32 %r1899, %r1900, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2195, %r1899, 2021161080;
	xor.b32  	%r1912, %r2195, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1903, %r1904}, {%r1905}, {%r1906}, {%r1871, %r1872};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1909, %r1910}, {%r1911}, {%r1912}, {%r1877, %r1878};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1915, %r1916}, {%r1905}, {%r1912}, {%r1889, %r1890};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1921, %r1922}, {%r1911}, {%r1906}, {%r1915, %r1916};
	// end inline asm
	ld.shared.u32 	%r1928, [%rd330+40];
	// begin inline asm
	lop3.b32 %r1927, %r1928, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2196, %r1927, 2021161080;
	xor.b32  	%r1938, %r2196, -2139062144;
	shr.u32 	%r1932, %r1928, 4;
	// begin inline asm
	lop3.b32 %r1931, %r1932, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2197, %r1931, 2021161080;
	xor.b32  	%r1944, %r2197, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1935, %r1936}, {%r1937}, {%r1938}, {%r1903, %r1904};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1941, %r1942}, {%r1943}, {%r1944}, {%r1909, %r1910};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1947, %r1948}, {%r1937}, {%r1944}, {%r1921, %r1922};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1953, %r1954}, {%r1943}, {%r1938}, {%r1947, %r1948};
	// end inline asm
	ld.shared.u32 	%r1960, [%rd330+44];
	// begin inline asm
	lop3.b32 %r1959, %r1960, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2198, %r1959, 2021161080;
	xor.b32  	%r1970, %r2198, -2139062144;
	shr.u32 	%r1964, %r1960, 4;
	// begin inline asm
	lop3.b32 %r1963, %r1964, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2199, %r1963, 2021161080;
	xor.b32  	%r1976, %r2199, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1967, %r1968}, {%r1969}, {%r1970}, {%r1935, %r1936};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1973, %r1974}, {%r1975}, {%r1976}, {%r1941, %r1942};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1979, %r1980}, {%r1969}, {%r1976}, {%r1953, %r1954};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1985, %r1986}, {%r1975}, {%r1970}, {%r1979, %r1980};
	// end inline asm
	ld.shared.u32 	%r1992, [%rd330+48];
	// begin inline asm
	lop3.b32 %r1991, %r1992, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2200, %r1991, 2021161080;
	xor.b32  	%r2002, %r2200, -2139062144;
	shr.u32 	%r1996, %r1992, 4;
	// begin inline asm
	lop3.b32 %r1995, %r1996, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2201, %r1995, 2021161080;
	xor.b32  	%r2008, %r2201, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1999, %r2000}, {%r2001}, {%r2002}, {%r1967, %r1968};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2005, %r2006}, {%r2007}, {%r2008}, {%r1973, %r1974};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2011, %r2012}, {%r2001}, {%r2008}, {%r1985, %r1986};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2017, %r2018}, {%r2007}, {%r2002}, {%r2011, %r2012};
	// end inline asm
	ld.shared.u32 	%r2024, [%rd330+52];
	// begin inline asm
	lop3.b32 %r2023, %r2024, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2202, %r2023, 2021161080;
	xor.b32  	%r2034, %r2202, -2139062144;
	shr.u32 	%r2028, %r2024, 4;
	// begin inline asm
	lop3.b32 %r2027, %r2028, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2203, %r2027, 2021161080;
	xor.b32  	%r2040, %r2203, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2031, %r2032}, {%r2033}, {%r2034}, {%r1999, %r2000};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2037, %r2038}, {%r2039}, {%r2040}, {%r2005, %r2006};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2043, %r2044}, {%r2033}, {%r2040}, {%r2017, %r2018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2049, %r2050}, {%r2039}, {%r2034}, {%r2043, %r2044};
	// end inline asm
	ld.shared.u32 	%r2056, [%rd330+56];
	// begin inline asm
	lop3.b32 %r2055, %r2056, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2204, %r2055, 2021161080;
	xor.b32  	%r2066, %r2204, -2139062144;
	shr.u32 	%r2060, %r2056, 4;
	// begin inline asm
	lop3.b32 %r2059, %r2060, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2205, %r2059, 2021161080;
	xor.b32  	%r2072, %r2205, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2063, %r2064}, {%r2065}, {%r2066}, {%r2031, %r2032};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2069, %r2070}, {%r2071}, {%r2072}, {%r2037, %r2038};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2075, %r2076}, {%r2065}, {%r2072}, {%r2049, %r2050};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2081, %r2082}, {%r2071}, {%r2066}, {%r2075, %r2076};
	// end inline asm
	ld.shared.u32 	%r2088, [%rd330+60];
	// begin inline asm
	lop3.b32 %r2087, %r2088, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2206, %r2087, 2021161080;
	xor.b32  	%r2098, %r2206, -2139062144;
	shr.u32 	%r2092, %r2088, 4;
	// begin inline asm
	lop3.b32 %r2091, %r2092, %r1091, %r1092, 40;
	// end inline asm
	add.s32 	%r2207, %r2091, 2021161080;
	xor.b32  	%r2104, %r2207, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2095, %r2096}, {%r2097}, {%r2098}, {%r2063, %r2064};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2101, %r2102}, {%r2103}, {%r2104}, {%r2069, %r2070};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2107, %r2108}, {%r2097}, {%r2104}, {%r2081, %r2082};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2113, %r2114}, {%r2103}, {%r2098}, {%r2107, %r2108};
	// end inline asm
	sub.s32 	%r2208, %r2095, %r2101;
	add.s32 	%r2209, %r2208, 8;
	shr.s32 	%r2121, %r2209, 4;
	add.s32 	%r2210, %r2113, 8;
	shr.s32 	%r2120, %r2210, 4;
	sub.s32 	%r2211, %r2096, %r2102;
	add.s32 	%r2212, %r2211, 8;
	shr.s32 	%r2124, %r2212, 4;
	add.s32 	%r2213, %r2114, 8;
	shr.s32 	%r2123, %r2213, 4;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2119, %r2120, %r2121;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2122, %r2123, %r2124;
	// end inline asm
	add.s32 	%r2214, %r2172, %r79;
	mul.wide.u32 	%rd336, %r2214, 4;
	add.s64 	%rd337, %rd181, %rd336;
	st.shared.u32 	[%rd337], %r2119;
	add.s32 	%r2215, %r2174, %r79;
	mul.wide.u32 	%rd338, %r2215, 4;
	add.s64 	%rd339, %rd181, %rd338;
	st.shared.u32 	[%rd339], %r2122;
	add.s32 	%r2419, %r2419, 8;
	setp.ne.s32 	%p41, %r2419, 32;
	@%p41 bra 	$L__BB0_14;
// %bb.15:                              // %L28463
                                        //   in Loop: Header=BB0_13 Depth=2
	bar.sync 	0;
	ld.shared.u32 	%r2234, [%rd16];
	ld.shared.u32 	%r2235, [%rd17];
	ld.shared.u32 	%r2236, [%rd18];
	ld.shared.u32 	%r2237, [%rd19];
	ld.shared.u32 	%r2238, [%rd20];
	ld.shared.u32 	%r2239, [%rd21];
	ld.shared.u32 	%r2240, [%rd22];
	ld.shared.u32 	%r2241, [%rd23];
	ld.shared.u32 	%r2242, [%rd24];
	ld.shared.u32 	%r2243, [%rd25];
	ld.shared.u32 	%r2244, [%rd26];
	ld.shared.u32 	%r2245, [%rd27];
	ld.shared.u32 	%r2246, [%rd28];
	ld.shared.u32 	%r2247, [%rd29];
	ld.shared.u32 	%r2248, [%rd30];
	ld.shared.u32 	%r2249, [%rd31];
	cvt.s32.s16 	%r2250, %r2234;
	shr.s32 	%r2251, %r2234, 16;
	cvt.s32.s16 	%r2252, %r2235;
	shr.s32 	%r2253, %r2235, 16;
	cvt.s32.s16 	%r2254, %r2236;
	shr.s32 	%r2255, %r2236, 16;
	cvt.s32.s16 	%r2256, %r2237;
	shr.s32 	%r2257, %r2237, 16;
	cvt.s32.s16 	%r2258, %r2238;
	shr.s32 	%r2259, %r2238, 16;
	cvt.s32.s16 	%r2260, %r2239;
	shr.s32 	%r2261, %r2239, 16;
	cvt.s32.s16 	%r2262, %r2240;
	shr.s32 	%r2263, %r2240, 16;
	cvt.s32.s16 	%r2264, %r2241;
	shr.s32 	%r2265, %r2241, 16;
	cvt.s32.s16 	%r2266, %r2242;
	shr.s32 	%r2267, %r2242, 16;
	cvt.s32.s16 	%r2268, %r2243;
	shr.s32 	%r2269, %r2243, 16;
	cvt.s32.s16 	%r2270, %r2244;
	shr.s32 	%r2271, %r2244, 16;
	cvt.s32.s16 	%r2272, %r2245;
	shr.s32 	%r2273, %r2245, 16;
	cvt.s32.s16 	%r2274, %r2246;
	shr.s32 	%r2275, %r2246, 16;
	cvt.s32.s16 	%r2276, %r2247;
	shr.s32 	%r2277, %r2247, 16;
	cvt.s32.s16 	%r2278, %r2248;
	shr.s32 	%r2279, %r2248, 16;
	cvt.s32.s16 	%r2280, %r2249;
	shr.s32 	%r2281, %r2249, 16;
	add.s32 	%r2282, %r2250, %r80;
	add.s32 	%r2283, %r2282, %r2252;
	add.s32 	%r2284, %r2283, %r2254;
	add.s32 	%r2285, %r2284, %r2256;
	shr.s32 	%r2286, %r2285, %r81;
	add.s32 	%r2287, %r2251, %r80;
	add.s32 	%r2288, %r2287, %r2253;
	add.s32 	%r2289, %r2288, %r2255;
	add.s32 	%r2290, %r2289, %r2257;
	shr.s32 	%r2291, %r2290, %r81;
	add.s32 	%r2292, %r2258, %r80;
	add.s32 	%r2293, %r2292, %r2260;
	add.s32 	%r2294, %r2293, %r2262;
	add.s32 	%r2295, %r2294, %r2264;
	shr.s32 	%r2296, %r2295, %r81;
	add.s32 	%r2297, %r2259, %r80;
	add.s32 	%r2298, %r2297, %r2261;
	add.s32 	%r2299, %r2298, %r2263;
	add.s32 	%r2300, %r2299, %r2265;
	shr.s32 	%r2301, %r2300, %r81;
	add.s32 	%r2302, %r2266, %r80;
	add.s32 	%r2303, %r2302, %r2268;
	add.s32 	%r2304, %r2303, %r2270;
	add.s32 	%r2305, %r2304, %r2272;
	shr.s32 	%r2306, %r2305, %r81;
	add.s32 	%r2307, %r2267, %r80;
	add.s32 	%r2308, %r2307, %r2269;
	add.s32 	%r2309, %r2308, %r2271;
	add.s32 	%r2310, %r2309, %r2273;
	shr.s32 	%r2311, %r2310, %r81;
	add.s32 	%r2312, %r2274, %r80;
	add.s32 	%r2313, %r2312, %r2276;
	add.s32 	%r2314, %r2313, %r2278;
	add.s32 	%r2315, %r2314, %r2280;
	shr.s32 	%r2316, %r2315, %r81;
	add.s32 	%r2317, %r2275, %r80;
	add.s32 	%r2318, %r2317, %r2277;
	add.s32 	%r2319, %r2318, %r2279;
	add.s32 	%r2320, %r2319, %r2281;
	shr.s32 	%r2321, %r2320, %r81;
	max.s32 	%r2322, %r2286, -7;
	min.s32 	%r2221, %r2322, 7;
	setp.ne.s32 	%p42, %r2221, %r2286;
	or.pred  	%p43, %p69, %p42;
	max.s32 	%r2323, %r2291, -7;
	min.s32 	%r2228, %r2323, 7;
	setp.ne.s32 	%p44, %r2228, %r2291;
	or.pred  	%p45, %p44, %p43;
	max.s32 	%r2324, %r2296, -7;
	min.s32 	%r2220, %r2324, 7;
	setp.ne.s32 	%p46, %r2220, %r2296;
	or.pred  	%p47, %p45, %p46;
	max.s32 	%r2325, %r2301, -7;
	min.s32 	%r2227, %r2325, 7;
	setp.ne.s32 	%p48, %r2227, %r2301;
	or.pred  	%p49, %p48, %p47;
	max.s32 	%r2326, %r2306, -7;
	min.s32 	%r2218, %r2326, 7;
	setp.ne.s32 	%p50, %r2218, %r2306;
	or.pred  	%p51, %p49, %p50;
	max.s32 	%r2327, %r2311, -7;
	min.s32 	%r2225, %r2327, 7;
	setp.ne.s32 	%p52, %r2225, %r2311;
	or.pred  	%p53, %p52, %p51;
	max.s32 	%r2328, %r2316, -7;
	min.s32 	%r2217, %r2328, 7;
	setp.ne.s32 	%p54, %r2217, %r2316;
	or.pred  	%p55, %p53, %p54;
	max.s32 	%r2329, %r2321, -7;
	min.s32 	%r2224, %r2329, 7;
	setp.ne.s32 	%p56, %r2224, %r2321;
	or.pred  	%p69, %p56, %p55;
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2216, %r2217, %r2218, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2219, %r2220, %r2221, %r2216;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2223, %r2224, %r2225, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2226, %r2227, %r2228, %r2223;
	// end inline asm
	shl.b32 	%r2233, %r2226, 4;
	// begin inline asm
	lop3.b32 %r2340, %r1092, %r2219, %r2233, 202;
	// end inline asm
	setp.eq.s32 	%p57, %r2412, 0;
	selp.b32 	%r2417, %r2340, %r2417, %p57;
	selp.b32 	%r2418, %r2340, %r2418, %p57;
	setp.eq.s32 	%p58, %r2412, 32;
	selp.b32 	%r2413, %r2340, %r2413, %p58;
	selp.b32 	%r2414, %r2340, %r2414, %p58;
	setp.eq.s32 	%p59, %r2412, 64;
	selp.b32 	%r2415, %r2340, %r2415, %p59;
	selp.b32 	%r2416, %r2340, %r2416, %p59;
	add.s32 	%r113, %r2412, 32;
	setp.ne.s32 	%p60, %r2412, 96;
	mov.u32 	%r2412, %r113;
	@%p60 bra 	$L__BB0_13;
// %bb.16:                              // %L30869.L30875_crit_edge
                                        //   in Loop: Header=BB0_11 Depth=1
	setp.eq.s32 	%p61, %r84, 0;
	setp.eq.s32 	%p62, %r83, 0;
	setp.eq.s32 	%p63, %r82, 0;
	// begin inline asm
	prmt.b32 %r2330, %r2417, %r2413, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2334, %r2418, %r2414, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2338, %r2415, %r2340, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2342, %r2416, %r2340, %r400;
	// end inline asm
	selp.b32 	%r2378, %r2334, %r2330, %p63;
	shfl.sync.bfly.b32	%r2379, %r2378, 1, 31, -1;
	selp.b32 	%r2347, %r2330, %r2379, %p63;
	selp.b32 	%r2348, %r2379, %r2334, %p63;
	selp.b32 	%r2380, %r2342, %r2338, %p63;
	shfl.sync.bfly.b32	%r2381, %r2380, 1, 31, -1;
	selp.b32 	%r2355, %r2338, %r2381, %p63;
	selp.b32 	%r2356, %r2381, %r2342, %p63;
	// begin inline asm
	prmt.b32 %r2346, %r2347, %r2348, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2350, %r2347, %r2348, %r400;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2354, %r2355, %r2356, %r396;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2358, %r2355, %r2356, %r400;
	// end inline asm
	selp.b32 	%r2382, %r2354, %r2346, %p62;
	shfl.sync.bfly.b32	%r2383, %r2382, 2, 31, -1;
	selp.b32 	%r2363, %r2346, %r2383, %p62;
	selp.b32 	%r2364, %r2383, %r2354, %p62;
	selp.b32 	%r2384, %r2358, %r2350, %p62;
	shfl.sync.bfly.b32	%r2385, %r2384, 2, 31, -1;
	selp.b32 	%r2371, %r2350, %r2385, %p62;
	selp.b32 	%r2372, %r2385, %r2358, %p62;
	// begin inline asm
	prmt.b32 %r2362, %r2363, %r2364, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2366, %r2363, %r2364, %r144;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2370, %r2371, %r2372, %r140;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2374, %r2371, %r2372, %r144;
	// end inline asm
	selp.b32 	%r2386, %r2370, %r2362, %p61;
	shfl.sync.bfly.b32	%r2387, %r2386, 4, 31, -1;
	selp.b32 	%r2388, %r2362, %r2387, %p61;
	selp.b32 	%r2389, %r2387, %r2370, %p61;
	selp.b32 	%r2390, %r2374, %r2366, %p61;
	shfl.sync.bfly.b32	%r2391, %r2390, 4, 31, -1;
	selp.b32 	%r2392, %r2366, %r2391, %p61;
	selp.b32 	%r2393, %r2391, %r2374, %p61;
	selp.b32 	%r2394, %r2392, %r2388, %p63;
	shfl.sync.bfly.b32	%r2395, %r2394, 1, 31, -1;
	selp.b32 	%r2396, %r2388, %r2395, %p63;
	selp.b32 	%r2397, %r2395, %r2392, %p63;
	selp.b32 	%r2398, %r2393, %r2389, %p63;
	shfl.sync.bfly.b32	%r2399, %r2398, 1, 31, -1;
	selp.b32 	%r2400, %r2389, %r2399, %p63;
	selp.b32 	%r2401, %r2399, %r2393, %p63;
	and.b32  	%r2402, %r2411, 16256;
	or.b32  	%r2403, %r2402, %r86;
	or.b32  	%r2404, %r85, %r2403;
	cvt.u64.u32 	%rd340, %r2404;
	add.s64 	%rd341, %rd4, %rd340;
	st.global.v4.u32 	[%rd341], {%r2396, %r2400, %r2397, %r2401};
	add.s32 	%r114, %r2411, 128;
	setp.ne.s32 	%p64, %r2411, 65408;
	mov.u32 	%r2411, %r114;
	@%p64 bra 	$L__BB0_11;
$L__BB0_17:                             // %L31256
	selp.u32 	%r2405, 1, 0, %p69;
	{ 
	.reg .pred 	%p1; 
	.reg .pred 	%p2; 
	setp.ne.u32 	%p1, %r2405, 0; 
	bar.red.or.pred 	%p2, 0, %p1; 
	selp.u32 	%r2406, 1, 0, %p2; 
	}
	setp.eq.s32 	%p65, %r2406, 0;
	or.pred  	%p66, %p65, %p1;
	@%p66 bra 	$L__BB0_19;
// %bb.18:                              // %L31287
	st.global.u32 	[%rd8], %r127;
$L__BB0_19:                             // %L31333
	mov.u32 	%r2408, 0;
	st.global.u32 	[%rd7], %r2408;
	ret;
$L__BB0_7:                              // %L152
	mov.u32 	%r2409, 2;
	st.global.u32 	[%rd7], %r2409;
	mov.u64 	%rd342, exception2743;
	cvta.global.u64 	%rd343, %rd342;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd343;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 6
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd32;
	st.param.b32 	[param0+8], %r119;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 7
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
$L__BB0_1:                              // %L8
	mov.u64 	%rd33, exception1;
	cvta.global.u64 	%rd34, %rd33;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd34;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 0
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd32;
	st.param.b32 	[param0+8], %r119;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 1
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
$L__BB0_3:                              // %L24
	mov.u64 	%rd35, exception1;
	cvta.global.u64 	%rd36, %rd35;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 2
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd32;
	st.param.b32 	[param0+8], %r119;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 3
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
$L__BB0_21:                             // %L370
	mov.u32 	%r134, 3;
	st.global.u32 	[%rd7], %r134;
	mov.u64 	%rd40, exception2743;
	cvta.global.u64 	%rd41, %rd40;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[16];
	st.param.b64 	[param0+0], %rd32;
	st.param.b32 	[param0+8], %r119;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	trap;
	trap;
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}

;Compilation command line:
; /opt/amd/llvm/bin/clang -x assembler -target amdgcn--amdhsa -mcpu=fiji -c -o test.o kern.isa
; /opt/amd/llvm/bin/clang -target amdgcn--amdhsa test.o -v -o test.hsaco

.hsa_code_object_version 2,0
.hsa_code_object_isa 8,0,3,"AMD","AMDGPU"

.text
.globl CHIME_X
.p2align 8
.amdgpu_hsa_kernel CHIME_X

CHIME_X:
  .amd_kernel_code_t

    kernel_code_version_major = 1
    kernel_code_version_minor = 0
    machine_kind = 1
    machine_version_major = 8
    machine_version_minor = 0
    machine_version_stepping = 3
    is_ptr64 = 1

    enable_sgpr_kernarg_segment_ptr = 1
    kernarg_segment_byte_size = 40

    ;initialize lx,ly to v0,v1
    compute_pgm_rsrc2_tidig_comp_cnt = 1

    ;initialize gy,gz to s3,s4 (3 not understood)
    compute_pgm_rsrc2_tgid_y_en = 1
    compute_pgm_rsrc2_tgid_z_en = 1
    compute_pgm_rsrc2_user_sgpr = 3

    workitem_vgpr_count = 47
    wavefront_sgpr_count = 24

    ;following http://gpuopen.com/amdgcn-assembly/
    ;compute_pgm_rsrc1_vgprs = (workitem_vgpr_count-1)/4
    ;compute_pgm_rsrc1_sgprs = (wavefront_sgpr_count-1)/8
    compute_pgm_rsrc1_vgprs = 12
    compute_pgm_rsrc1_sgprs = 3

    ;these lines are required for LDS (granularity is 512, I guess?)
    compute_pgm_rsrc2_lds_size = 7;6
    workgroup_group_segment_byte_size = 3136;3072 ;use stride of 49 to avoid bank conflicts

 .end_amd_kernel_code_t

;enable LDS
s_mov_b32 m0, -1

;s[0:1] contains input array
;s[10:11] contains the presum buffer
;s[14:15] contains config struct
;s[20:21] contains output array
;s[22:23] contains idx lookup
  s_load_dwordx2 s[10:11], s[0:1] 0x08
  s_load_dwordx2 s[14:15], s[0:1] 0x20
  s_load_dwordx2 s[20:21], s[0:1] 0x10
  s_load_dwordx2 s[22:23], s[0:1] 0x18
  s_load_dwordx2 s[0:1], s[0:1] 0x0
  s_waitcnt 0

;load config: N_ITER to s12
  s_load_dword s12, s[14:15] 0x08
  s_waitcnt 0

;load config: N_ELEM to s14, N_INTG to s15
  s_load_dwordx2 s[14:15], s[14:15]
;TODO: assert N_ELEM = N*32
  s_waitcnt 0


  ;v0, v1 are ix,iy
  ;ix = localx/2;
  ;iy = localy*2 + (localx&0x1);
  v_mov_b32 v6, v0
  v_and_b32 v4, 0x1, v0
  v_lshrrev_b32 v0, 1, v0
  v_mad_u32_u24 v1, v1, 2, v4

  ;extract blk_x, blk_y to s8, s9
  s_mul_i32 s8, 8, s4 ;8B per block id
  s_add_u32 s22, s22, s8
  s_addc_u32 s23, s23, 0
  s_load_dwordx2 s[8:9], s[22:23]
  s_waitcnt 0
  s_mul_i32 s8, 8, s8 ;8 WI per block
  s_mul_i32 s9, 8, s9 ;8 WI per block

  ;v3 == T = (ix+iy)%8 + (time_slice * N_INTG)
  v_add_u32_e32 v3, vcc, v1, v0
  v_and_b32_e32 v3, 0x7, v3
  v_mov_b32 v2, s15  ;N_INTG
  v_mad_u32_u24 v3, s3, v2, v3

;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; unpack presum here!!! ;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;  s_cmp_lt_u32 s3, 1000
  s_cmp_gt_u32 s3, 0
  s_cbranch_scc1 SKIP_PRESEED

    ;calculate base x
;    v_mov_b32 v20, s8         ;group_x
;    v_lshlrev_b32 v20, 2, v20 ;  * 32/8
;    v_lshlrev_b32 v21, 2, v0  ;  + 4*local
;    v_add_u32_e32 v21, vcc, v21, v20
;    v_lshlrev_b32 v21, 3, v21
;    v_add_u32_e32 v20, vcc, s10, v21
    v_add_u32_e32 v20, vcc, s8, v0 ;WI x
    v_lshlrev_b32 v20, 5, v20      ; 4 * 4B * 2 / WI
    v_add_u32_e32 v20, vcc, s10, v20
    v_mov_b32 v21, s11
    v_addc_u32 v21, vcc, v21, 0, vcc
    flat_load_dwordx4 v[22:25], v[20:21] ;load x's into v22-29
    v_add_u32_e32 v20, vcc, 16, v20
    v_addc_u32 v21, vcc, v21, 0, vcc
    flat_load_dwordx4 v[26:29], v[20:21] ;load x's into v22-29

    ;calculate base y
;    v_mov_b32 v30, s9         ;group_y
;    v_lshlrev_b32 v30, 2, v30 ;  * 32/8
;    v_lshlrev_b32 v31, 2, v1  ;  + 4*local
;    v_add_u32_e32 v31, vcc, v31, v30
;    v_lshlrev_b32 v31, 3, v31
;    v_add_u32_e32 v30, vcc, s10, v31
    v_add_u32_e32 v30, vcc, s9, v1 ;WI y
    v_lshlrev_b32 v30, 5, v30      ; 4 * 4B * 2 / WI
    v_add_u32_e32 v30, vcc, s10, v30
    v_mov_b32 v31, s11
    v_addc_u32 v31, vcc, v31, 0, vcc
    flat_load_dwordx4 v[32:35], v[30:31] ;load y's into v32-39
    v_add_u32_e32 v30, vcc, 16, v30
    v_addc_u32 v31, vcc, v31, 0, vcc
    flat_load_dwordx4 v[36:39], v[30:31] ;load y's into v32-39

    s_waitcnt 0
;   re += 128 * N_TIME - 8 * (xr+yr+xi+yi)
;   im += 8 * (xr-yr-xi+yi)

    s_mul_i32 s11, 128, s12 ;128 * N_TIME

   ;stride
    s_mov_b32 s5, 228  ;stride = 32*4*2 - 3*offset
    s_mov_b32 s6, 4

    ;write
    v_mov_b32 v10, 0                 ;global offset? [time? gate?]
    v_add_u32_e32 v10, vcc, s4, v10 ;block ID
    v_lshlrev_b32 v10, 3, v10       ;8 y / block
    v_add_u32_e32 v10, vcc, v1, v10 ;item Y
    v_lshlrev_b32 v10, 5, v10       ;32 x / y
    v_add_u32_e32 v10, vcc, v0, v10 ;item X
    v_lshlrev_b32 v10, 3, v10       ;8 output / x
    v_lshlrev_b32 v10, 2, v10       ;4B / output
    v_mov_b32 v20, s20               ;output addresss
    v_add_u32_e32 v20, vcc, v10, v20
    v_mov_b32 v21, s21
    v_addc_u32 v21, vcc, v21, 0, vcc

    ;x0y0
    ; IM = 8 * (xr-yr-xi+yi)
    ; RE = 128*NT - 8 * (xr+yr+xi+yi)
    v_add_u32_e32 v10, vcc, v23, v32 ;xr0+yi0
    v_add_u32_e32 v11, vcc, v22, v33 ;xi0+yr0
      v_sub_u32_e32 v12, vcc, v10, v11
    v_add_u32_e32 v13, vcc, v10, v11
    v_sub_u32_e32 v13, vcc, s11, v13
      ;v_add_u32_e32 v20, vcc, s6, v20
      ;v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v12
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v13
  ;x1y0
    v_add_u32_e32 v10, vcc, v25, v32 ;xr1+yi0
    v_add_u32_e32 v11, vcc, v24, v33 ;xi1+yr0
      v_sub_u32_e32 v14, vcc, v10, v11
    v_add_u32_e32 v15, vcc, v10, v11
    v_sub_u32_e32 v15, vcc, s11, v15
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v14
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v15
  ;x2y0
    v_add_u32_e32 v10, vcc, v27, v32 ;xr2+yi0
    v_add_u32_e32 v11, vcc, v26, v33 ;xi2+yr0
      v_sub_u32_e32 v16, vcc, v10, v11
    v_add_u32_e32 v17, vcc, v10, v11
    v_sub_u32_e32 v17, vcc, s11, v17
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v16
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v17
  ;x3y0
    v_add_u32_e32 v10, vcc, v29, v32 ;xr3+yi0
    v_add_u32_e32 v11, vcc, v28, v33 ;xi3+yr0
      v_sub_u32_e32 v18, vcc, v10, v11
    v_add_u32_e32 v19, vcc, v10, v11
    v_sub_u32_e32 v19, vcc, s11, v19
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v18
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v19

    s_waitcnt lgkmcnt(0)
    s_waitcnt 0



    ;x0y1
    ; IM = 8 * (xr-yr-xi+yi)
    ; RE = 128*NT - 8 * (xr+yr+xi+yi)
    v_add_u32_e32 v10, vcc, v23, v34 ;xr0+yi1
    v_add_u32_e32 v11, vcc, v22, v35 ;xi0+yr1
      v_sub_u32_e32 v12, vcc, v10, v11
    v_add_u32_e32 v13, vcc, v10, v11
    v_sub_u32_e32 v13, vcc, s11, v13
      v_add_u32_e32 v20, vcc, s5, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v12
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v13
  ;x1y1
    v_add_u32_e32 v10, vcc, v25, v34 ;xr1+yi1
    v_add_u32_e32 v11, vcc, v24, v35 ;xi1+yr1
      v_sub_u32_e32 v14, vcc, v10, v11
    v_add_u32_e32 v15, vcc, v10, v11
    v_sub_u32_e32 v15, vcc, s11, v15
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v14
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v15
  ;x2y1
    v_add_u32_e32 v10, vcc, v27, v34 ;xr2+yi1
    v_add_u32_e32 v11, vcc, v26, v35 ;xi2+yr1
      v_sub_u32_e32 v16, vcc, v10, v11
    v_add_u32_e32 v17, vcc, v10, v11
    v_sub_u32_e32 v17, vcc, s11, v17
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v16
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v17
  ;x3y1
    v_add_u32_e32 v10, vcc, v29, v34 ;xr3+yi1
    v_add_u32_e32 v11, vcc, v28, v35 ;xi3+yr1
      v_sub_u32_e32 v18, vcc, v10, v11
    v_add_u32_e32 v19, vcc, v10, v11
    v_sub_u32_e32 v19, vcc, s11, v19
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v18
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v19

    s_waitcnt 0



    ;x0y2
    ; IM = 8 * (xr-yr-xi+yi)
    ; RE = 128*NT - 8 * (xr+yr+xi+yi)
    v_add_u32_e32 v10, vcc, v23, v36 ;xr0+yi2
    v_add_u32_e32 v11, vcc, v22, v37 ;xi0+yr2
      v_sub_u32_e32 v12, vcc, v10, v11
    v_add_u32_e32 v13, vcc, v10, v11
    v_sub_u32_e32 v13, vcc, s11, v13
      v_add_u32_e32 v20, vcc, s5, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v12
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v13
  ;x1y2
    v_add_u32_e32 v10, vcc, v25, v36 ;xr1+yi2
    v_add_u32_e32 v11, vcc, v24, v37 ;xi1+yr2
      v_sub_u32_e32 v14, vcc, v10, v11
    v_add_u32_e32 v15, vcc, v10, v11
    v_sub_u32_e32 v15, vcc, s11, v15
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v14
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v15
  ;x2y2
    v_add_u32_e32 v10, vcc, v27, v36 ;xr2+yi2
    v_add_u32_e32 v11, vcc, v26, v37 ;xi2+yr2
      v_sub_u32_e32 v16, vcc, v10, v11
    v_add_u32_e32 v17, vcc, v10, v11
    v_sub_u32_e32 v17, vcc, s11, v17
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v16
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v17
  ;x3y2
    v_add_u32_e32 v10, vcc, v29, v36 ;xr3+yi2
    v_add_u32_e32 v11, vcc, v28, v37 ;xi3+yr2
      v_sub_u32_e32 v18, vcc, v10, v11
    v_add_u32_e32 v19, vcc, v10, v11
    v_sub_u32_e32 v19, vcc, s11, v19
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v18
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v19

    s_waitcnt 0



    ;x0y3
    ; IM = 8 * (xr-yr-xi+yi)
    ; RE = 128*NT - 8 * (xr+yr+xi+yi)
    v_add_u32_e32 v10, vcc, v23, v38 ;xr0+yi3
    v_add_u32_e32 v11, vcc, v22, v39 ;xi0+yr3
      v_sub_u32_e32 v12, vcc, v10, v11
    v_add_u32_e32 v13, vcc, v10, v11
    v_sub_u32_e32 v13, vcc, s11, v13
      v_add_u32_e32 v20, vcc, s5, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v12
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v13
  ;x1y3
    v_add_u32_e32 v10, vcc, v25, v38 ;xr1+yi3
    v_add_u32_e32 v11, vcc, v24, v39 ;xi1+yr3
      v_sub_u32_e32 v14, vcc, v10, v11
    v_add_u32_e32 v15, vcc, v10, v11
    v_sub_u32_e32 v15, vcc, s11, v15
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v14
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v15
  ;x2y3
    v_add_u32_e32 v10, vcc, v27, v38 ;xr2+yi3
    v_add_u32_e32 v11, vcc, v26, v39 ;xi2+yr3
      v_sub_u32_e32 v16, vcc, v10, v11
    v_add_u32_e32 v17, vcc, v10, v11
    v_sub_u32_e32 v17, vcc, s11, v17
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v16
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v17
  ;x3y3
    v_add_u32_e32 v10, vcc, v29, v38 ;xr3+yi3
    v_add_u32_e32 v11, vcc, v28, v39 ;xi3+yr3
      v_sub_u32_e32 v18, vcc, v10, v11
    v_add_u32_e32 v19, vcc, v10, v11
    v_sub_u32_e32 v19, vcc, s11, v19
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v18
      v_add_u32_e32 v20, vcc, s6, v20
      v_addc_u32 v21, vcc, v21, 0, vcc
    flat_atomic_add v[20:21], v19

    s_waitcnt 0


  SKIP_PRESEED:


  ;x destination
 ; dest_x = (((iy-1)&0x6)<<3) + (localx^0x1);
  v_xor_b32_e32 v4, 0x1, v6
  v_add_u32_e32 v5, vcc, 7, v1
  v_and_b32_e32 v5, 0x6, v5
  v_lshlrev_b32 v5, 3, v5
  v_add_u32_e32 v44, vcc, v4, v5
  v_lshlrev_b32 v44, 2, v44

  ;constants & masks in s1X
  s_mov_b32 s10, 0x00010000
  s_mov_b32 s11, 0x80008000
  s_mov_b32 s16, 0x7fff7fff ;used to be s12, breaks everything!!!

  v_mov_b32 v20, 0
  v_mov_b32 v21, 0
  v_mov_b32 v22, 0
  v_mov_b32 v23, 0
  v_mov_b32 v24, 0
  v_mov_b32 v25, 0
  v_mov_b32 v26, 0
  v_mov_b32 v27, 0
  v_mov_b32 v28, 0
  v_mov_b32 v29, 0
  v_mov_b32 v30, 0
  v_mov_b32 v31, 0
  v_mov_b32 v32, 0
  v_mov_b32 v33, 0
  v_mov_b32 v34, 0
  v_mov_b32 v35, 0
  v_mov_b32 v36, 0
  v_mov_b32 v37, 0
  v_mov_b32 v38, 0
  v_mov_b32 v39, 0
  v_mov_b32 v40, 0
  v_mov_b32 v41, 0
  v_mov_b32 v42, 0
  v_mov_b32 v43, 0

  ;zero LDS overflow
  ;address_offset:
  v_lshlrev_b32 v2, 3, v0       ;(8 * lx
  v_add_u32_e32 v2, vcc, v2, v1 ;   + ly)
  v_mul_lo_u32 v2, 49, v2       ;   * (4 * 12 + 1)

  ds_write_b32 v2, v20 offset:0x00
  ds_write_b32 v2, v21 offset:0x04
  ds_write_b32 v2, v22 offset:0x08
  ds_write_b32 v2, v23 offset:0x0c
  ds_write_b32 v2, v24 offset:0x10
  ds_write_b32 v2, v25 offset:0x14
  ds_write_b32 v2, v26 offset:0x18
  ds_write_b32 v2, v27 offset:0x1c
  ds_write_b32 v2, v28 offset:0x20
  ds_write_b32 v2, v29 offset:0x24
  ds_write_b32 v2, v30 offset:0x28
  ds_write_b32 v2, v31 offset:0x2c

  ;**X** Load 4-byte from T*N_ELEM + input_x*4 (addr in bytes)
  v_mul_lo_u32 v4, s14, v3
  v_add_u32_e32 v5, vcc, s8, v0
  v_lshlrev_b32 v5, 2, v5 ;x<<log_2(#grid)
  v_add_u32_e32 v4, vcc, v5, v4
  v_add_u32_e32 v4, vcc, s0, v4
  v_mov_b32 v5, s1
  v_addc_u32 v5, vcc, v5, 0, vcc
  flat_load_dword v45, v[4:5]

  ;**Y** Load 4-byte from T*N_ELEM + input_y*4 (addr in bytes)
  v_mul_lo_u32 v6, s14, v3
  v_add_u32_e32 v7, vcc, s9, v1
  v_lshlrev_b32 v7, 2, v7 ;y<<log_2(#grid)
  v_add_u32_e32 v6, vcc, v7, v6
  v_add_u32_e32 v6, vcc, s0, v6
  v_mov_b32 v7, s1
  v_addc_u32 v7, vcc, v7, 0, vcc
  flat_load_dword v46, v[6:7]

  ;Stride: 8 timesteps * N_ELEM
  s_lshl_b32 s14, s14, 3

  ;LOOP over ALL timesteps, counter in s17
  s_mov_b32 s17, 0
  LOOP_OUTER:
    ;LOOP over 64 timesteps, counter in s18
    s_mov_b32 s18, 0
    LOOP_64:
      s_waitcnt 0
      v_mov_b32 v8, v45
      v_mov_b32 v9, v46

    s_add_u32 s17, 8, s17
    s_cmp_ge_u32 s17, s15 ;N_INTG
    s_cbranch_scc1 SKIP_LOAD
      ;CAN THESE READ OFF THE END? PAD INPUT DATA?
      ;**X** Load 4-byte from T*N_ELEM + input_x*4 (addr in bytes)
      v_add_u32_e32 v4, vcc, s14, v4
      v_addc_u32 v5, vcc, v5, 0, vcc
      flat_load_dword v45, v[4:5]

      ;**Y** Load 4-byte from T*N_ELEM + input_y*4 (addr in bytes)
      v_add_u32_e32 v6, vcc, s14, v6
      v_addc_u32 v7, vcc, v7, 0, vcc
      flat_load_dword v46, v[6:7]

    SKIP_LOAD:

      ;unpack X_000i000r (v10-13) & X_000i000i (v14-15)
      ;rewrite for v2, v3
      v_bfe_u32 v2, v8,  0, 4  ;i[0]
      v_bfe_u32 v3, v8,  4, 4  ;r[0]
      v_mad_u32_u24 v10, s10, v2, v3
      v_bfe_u32 v3, v8,  8, 4  ;i[1]
      v_mad_u32_u24 v14, s10, v3, v2
      v_bfe_u32 v2, v8, 12, 4  ;r[1]
      v_mad_u32_u24 v11, s10, v3, v2

      v_bfe_u32 v2, v8, 16, 4  ;i[2]
      v_bfe_u32 v3, v8, 20, 4  ;r[2]
      v_mad_u32_u24 v12, s10, v2, v3
      v_bfe_u32 v3, v8, 24, 4  ;i[3]
      v_mad_u32_u24 v15, s10, v3, v2
      v_bfe_u32 v2, v8, 28, 4  ;r[3]
      v_mad_u32_u24 v13, s10, v3, v2

      ;unpack Y_000r000i (v16-19)
      v_bfe_u32 v2, v9,  0, 4  ;i[0]
      v_bfe_u32 v3, v9,  4, 4  ;r[0]
      v_mad_u32_u24 v16, s10, v3, v2
      v_bfe_u32 v2, v9,  8, 4  ;i[1]
      v_bfe_u32 v3, v9, 12, 4  ;r[1]
      v_mad_u32_u24 v17, s10, v3, v2

      v_bfe_u32 v2, v9, 16, 4  ;i[2]
      v_bfe_u32 v3, v9, 20, 4  ;r[2]
      v_mad_u32_u24 v18, s10, v3, v2
      v_bfe_u32 v2, v9, 24, 4  ;i[3]
      v_bfe_u32 v3, v9, 28, 4  ;r[3]
      v_mad_u32_u24 v19, s10, v3, v2

      ;LOOP over 8 timesteps, unrolled
      ;LOOP_INNER8:
        ;no s_waitcnt between -- 35 ops ok without bank conflicts
        ;ITERATION 0
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 1
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 2
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 3
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 4
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 5
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 6
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
          ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
          ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
          ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
          ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
          ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
          ds_bpermute_b32 v15, v44, v15
        v_mov_b32_dpp v16, v16 row_ror:2
        v_mov_b32_dpp v17, v17 row_ror:2
        v_mov_b32_dpp v18, v18 row_ror:2
        v_mov_b32_dpp v19, v19 row_ror:2
        ;ITERATION 7 - No need to pass data around!
        v_mad_u32_u24 v20, v10, v16, v20 ;x0y0
        v_mad_u32_u24 v24, v10, v17, v24 ;x0y1
        v_mad_u32_u24 v32, v10, v18, v32 ;x0y2
        v_mad_u32_u24 v36, v10, v19, v36 ;x0y3
        ;  ds_bpermute_b32 v10, v44, v10
        v_mad_u32_u24 v21, v11, v16, v21 ;x1y0
        v_mad_u32_u24 v25, v11, v17, v25 ;x1y1
        v_mad_u32_u24 v33, v11, v18, v33 ;x1y2
        v_mad_u32_u24 v37, v11, v19, v37 ;x1y3
        ;  ds_bpermute_b32 v11, v44, v11
        v_mad_u32_u24 v22, v12, v16, v22 ;x2y0
        v_mad_u32_u24 v26, v12, v17, v26 ;x2y1
        v_mad_u32_u24 v34, v12, v18, v34 ;x2y2
        v_mad_u32_u24 v38, v12, v19, v38 ;x2y3
        ;  ds_bpermute_b32 v12, v44, v12
        v_mad_u32_u24 v23, v13, v16, v23 ;x3y0
        v_mad_u32_u24 v27, v13, v17, v27 ;x3y1
        v_mad_u32_u24 v35, v13, v18, v35 ;x3y2
        v_mad_u32_u24 v39, v13, v19, v39 ;x3y3
        ;  ds_bpermute_b32 v13, v44, v13
        v_bfe_u32 v2, v16, 16, 4
        v_bfe_u32 v3, v17, 16, 4
        v_bfe_u32 v8, v18, 16, 4
        v_bfe_u32 v9, v19, 16, 4
        v_mad_u32_u24 v28, v14, v2, v28 ;x01y0
        v_mad_u32_u24 v30, v14, v3, v30 ;x01y1
        v_mad_u32_u24 v40, v14, v8, v40 ;x01y2
        v_mad_u32_u24 v42, v14, v9, v42 ;x01y3
        ;  ds_bpermute_b32 v14, v44, v14
        v_mad_u32_u24 v29, v15, v2, v29 ;x23y0
        v_mad_u32_u24 v31, v15, v3, v31 ;x23y1
        v_mad_u32_u24 v41, v15, v8, v41 ;x23y2
        v_mad_u32_u24 v43, v15, v9, v43 ;x23y3
        ;  ds_bpermute_b32 v15, v44, v15
        ;v_mov_b32_dpp v16, v16 row_ror:2
        ;v_mov_b32_dpp v17, v17 row_ror:2
        ;v_mov_b32_dpp v18, v18 row_ror:2
        ;v_mov_b32_dpp v19, v19 row_ror:2
      ;END OF INNER LOOP

      ;increment T by 8
      s_add_u32 s18, 8, s18
      s_cmp_lt_u32 s18, 64
    s_cbranch_scc1 LOOP_64
    ;END OF 64 LOOP

    ;overflow to LDS
    ;address_offset:
    v_lshlrev_b32 v2, 3, v0       ;(8 * lx
    v_add_u32_e32 v2, vcc, v2, v1 ;   + ly)
    v_mul_lo_u32 v3, 49, v2       ;   * (4 * 12 + 1)

    v_and_b32     v2,  s11, v20
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v10, s11, v21
    v_lshrrev_b32 v10,   7, v10
    v_add_u32_e32 v10, vcc, v2, v10
    ds_add_u32 v3, v10 offset:0x00

    v_and_b32     v2,  s11, v22
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v11, s11, v23
    v_lshrrev_b32 v11,   7, v11
    v_add_u32_e32 v11, vcc, v2, v11
    ds_add_u32 v3, v11 offset:0x04

    v_and_b32     v2,  s11, v24
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v12, s11, v25
    v_lshrrev_b32 v12,   7, v12
    v_add_u32_e32 v12, vcc, v2, v12
    ds_add_u32 v3, v12 offset:0x08

    v_and_b32     v2,  s11, v26
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v13, s11, v27
    v_lshrrev_b32 v13,   7, v13
    v_add_u32_e32 v13, vcc, v2, v13
    ds_add_u32 v3, v13 offset:0x0c

    v_and_b32     v2,  s11, v28
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v14, s11, v29
    v_lshrrev_b32 v14,   7, v14
    v_add_u32_e32 v14, vcc, v2, v14
    ds_add_u32 v3, v14 offset:0x10

    v_and_b32     v2,  s11, v30
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v15, s11, v31
    v_lshrrev_b32 v15,   7, v15
    v_add_u32_e32 v15, vcc, v2, v15
    ds_add_u32 v3, v15 offset:0x14


    v_and_b32     v2,  s11, v32
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v16, s11, v33
    v_lshrrev_b32 v16,   7, v16
    v_add_u32_e32 v16, vcc, v2, v16
    ds_add_u32 v3, v16 offset:0x18

    v_and_b32     v2,  s11, v34
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v17, s11, v35
    v_lshrrev_b32 v17,   7, v17
    v_add_u32_e32 v17, vcc, v2, v17
    ds_add_u32 v3, v17 offset:0x1c

    v_and_b32     v2,  s11, v36
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v18, s11, v37
    v_lshrrev_b32 v18,   7, v18
    v_add_u32_e32 v18, vcc, v2, v18
    ds_add_u32 v3, v18 offset:0x20

    v_and_b32     v2,  s11, v38
    v_lshrrev_b32 v2,   15, v2
    v_and_b32     v19, s11, v39
    v_lshrrev_b32 v19,   7, v19
    v_add_u32_e32 v19, vcc, v2, v19
    ds_add_u32 v3, v19 offset:0x24

    v_and_b32     v2, s11, v40
    v_lshrrev_b32 v2,  15, v2
    v_and_b32     v8, s11, v41
    v_lshrrev_b32 v8,   7, v8
    v_add_u32_e32 v8, vcc, v2, v8
    ds_add_u32 v3, v8 offset:0x28

    v_and_b32     v2, s11, v42
    v_lshrrev_b32 v2,  15, v2
    v_and_b32     v9, s11, v43
    v_lshrrev_b32 v9,   7, v9
    v_add_u32_e32 v9, vcc, v2, v9
    ds_add_u32 v3, v9 offset:0x2c

    ;strip overflow bits
    v_and_b32 v20, s16, v20
    v_and_b32 v21, s16, v21
    v_and_b32 v22, s16, v22
    v_and_b32 v23, s16, v23
    v_and_b32 v24, s16, v24
    v_and_b32 v25, s16, v25
    v_and_b32 v26, s16, v26
    v_and_b32 v27, s16, v27
    v_and_b32 v28, s16, v28
    v_and_b32 v29, s16, v29
    v_and_b32 v30, s16, v30
    v_and_b32 v31, s16, v31
    v_and_b32 v32, s16, v32
    v_and_b32 v33, s16, v33
    v_and_b32 v34, s16, v34
    v_and_b32 v35, s16, v35
    v_and_b32 v36, s16, v36
    v_and_b32 v37, s16, v37
    v_and_b32 v38, s16, v38
    v_and_b32 v39, s16, v39
    v_and_b32 v40, s16, v40
    v_and_b32 v41, s16, v41
    v_and_b32 v42, s16, v42
    v_and_b32 v43, s16, v43

  s_waitcnt 0

;    s_add_u32 s17, 64, s17 ;moved up for bypass
    s_cmp_lt_u32 s17, s15 ;N_INTG
  s_cbranch_scc1 LOOP_OUTER
  ;END OF OUTER LOOP









;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; generate & write outputs ;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;



 ;write offset
  v_mov_b32 v3, 0               ;global offset? [time? gate?]
  v_add_u32_e32 v3, vcc, s4, v3 ;block ID
  v_lshlrev_b32 v3, 3, v3       ;8 y / block
  v_add_u32_e32 v3, vcc, v1, v3 ;item Y
  v_lshlrev_b32 v3, 5, v3       ;32 x / y
  v_add_u32_e32 v3, vcc, v0, v3 ;item X
  v_lshlrev_b32 v3, 3, v3       ;8 output / x
  v_lshlrev_b32 v3, 2, v3       ;4B / output
 ;stride
  s_mov_b32 s5, 228  ;stride = 32*4*2 - offset
  s_mov_b32 s6, 4


 ;overflow from LDS
  ;address_offset:
  v_lshlrev_b32 v2, 3, v0       ;(8 * lx
  v_add_u32_e32 v2, vcc, v2, v1 ;   + ly)
  v_mul_lo_u32 v2, 49, v2       ;   * (4 * 12 + 1)

  ds_read_b32  v8, v2 offset:0x00
  ds_read_b32  v9, v2 offset:0x04
  ds_read_b32 v10, v2 offset:0x10

  ds_read_b32 v11, v2 offset:0x08
  ds_read_b32 v12, v2 offset:0x0c
  ds_read_b32 v13, v2 offset:0x14

  ds_read_b32 v14, v2 offset:0x18
  ds_read_b32 v15, v2 offset:0x1c
  ds_read_b32 v16, v2 offset:0x28

  ds_read_b32 v17, v2 offset:0x20
  ds_read_b32 v18, v2 offset:0x24
  ds_read_b32 v19, v2 offset:0x2c

  s_waitcnt 0

  ;base addresss
   v_mov_b32 v2, s20
    v_add_u32_e32 v2, vcc, v3, v2
   v_mov_b32 v3, s21
    v_addc_u32 v3, vcc, v3, 0, vcc

;ROW 0
 ;x0y0
  v_bfe_u32 v4, v20,  0, 16  ;x0y0 IM
  v_bfe_u32 v5, v28,  0, 16
  v_sub_u32_e32 v4, vcc, v5, v4
    v_bfe_u32 v7, v8, 0, 8
    v_lshlrev_b32 v7, 15, v7
    v_sub_u32_e32 v4, vcc, v4, v7
    v_bfe_u32 v7, v10, 0, 8
    v_lshlrev_b32 v7, 15, v7
    v_add_u32_e32 v4, vcc, v7, v4
  v_bfe_u32 v5, v20, 16, 16  ;x0y0 RE
    v_bfe_u32 v6, v8, 16, 8
    v_lshlrev_b32 v6, 15, v6
    v_add_u32_e32 v5, vcc, v6, v5
 ;x1y0
  v_bfe_u32 v6, v21,  0, 16  ;x1y0 IM
  v_bfe_u32 v7, v28, 16, 16
  v_sub_u32_e32 v6, vcc, v7, v6
    v_bfe_u32 v7, v8, 8, 8
    v_lshlrev_b32 v7, 15, v7
    v_sub_u32_e32 v6, vcc, v6, v7
    v_bfe_u32 v7, v10, 16, 8
    v_lshlrev_b32 v7, 15, v7
    v_add_u32_e32 v6, vcc, v7, v6
  v_bfe_u32 v7, v21, 16, 16  ;x1y0 RE
    v_bfe_u32 v8, v8, 24, 8
    v_lshlrev_b32 v8, 15, v8
    v_add_u32_e32 v7, vcc, v8, v7
 ;x2y0
  v_bfe_u32 v20, v22,  0, 16  ;x2y0 IM
  v_bfe_u32 v21, v29,  0, 16
  v_sub_u32_e32 v20, vcc, v21, v20
    v_bfe_u32 v8, v9, 0, 8
    v_lshlrev_b32 v8, 15, v8
    v_sub_u32_e32 v20, vcc, v20, v8
    v_bfe_u32 v8, v10, 8, 8
    v_lshlrev_b32 v8, 15, v8
    v_add_u32_e32 v20, vcc, v8, v20
  v_bfe_u32 v21, v22, 16, 16  ;x2y0 RE
    v_bfe_u32 v8, v9, 16, 8
    v_lshlrev_b32 v8, 15, v8
    v_add_u32_e32 v21, vcc, v8, v21
 ;x3y0
  v_bfe_u32 v22, v23,  0, 16  ;x3y0 IM
  v_bfe_u32 v28, v29, 16, 16
  v_sub_u32_e32 v22, vcc, v28, v22
    v_bfe_u32 v8, v9, 8, 8
    v_lshlrev_b32 v8, 15, v8
    v_sub_u32_e32 v22, vcc, v22, v8
    v_bfe_u32 v8, v10, 24, 8
    v_lshlrev_b32 v8, 15, v8
    v_add_u32_e32 v22, vcc, v8, v22
  v_bfe_u32 v23, v23, 16, 16  ;x3y0 RE
    v_bfe_u32 v8, v9, 24, 8
    v_lshlrev_b32 v8, 15, v8
    v_add_u32_e32 v23, vcc, v8, v23
  ;free: 8,9,10, 28,29
  ;used: 0,1,2,3, 4,5,6,7, 20,21,22,23

  ;addr calc & write
   ;no offset for the first one;
   ; hence no address rollover ;
   flat_atomic_add v[2:3], v4
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v5
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v6
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v7
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v20
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v21
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v22
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
   flat_atomic_add v[2:3], v23


 ;ROW 1
  ;free: 8,9,10, 28,29
  ;used: 0,1,2,3,4,5,6,7, 20,21,22,23
 ;x0y1
  v_bfe_u32 v8, v24,  0, 16  ;x0y1 IM
  v_bfe_u32 v9, v30,  0, 16
  v_sub_u32_e32 v8, vcc, v9, v8
    v_bfe_u32 v10, v11, 0, 8
    v_lshlrev_b32 v10, 15, v10
    v_sub_u32_e32 v8, vcc, v8, v10
    v_bfe_u32 v10, v13, 0, 8
    v_lshlrev_b32 v10, 15, v10
    v_add_u32_e32 v8, vcc, v10, v8
  v_bfe_u32 v9, v24, 16, 16  ;x0y1 RE
    v_bfe_u32 v10, v11, 16, 8
    v_lshlrev_b32 v10, 15, v10
    v_add_u32_e32 v9, vcc, v10, v9
;free: 10, 24,28,29
;used: 0,1,2,3,4,5,6,7,8,9, 20,21,22,23
 ;x1y1
  v_bfe_u32 v28, v25,  0, 16  ;x1y1 IM
  v_bfe_u32 v29, v30, 16, 16
  v_sub_u32_e32 v28, vcc, v29, v28
    v_bfe_u32 v10, v11, 8, 8
    v_lshlrev_b32 v10, 15, v10
    v_sub_u32_e32 v28, vcc, v28, v10
    v_bfe_u32 v10, v13, 16, 8
    v_lshlrev_b32 v10, 15, v10
    v_add_u32_e32 v28, vcc, v10, v28
  v_bfe_u32 v29, v25, 16, 16  ;x1y1 RE
    v_bfe_u32 v10, v11, 24, 8
    v_lshlrev_b32 v10, 15, v10
    v_add_u32_e32 v29, vcc, v10, v29
;free: 10,11 24,25, 30
;used: 0,1,2,3,4,5,6,7,8,9, 20,21,22,23, 28,29
 ;x2y1
  v_bfe_u32 v10, v26,  0, 16  ;x2y1 IM
  v_bfe_u32 v11, v31,  0, 16
  v_sub_u32_e32 v10, vcc, v11, v10
    v_bfe_u32 v30, v12, 0, 8
    v_lshlrev_b32 v30, 15, v30
    v_sub_u32_e32 v10, vcc, v10, v30
    v_bfe_u32 v30, v13, 8, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v10, vcc, v30, v10
  v_bfe_u32 v11, v26, 16, 16  ;x2y1 RE
    v_bfe_u32 v30, v12, 16, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v11, vcc, v30, v11
;free: 24,25,26, 30
;used: 0,1,2,3,4,5,6,7,8,9,10,11, 20,21,22,23, 28,29
 ;x3y1
  v_bfe_u32 v24, v27,  0, 16  ;x3y1 IM
  v_bfe_u32 v25, v31, 16, 16
  v_sub_u32_e32 v24, vcc, v25, v24
    v_bfe_u32 v30, v12, 8, 8
    v_lshlrev_b32 v30, 15, v30
    v_sub_u32_e32 v24, vcc, v24, v30
    v_bfe_u32 v30, v13, 24, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v24, vcc, v30, v24
  v_bfe_u32 v25, v27, 16, 16  ;x3y1 RE
    v_bfe_u32 v30, v12, 24, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v25, vcc, v30, v25


    v_add_u32_e32 v2, vcc, s5, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v8
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v9
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v28
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v29
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v10
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v11
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v24
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v25


 ;ROW 2
;free: 12,13, 26,27, 30,31
;used: 0,1,2,3,4,5,6,7,8,9,10,11, 20,21,22,23,24,25, 28,29
 ;x0y1
  v_bfe_u32 v12, v32,  0, 16  ;x0y1 IM
  v_bfe_u32 v13, v40,  0, 16
  v_sub_u32_e32 v12, vcc, v13, v12
    v_bfe_u32 v30, v14, 0, 8
    v_lshlrev_b32 v30, 15, v30
    v_sub_u32_e32 v12, vcc, v12, v30
    v_bfe_u32 v30, v16, 0, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v12, vcc, v30, v12
  v_bfe_u32 v13, v32, 16, 16  ;x0y1 RE
    v_bfe_u32 v30, v14, 16, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v13, vcc, v30, v13
 ;x1y1
  v_bfe_u32 v26, v33,  0, 16  ;x1y1 IM
  v_bfe_u32 v27, v40, 16, 16
  v_sub_u32_e32 v26, vcc, v27, v26
    v_bfe_u32 v30, v14, 8, 8
    v_lshlrev_b32 v30, 15, v30
    v_sub_u32_e32 v26, vcc, v26, v30
    v_bfe_u32 v30, v16, 16, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v26, vcc, v30, v26
  v_bfe_u32 v27, v33, 16, 16  ;x1y1 RE
    v_bfe_u32 v30, v14, 24, 8
    v_lshlrev_b32 v30, 15, v30
    v_add_u32_e32 v27, vcc, v30, v27
;free: 30,31,32,33
;used: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,
;      20,21,22,23,24,25,26,27,28,29
 ;x2y1
  v_bfe_u32 v30, v34,  0, 16  ;x2y1 IM
  v_bfe_u32 v31, v41,  0, 16
  v_sub_u32_e32 v30, vcc, v31, v30
    v_bfe_u32 v33, v15, 0, 8
    v_lshlrev_b32 v33, 15, v33
    v_sub_u32_e32 v30, vcc, v30, v33
    v_bfe_u32 v33, v16, 8, 8
    v_lshlrev_b32 v33, 15, v33
    v_add_u32_e32 v30, vcc, v33, v30
  v_bfe_u32 v31, v34, 16, 16  ;x2y1 RE
    v_bfe_u32 v33, v15, 16, 8
    v_lshlrev_b32 v33, 15, v33
    v_add_u32_e32 v31, vcc, v33, v31
 ;x3y1
  v_bfe_u32 v32, v35,  0, 16  ;x3y1 IM
  v_bfe_u32 v33, v41, 16, 16
  v_sub_u32_e32 v32, vcc, v33, v32
    v_bfe_u32 v34, v15, 8, 8
    v_lshlrev_b32 v34, 15, v34
    v_sub_u32_e32 v32, vcc, v32, v34
    v_bfe_u32 v34, v16, 24, 8
    v_lshlrev_b32 v34, 15, v34
    v_add_u32_e32 v32, vcc, v34, v32
  v_bfe_u32 v33, v35, 16, 16  ;x3y1 RE
    v_bfe_u32 v34, v15, 24, 8
    v_lshlrev_b32 v34, 15, v34
    v_add_u32_e32 v33, vcc, v34, v33


    v_add_u32_e32 v2, vcc, s5, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v12
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v13
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v26
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v27
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v30
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v31
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v32
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v33


 ;ROW 3
;free: 14,15,16,34,35,40,41
;used: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,
;      20,21,22,23,24,25,26,27,28,29,30,31,32,33
 ;x0y1
  v_bfe_u32 v14, v36,  0, 16  ;x0y1 IM
  v_bfe_u32 v15, v42,  0, 16
  v_sub_u32_e32 v14, vcc, v15, v14
    v_bfe_u32 v41, v17, 0, 8
    v_lshlrev_b32 v41, 15, v41
    v_sub_u32_e32 v14, vcc, v14, v41
    v_bfe_u32 v41, v19, 0, 8
    v_lshlrev_b32 v41, 15, v41
    v_add_u32_e32 v14, vcc, v41, v14
  v_bfe_u32 v15, v36, 16, 16  ;x0y1 RE
    v_bfe_u32 v41, v17, 16, 8
    v_lshlrev_b32 v41, 15, v41
    v_add_u32_e32 v15, vcc, v41, v15
;x1y1
  v_bfe_u32 v34, v37,  0, 16  ;x1y1 IM
  v_bfe_u32 v35, v42, 16, 16
  v_sub_u32_e32 v34, vcc, v35, v34
    v_bfe_u32 v41, v17, 8, 8
    v_lshlrev_b32 v41, 15, v41
    v_sub_u32_e32 v34, vcc, v34, v41
    v_bfe_u32 v41, v19, 16, 8
    v_lshlrev_b32 v41, 15, v41
    v_add_u32_e32 v34, vcc, v41, v34
  v_bfe_u32 v35, v37, 16, 16  ;x1y1 RE
    v_bfe_u32 v41, v17, 24, 8
    v_lshlrev_b32 v41, 15, v41
    v_add_u32_e32 v35, vcc, v41, v35
;free: 16,36,37,40,41,42
;used: 0,1,2,3,4,5,6,7,8,9,10,11,12,13, 15,16
;      20,21,22,23,24,25,26,27,28,29,30,31,32,33
;      34,35,
;x2y1
  v_bfe_u32 v36, v38,  0, 16  ;x2y1 IM
  v_bfe_u32 v37, v43,  0, 16
  v_sub_u32_e32 v36, vcc, v37, v36
    v_bfe_u32 v16, v18, 0, 8
    v_lshlrev_b32 v16, 15, v16
    v_sub_u32_e32 v36, vcc, v36, v16
    v_bfe_u32 v16, v19, 8, 8
    v_lshlrev_b32 v16, 15, v16
    v_add_u32_e32 v36, vcc, v16, v36
  v_bfe_u32 v37, v38, 16, 16  ;x2y1 RE
    v_bfe_u32 v16, v18, 16, 8
    v_lshlrev_b32 v16, 15, v16
    v_add_u32_e32 v37, vcc, v16, v37
;x3y1
  v_bfe_u32 v41, v39,  0, 16  ;x3y1 IM
  v_bfe_u32 v42, v43, 16, 16
  v_sub_u32_e32 v41, vcc, v42, v41
    v_bfe_u32 v16, v18, 8, 8
    v_lshlrev_b32 v16, 15, v16
    v_sub_u32_e32 v41, vcc, v41, v16
    v_bfe_u32 v16, v19, 24, 8
    v_lshlrev_b32 v16, 15, v16
    v_add_u32_e32 v41, vcc, v16, v41
  v_bfe_u32 v42, v39, 16, 16  ;x3y1 RE
    v_bfe_u32 v16, v18, 24, 8
    v_lshlrev_b32 v16, 15, v16
    v_add_u32_e32 v42, vcc, v16, v42

    v_add_u32_e32 v2, vcc, s5, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v14
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v15
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v34
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v35
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v36
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v37
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v41
    v_add_u32_e32 v2, vcc, s6, v2
    v_addc_u32 v3, vcc, v3, 0, vcc
  flat_atomic_add v[2:3], v42

;free: 16,17,18,19, 38,39,40,
;used: 0,1,2,3,4,5,6,7,8,9,10,11,12,13, 15,16
;      20,21,22,23,24,25,26,27,28,29,30,31,32,33
;      34,35,36,37, 41,42

s_waitcnt 0
s_endpgm

.Lfunc_end0:
     .size   CHIME_X, .Lfunc_end0-CHIME_X

